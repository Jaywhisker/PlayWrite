{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Notebook on training a InceptionV3 Encoder and LSTM Decoder Image Captioning Model with Attention\n",
    "What to expect:\n",
    "1. Application of custom data loaders\n",
    "2. Application of image captioning model with adaptive attention (Finetune and Pertrained InceptionV3 and Resnet50 models)\n",
    "3. Application of BLEU and ROGUE\n",
    "4. Application of model training, validation and test\n",
    "5. Application of model inference\n",
    "\n",
    "- Note: Due to the limitation of relative imports in notebooks, the full codes from src/helper functions have been copy pasted here to prevent the need for running imports\n",
    "- Proper python functions in src/main.py will be rely on imports\n",
    "\n",
    "Image Size for InceptionV3 must be larger than (299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from PIL import Image, ImageOps\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Application of custom data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer to split sentences into a list of words\n",
    "word_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class Vocabulary():\n",
    "  \"\"\"\n",
    "  Class to convert the captions to index sequential tensors\n",
    "\n",
    "  Args:\n",
    "    freq_threshold (int, optional): How many times a word has to appear in dataset before it can be added to the vocabulary. Defaults to 2\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, freq_threshold:int=2):\n",
    "    self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"} #index to sentence\n",
    "    self.stoi = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\": 2, \"<UNK>\":3} #sentence to index\n",
    "    self.freq_threshold = freq_threshold #threshold for adding a word to the vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.itos)\n",
    "\n",
    "  @staticmethod\n",
    "  def tokenizer_eng(text):\n",
    "    #convert sentence to list of words\n",
    "    return [tok.text.lower() for tok in word_tokenizer.tokenizer(text)] #convert sentence to words\n",
    "\n",
    "\n",
    "  def build_vocabulary(self, sentence_list):\n",
    "    frequencies = {}\n",
    "    idx = 4 #0-3 are for special tokens\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "      for word in self.tokenizer_eng(sentence): #convert sentence to words\n",
    "        if word not in frequencies:\n",
    "          frequencies[word] = 1\n",
    "        else:\n",
    "          frequencies[word] += 1\n",
    "\n",
    "        if frequencies[word] == self.freq_threshold: #once met freq_threshold, add to vocab list\n",
    "          self.stoi[word] = idx\n",
    "          self.itos[idx] = word\n",
    "          idx += 1\n",
    "\n",
    "  def numericalize(self, text):\n",
    "    tokenized_text = self.tokenizer_eng(text) #convert annnotations to labels by converting each word to the index inside the vocab, else UNK tag\n",
    "    return [\n",
    "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "        for token in tokenized_text\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Class to setup the custom Dataset for pyTorch Dataloader\n",
    "\n",
    "    Args:\n",
    "        Note: the order of the csv_file and root_dir are directly related, csv_file[0] contains captions for images in root_dir[0]\n",
    "        \n",
    "        csv_file (list): Lists of path to CSV files with annotations.\n",
    "        root_dir (list): List of directory containing images.\n",
    "        img_size (tuple, optional): Image size in the format (width, height), defaults to (256,256)\n",
    "        transform (callable, optional): Optional torchvision transform to be applied on a sample, defaults to None\n",
    "        freq_threshold (int, optional): Freq threshold for Vocabulary Class, defaults to 2\n",
    "        vocabulary (Vocabulary, optional): Determines to use an existing vocabulary or create own, defaults to None\n",
    "    Returns:\n",
    "        image: transformed image\n",
    "        labels: tensor object of the labels\n",
    "        all_image_captions: list containing all the captions of the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file:list, root_dir:list, img_size:tuple=(256,256), transform=None, freq_threshold=2, vocabulary=None):\n",
    "        \n",
    "        #dataframe with col name ['image_filename', 'image_captions'] from csv file\n",
    "        self.annotations = pd.DataFrame()\n",
    "        #list containing the int boundary on which image path to look at\n",
    "        #list will containing the num of images in directory which is the boundary\n",
    "        self.root_dir_boundary = []\n",
    "        \n",
    "        for idx, label_files in enumerate(csv_file): \n",
    "            labels = pd.read_csv(label_files, index_col=0) #remove index col\n",
    "            self.annotations = pd.concat([self.annotations, labels], ignore_index=True) #merging annotations into 1 dataset\n",
    "\n",
    "            #getting the image boundary on which idx belongs to which image file path\n",
    "            if idx == 0: \n",
    "                self.root_dir_boundary.append(len(labels))\n",
    "            else:\n",
    "                #get the number of images in root directory and add with the previous to get the range of index that are in this filepath\n",
    "                self.root_dir_boundary.append(self.root_dir_boundary[idx-1] + len(labels))\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "        #initialise vocabulary\n",
    "        if vocabulary == None:\n",
    "            self.vocab = Vocabulary(freq_threshold)\n",
    "            self.vocab.build_vocabulary(self.annotations.iloc[:,1].to_list()) #build vocab with all captions\n",
    "        else:\n",
    "            self.vocab = vocabulary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.annotations.iloc[idx, 0] #Image name as column 0\n",
    "        #finding the correct root directory filepath in the list does the image belong to\n",
    "        image_dir_idx = 0\n",
    "        while idx >= self.root_dir_boundary[image_dir_idx] and image_dir_idx < len(self.root_dir_boundary):\n",
    "            image_dir_idx += 1\n",
    "\n",
    "        img_path = f\"{self.root_dir[image_dir_idx]}/{img_name}\"\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        # image = ImageOps.pad(image, self.img_size) #resize image\n",
    "        annotation = self.annotations.iloc[idx, 1] #Annotation as column 1\n",
    "        \n",
    "        #converting caption to index tensor\n",
    "        numercalized_annotations = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numercalized_annotations += self.vocab.numericalize(annotation)\n",
    "        numercalized_annotations.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        #create list of all captions associated with the image (for BLEU & ROUGE score)\n",
    "        all_img_captions = self.annotations[self.annotations['image_filename'] == img_name]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(numercalized_annotations), all_img_captions.iloc[:,1].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class padAnnotations():\n",
    "  \"\"\"\n",
    "  Collate function to pad all caption to the same length as max(len(caption)) in a batch\n",
    "\n",
    "  Args:\n",
    "    pad_idx (int): Index Label for the <PAD> token\n",
    "    batch_first (boolean, optional): Decide if the dataset labels should be batch first\n",
    "                                     Either returns (batch size, seq length) or (seq length, batch size)\n",
    "  \n",
    "  Returns:\n",
    "    img: batch image object\n",
    "    labels: batch of tensors of the captions, converted to the same length by adding <PAD>\n",
    "    all_labels: batch of lists of the captions of the images\n",
    "  \"\"\"\n",
    "  def __init__(self, pad_idx, batch_first = False):\n",
    "    self.batch_first = batch_first\n",
    "    self.pad_idx = pad_idx\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    imgs = [item[0].unsqueeze(0) for item in batch] \n",
    "    imgs = torch.cat(imgs, dim=0)\n",
    "    labels = [item[1] for item in batch]\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "    all_labels = [item[2] for item in batch]\n",
    "\n",
    "    return imgs, labels, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_img_caption_data(dataloader, batch_size: int, dataset: CustomDataset, num_batches: int = 1, num_samples: int=9):\n",
    "    \"\"\"\n",
    "    Function to visualise the dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader (dataloader object): Pytorch dataloader object to visualise\n",
    "        batch_size (int): Batch Size of dataloader\n",
    "        dataset (CustomDataset): dataset used to create dataloader (required to get the vocabulary)\n",
    "        num_batches (int, optional): How many batches to visualise, defaults to 1\n",
    "        num_samples (int, optional): How many images per batch to visualise, defaults to 9\n",
    "\n",
    "    Returns:\n",
    "        Plot of image with its caption and the list of the captions to the image\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        train_features, train_labels, train_all_captions = next(iter(dataloader))\n",
    "        if i == 0:  # Print shape\n",
    "            print(f\"Feature batch shape: {train_features.size()}\")\n",
    "            print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "            print(f\"All captions batch size: {len(train_all_captions)}\")\n",
    "\n",
    "        for j in range(num_samples):\n",
    "            #Prepare image to print\n",
    "            img = train_features[j].squeeze()\n",
    "            img = np.transpose(img.numpy(), (1, 2, 0))  #Convert from (channel, height, width) to (height, width, channel) for matplotlib\n",
    "\n",
    "            #Transpose train_labels and handle out of range indices\n",
    "            label = train_labels[:, j] if train_labels.size(0) != batch_size else train_labels[j]\n",
    "            string_label = [dataset.vocab.itos[idx] for idx in label.tolist()]\n",
    "            actual_caption = \" \".join([token for token in string_label if token not in ['<PAD>', '<SOS>', '<EOS>']])\n",
    "\n",
    "            #Create a new plot for each image\n",
    "            fig, ax = plt.subplots(figsize=(5, 5))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'Caption: {actual_caption}\\n\\nAll Possible Captions:\\n' + \"\\n\".join(train_all_captions[j]), loc='left')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (299,299)\n",
    "batch_size = 64\n",
    "\n",
    "#Define transforms for image preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#No data augmentation\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual dataset for this notebook\n",
    "\n",
    "#Create train dataset\n",
    "dataset = CustomDataset(csv_file=['../input/Landscape/Train/Labels/Blip_Label_Clean.csv', '../input/Landscape/Train/Labels/Kosmos_Label_Clean.csv', '../input/FilteredFlicker/Train/Labels/Label.csv'],\n",
    "                        root_dir=['../input/Landscape/Train/Images', '../input/Landscape/Train/Images', '../input/FilteredFlicker/Train/Images'],\n",
    "                        transform=train_transform,\n",
    "                        img_size=image_size)\n",
    "\n",
    "#Create val and test dataset, note that\n",
    "#1. transform uses eval transform with no data augmentation\n",
    "#2. vocab is the original train dataset vocab (to map the correct index)\n",
    "val_dataset = CustomDataset(csv_file=['../input/Landscape/Validation/Labels/Blip_Label_Clean.csv', '../input/Landscape/Validation/Labels/Kosmos_Label_Clean.csv', '../input/FilteredFlicker/Validation/Labels/Label.csv'],\n",
    "                        root_dir=['../input/Landscape/Validation/Images', '../input/Landscape/Validation/Images', '../input/FilteredFlicker/Validation/Images'],\n",
    "                        transform=eval_transform,\n",
    "                        img_size=image_size,\n",
    "                        vocabulary=dataset.vocab)\n",
    "\n",
    "#only focusing on landscape image, please add flicker30k if u would like\n",
    "test_dataset = CustomDataset(csv_file=['../input/Landscape/Test/Labels/Blip_Label_Clean.csv', '../input/Landscape/Test/Labels/Kosmos_Label_Clean.csv'],\n",
    "                        root_dir=['../input/Landscape/Test/Images', '../input/Landscape/Test/Images'],\n",
    "                        transform=eval_transform,\n",
    "                        img_size=image_size,\n",
    "                        vocabulary=dataset.vocab)\n",
    "\n",
    "\n",
    "#create dataloaders, just update the dataset it is from\n",
    "train_dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=True\n",
    "                        ))\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=True\n",
    "                        ))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=True\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img_caption_data(train_dataloader, batch_size, dataset, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Application of image captioning with attention model\n",
    "- Implementing a InceptionV3 and a Resnet50 Encoder model that accepts fine-tuned model to a Attention and LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3EncoderCNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    InceptionV3 CNN Encoder Model, feature extraction layer (mixed_7c) is always trainable\n",
    "\n",
    "    Args:\n",
    "        finetuned_model: Finetuned inceptionV3 model, else None\n",
    "        train_cnn (bool, optional): Determines if the entire CNN model will be unfreeze and trained during the training. Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(self, finetuned_model, train_cnn:bool=False):\n",
    "        super(InceptionV3EncoderCNN, self).__init__()\n",
    "        if finetuned_model != None:\n",
    "            self.inception = list(finetuned_model.children())[0]\n",
    "        \n",
    "        else:\n",
    "            self.inception = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "        self.inception.aux_logits = False\n",
    "        \n",
    "        #Remove last classification layer\n",
    "        self.inception.fc = torch.nn.Identity()\n",
    "\n",
    "        #Variable that will hold the features\n",
    "        self.features = None\n",
    "        \n",
    "        #Register the hook to capture features at output of last CNN layer\n",
    "        self.inception.Mixed_7c.register_forward_hook(self.capture_features_hook)\n",
    "\n",
    "        #Train the feature map, the rest depends on train_CNN\n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if 'Mixed_7c' in name:\n",
    "                param.requires_grad_(True)\n",
    "            else:\n",
    "                param.requires_grad_(train_cnn)\n",
    "\n",
    "\n",
    "    def capture_features_hook(self, module, input, output):\n",
    "        self.features = output #update feature \n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Take images and return feature maps of size (batch, height*width)\n",
    "        \"\"\"\n",
    "        _ = self.inception(images)  #Pass through the inception network\n",
    "        batch, feature_maps, size_1, size_2 = self.features.size()  #self.features contain the feature map of size (batch size, 2048, 8,8)\n",
    "        features = self.features.permute(0, 2, 3, 1)\n",
    "        features = features.view(batch, size_1*size_2, feature_maps) #resize to (batch size, h*w, feature_maps)\n",
    "\n",
    "        return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoderCNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet50 CNN Encoder Model, feature extraction layer (fc) is always trainable\n",
    "\n",
    "    Args:\n",
    "        finetuned_model: Finetuned inceptionV3 model, else None\n",
    "        train_cnn (bool, optional): Determines if the entire CNN model will be unfreeze and trained during the training\n",
    "    \"\"\"\n",
    "    def __init__(self, finetuned_model, train_cnn:bool=False):\n",
    "        super(ResNetEncoderCNN, self).__init__()\n",
    "\n",
    "        if finetuned_model != None:\n",
    "            resnet = list(finetuned_model.children())[0]\n",
    "        else:\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        #Train the feature map, the rest depends on train_CNN\n",
    "        for name, param in resnet.named_parameters(): \n",
    "            if \"layer4.2.conv3.weight\" in name:\n",
    "                param.requires_grad_(True)\n",
    "            else:\n",
    "                param.requires_grad_(train_cnn)\n",
    "        \n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = torch.nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Take images and return feature maps of size (batch, height*width)\n",
    "        \"\"\"\n",
    "        features = self.resnet(images)\n",
    "        # first, we need to resize the tensor to be \n",
    "        # (batch, h*w, feature_maps)\n",
    "        batch, feature_maps, size_1, size_2 = features.size()       \n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        features = features.view(batch, size_1*size_2, feature_maps)\n",
    "       \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Attention Module\n",
    "\n",
    "    Args:\n",
    "        feature_dim (int): Dimension of feature maps (h*w)\n",
    "        hidden_dim (int): Dimension of hidden states\n",
    "        output_dim (int, optional): Dimension of output, default to 1\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim:int, hidden_dim:int, output_dim:int = 1):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "         # fully-connected layer to learn first weight matrix Wa\n",
    "        self.W_a = torch.nn.Linear(feature_dim, hidden_dim)\n",
    "        # fully-connected layer to learn the second weight matrix Ua\n",
    "        self.U_a = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        # fully-connected layer to produce score (output), learning weight matrix va\n",
    "        self.v_a = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, features, hidden_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: image features from Encoder\n",
    "            hidden_state: hidden state output for Decoder\n",
    "\n",
    "        Returns:\n",
    "            context: context vector with size (1,2048)\n",
    "            atten_weights: probabilities of feature relevance \n",
    "        \"\"\"\n",
    "        #add additional dimension to a hidden (required for summation) \n",
    "        hidden_state = hidden_state.unsqueeze(1) #(batch size, 1, seq length)\n",
    "\n",
    "        atten_1 = self.W_a(features) #(batch size, h*w, hidden_dim)\n",
    "        atten_2 = self.U_a(hidden_state) #(batch size, 1, hidden_dim)\n",
    "\n",
    "        #apply tangent to combine result from 2 fc layers\n",
    "        atten_tan = torch.tanh(atten_1+atten_2)\n",
    "        atten_score = self.v_a(atten_tan) #(batch size, hidden_dim)\n",
    "        atten_weight = torch.nn.functional.softmax(atten_score, dim = 1) #get softmax probablilities\n",
    "\n",
    "        #multiply each vector with its softmax score and sum to get attention context vector\n",
    "        context = torch.sum(atten_weight * features,  dim = 1) #size of context equals to a number of feature maps\n",
    "        atten_weight = atten_weight.squeeze(dim=2)\n",
    "        \n",
    "        return context, atten_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(torch.nn.Module):\n",
    "     \"\"\"\n",
    "     LSTM decoder model\n",
    "\n",
    "     Args:\n",
    "          feature_dim (int): Feature Map dimension (h*w)\n",
    "          embed_size (int): Embedding dimension to embed words\n",
    "          hidden_size (int): Hidden state dimension for LSTM\n",
    "          vocab_size (int): Total number of unique vocab\n",
    "          drop_prob (float, optional): Dropout layer probability, deafults to 0.5\n",
    "          sample_temp (float, optional): Scale outputs before softmax to allow the model to be more picky as the differences are exaggerated. Defaults to 0.5\n",
    "     \"\"\"\n",
    "     def __init__(self, feature_dim:int, embedding_dim:int, hidden_dim:int, vocab_size:int, drop_prob:float=0.5, sample_temp:float=0.5):\n",
    "          super(DecoderRNN, self).__init__()\n",
    "          \n",
    "          self.feature_dim = feature_dim\n",
    "          self.embedding_dim = embedding_dim\n",
    "          self.hidden_dim = hidden_dim\n",
    "          self.vocab_size = vocab_size\n",
    "          self.sample_temp = sample_temp #scale the outputs b4 softmax\n",
    "\n",
    "          #layers\n",
    "\n",
    "          #embedding layer that turns words into index \n",
    "          self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "          #lstm layer that takes in feature + embedding (image + caption) and output hidden_dim\n",
    "          self.lstm = torch.nn.LSTMCell(embedding_dim + feature_dim, hidden_dim)\n",
    "          #fc linear layer that predicts next word\n",
    "          self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "          #attention layer\n",
    "          self.attention = BahdanauAttention(feature_dim, hidden_dim)\n",
    "          #dropout layer\n",
    "          self.drop = torch.nn.Dropout(p=drop_prob)\n",
    "          #initialisation of fully-connected layers\n",
    "          self.init_h = torch.nn.Linear(feature_dim, hidden_dim) #initiialising hidden state and cell memory using avg of feature\n",
    "          self.init_c = torch.nn.Linear(feature_dim, hidden_dim)\n",
    "\n",
    "     def init_hidden(self, features):\n",
    "          \"\"\"\n",
    "          Initializes hidden state and cell memory using average feature vector\n",
    "          Args:\n",
    "               features: feature map of the image\n",
    "          Returns:\n",
    "               h0: initial hidden state (short-term memory)\n",
    "               c0: initial cell state (long-term memory)\n",
    "          \"\"\"\n",
    "          mean_annotations = torch.mean(features, dim = 1) #getting average of the features\n",
    "          h0 = self.init_h(mean_annotations)\n",
    "          c0 = self.init_c(mean_annotations)\n",
    "          return h0, c0\n",
    "\n",
    "     def forward(self, features, captions, device:str, sample_prob:float=0.2):\n",
    "          \"\"\"\n",
    "          Args:\n",
    "               features: feature map of image\n",
    "               captions: true caption of image\n",
    "               device (str): cuda or cpu\n",
    "               sample_prob (float, optional): Probability for auto-regressive RNN where they train on RNN output rather than true layer, defaults to 0.2\n",
    "\n",
    "          \"\"\"\n",
    "          embed = self.embeddings(captions)\n",
    "          h,c = self.init_hidden(features)\n",
    "          batch_size = captions.size(0) #captions: (batch size, seq length)\n",
    "          seq_len = captions.size(1) \n",
    "          feature_size = features.size(1) #features: (batch size, size, 2048)\n",
    "\n",
    "          #storage of outputs and attention weights of lstm\n",
    "          outputs = torch.zeros(batch_size, seq_len, self.vocab_size).to(device)\n",
    "          atten_weights = torch.zeros(batch_size, seq_len, feature_size).to(device)\n",
    "\n",
    "          #scheduled sampling for training, using the models output to train instead of using the true output\n",
    "          #autoregressive RNN training, only when length of seq > 1 (cannot be first word)\n",
    "          for t in range(seq_len):\n",
    "               s_prob = 0.0 if t==0 else sample_prob\n",
    "               use_sampling = np.random.random() < s_prob\n",
    "\n",
    "               if not use_sampling: #no sampling\n",
    "                    word_embeddings = embed[:, t, :] #embedding until word t\n",
    "               \n",
    "               context, atten_weight = self.attention(features,h)\n",
    "               inputs = torch.cat([word_embeddings, context], 1) #embed captions and features for next lstm state\n",
    "               h, c = self.lstm(inputs, (h,c)) #pass through lstm\n",
    "               output = self.fc(self.drop(h))\n",
    "               \n",
    "               if use_sampling: #using predicted word instead of true output\n",
    "                    scaled_output = output/self.sample_temp #using scaling temp to amplify the values\n",
    "                    #this way softmax will have a larger difference in values\n",
    "                    #makes the model more selective of whats its picking \n",
    "                    \n",
    "                    scoring = torch.nn.functional.log_softmax(scaled_output, dim=1)\n",
    "                    top_idx = scoring.topk(1)[1]\n",
    "                    word_embeddings = self.embeddings(top_idx).squeeze(1) #update word embeddings with predicted instead of actual\n",
    "               \n",
    "               #update results\n",
    "               outputs[:,t,:] = output\n",
    "               atten_weights[:, t, :] = atten_weight\n",
    "\n",
    "          return outputs, atten_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptV3EncoderAttentionDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    InceptionV3 Encoder with Attention and LSTM Decoder\n",
    "\n",
    "    Args:\n",
    "        finetuned_model: Finetuned InceptionV3 model, else None\n",
    "        feature_dim (int): Feature Map dimension (h*w)\n",
    "        embedding_dim (int): Embedding dimension to embed words\n",
    "        hidden_dim (int): Hidden state dimension for LSTM\n",
    "        vocab_size (int): Total number of unique vocab\n",
    "        device (str): cuda or cpu\n",
    "        train_cnn (boolean, optional): Determines if inceptionCNN model is unfreezed. Defaults to False.\n",
    "        drop_prob (float, optional): Dropout layer probability. Defaults to 0.5.\n",
    "        sample_temp (float, optional): Scale outputs before softmax to allow the model to be more picky as the differences are exaggerated. Defaults to 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, finetuned_model, feature_dim:int, embedding_dim:int, hidden_dim:int, vocab_size:int, device:str, train_cnn:bool=False, drop_prob:float=0.5, sample_temp:float=0.5):\n",
    "        super(InceptV3EncoderAttentionDecoder, self).__init__()\n",
    "        self.encoder = InceptionV3EncoderCNN(finetuned_model, train_cnn)\n",
    "        self.decoder= DecoderRNN(feature_dim, embedding_dim, hidden_dim, vocab_size, drop_prob, sample_temp)\n",
    "        self.sample_temp = sample_temp\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, image, captions):\n",
    "        features = self.encoder(image)\n",
    "        outputs, atten_weights = self.decoder(features, captions, self.device, self.sample_temp)\n",
    "        return outputs, atten_weights\n",
    "    \n",
    "\n",
    "    #for inference\n",
    "    def caption_image(self, image, vocabulary:Vocabulary, device:str, max_length:int=50):\n",
    "        \"\"\"\n",
    "        Generate caption using a greedy algorithm based on image input\n",
    "\n",
    "        Args:\n",
    "            image: image input\n",
    "            vocabulary (Vocabulary): Vocabulary to decode predictions\n",
    "            device (str): cuda or cpu\n",
    "            max_length (int, optional): Max length of generated captions. Defaults to 50.\n",
    "\n",
    "        Returns:\n",
    "            captions: string caption in a list\n",
    "            atten_weights: probabilities of feature relevance \n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "\n",
    "        result_caption = []\n",
    "        result_weights = []\n",
    "\n",
    "        with torch.no_grad(): #no training\n",
    "            input_word = torch.tensor(1).unsqueeze(0).to(device)\n",
    "            result_caption.append(1)\n",
    "            features = self.encoder(image)\n",
    "            h, c = self.decoder.init_hidden(features)\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                embedded_word = self.decoder.embeddings(input_word)\n",
    "                context, atten_weight = self.decoder.attention(features, h)\n",
    "                # input_concat shape at time step t = (batch, embedding_dim + context size)\n",
    "                input_concat = torch.cat([embedded_word, context],  dim = 1)\n",
    "                h, c = self.decoder.lstm(input_concat, (h,c))\n",
    "                h = self.decoder.drop(h)\n",
    "                output = self.decoder.fc(h) \n",
    "                scoring = torch.nn.functional.log_softmax(output, dim=1)\n",
    "                top_idx = scoring[0].topk(1)[1]\n",
    "                result_caption.append(top_idx.item())\n",
    "                result_weights.append(atten_weight)\n",
    "                input_word = top_idx\n",
    "\n",
    "                if (len(result_caption) >= max_length or vocabulary.itos[input_word.item()] == \"<EOS>\"):\n",
    "                    break\n",
    "\n",
    "            return [vocabulary.itos[idx] for idx in result_caption], result_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoderAttentionDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Resnet50 Encoder with Attention and LSTM Decoder\n",
    "\n",
    "    Args:\n",
    "        finetuned_model: Finetuned InceptionV3 model, else None\n",
    "        feature_dim (int): Feature Map dimension (h*w)\n",
    "        embedding_dim (int): Embedding dimension to embed words\n",
    "        hidden_dim (int): Hidden state dimension for LSTM\n",
    "        vocab_size (int): Total number of unique vocab\n",
    "        device (str): cuda or cpu\n",
    "        train_cnn (boolean, optional): Determines if inceptionCNN model is unfreezed. Defaults to False.\n",
    "        drop_prob (float, optional): Dropout layer probability. Defaults to 0.5.\n",
    "        sample_temp (float, optional): Scale outputs before softmax to allow the model to be more picky as the differences are exaggerated. Defaults to 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, finetuned_model, feature_dim:int, embedding_dim:int, hidden_dim:int, vocab_size:int, device:str, train_cnn:bool=False, drop_prob:float=0.5, sample_temp:float=0.5):\n",
    "        super(ResNetEncoderAttentionDecoder, self).__init__()\n",
    "        self.encoder = ResNetEncoderCNN(finetuned_model, train_cnn)\n",
    "        self.decoder= DecoderRNN(feature_dim, embedding_dim, hidden_dim, vocab_size, drop_prob, sample_temp)\n",
    "        self.sample_temp = sample_temp\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, image, captions):\n",
    "        features = self.encoder(image)\n",
    "        outputs, atten_weights = self.decoder(features, captions, self.device, self.sample_temp)\n",
    "        return outputs, atten_weights\n",
    "    \n",
    "\n",
    "    #for inference\n",
    "    def caption_image(self, image, vocabulary:Vocabulary, device:str, max_length:int=50):\n",
    "        \"\"\"\n",
    "        Generate caption using a greedy algorithm based on image input\n",
    "\n",
    "        Args:\n",
    "            image: image input\n",
    "            vocabulary (Vocabulary): Vocabulary to decode predictions\n",
    "            device (str): cuda or cpu\n",
    "            max_length (int, optional): Max length of generated captions. Defaults to 50.\n",
    "\n",
    "        Returns:\n",
    "            captions: string caption in a list\n",
    "            atten_weights: probabilities of feature relevance \n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "\n",
    "        result_caption = []\n",
    "        result_weights = []\n",
    "\n",
    "        with torch.no_grad(): #no training\n",
    "            input_word = torch.tensor(1).unsqueeze(0).to(device)\n",
    "            result_caption.append(1)\n",
    "            features = self.encoder(image)\n",
    "            h, c = self.decoder.init_hidden(features)\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                embedded_word = self.decoder.embeddings(input_word)\n",
    "                context, atten_weight = self.decoder.attention(features, h)\n",
    "                # input_concat shape at time step t = (batch, embedding_dim + context size)\n",
    "                input_concat = torch.cat([embedded_word, context],  dim = 1)\n",
    "                h, c = self.decoder.lstm(input_concat, (h,c))\n",
    "                h = self.decoder.drop(h)\n",
    "                output = self.decoder.fc(h) \n",
    "                scoring = torch.nn.functional.log_softmax(output, dim=1)\n",
    "                top_idx = scoring[0].topk(1)[1]\n",
    "                result_caption.append(top_idx.item())\n",
    "                result_weights.append(atten_weight)\n",
    "                input_word = top_idx\n",
    "\n",
    "                if (len(result_caption) >= max_length or vocabulary.itos[input_word.item()] == \"<EOS>\"):\n",
    "                    break\n",
    "\n",
    "            return [vocabulary.itos[idx] for idx in result_caption], result_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Application of BLEU, ROGUE and METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def get_bleu_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the bleu 1-4 score of a corpus\n",
    "\n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {\"BLEU1\":, \"BLEU2\": ,\"BLEU3\": ,\"BLEU4\": }\n",
    "    \"\"\"\n",
    "    #Tokenize the predictions and references\n",
    "    predicted = [pred.split() for pred in predicted]\n",
    "    references = [[ref.split() for ref in refs] for refs in references]\n",
    "\n",
    "\n",
    "    BLEU1 = corpus_bleu(references, predicted, weights=(1, 0, 0, 0))\n",
    "    BLEU2 = corpus_bleu(references, predicted, weights=(0.5, 0.5, 0, 0))\n",
    "    BLEU3 = corpus_bleu(references, predicted, weights=(0.33, 0.33, 0.33, 0))\n",
    "    BLEU4 = corpus_bleu(references, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return {\"BLEU1\": BLEU1, \"BLEU2\": BLEU2, \"BLEU3\": BLEU3, \"BLEU4\": BLEU4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def get_rouge_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the rogue1, rogue2, rogueL, rogueLSum scores\n",
    "        \n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {'rouge1': , 'rouge2': , 'rougeL': , 'rougeLsum': }\n",
    "    \n",
    "    \"\"\"\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results = rouge.compute(predictions=predicted, references=references)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def get_meteor_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the meteor scores\n",
    "        \n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {'meteor':}\n",
    "    \n",
    "    \"\"\"\n",
    "    meteor = evaluate.load('meteor')\n",
    "    results = meteor.compute(predictions=predicted, references=references)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Application of model training, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(outputs, batch_first:bool, vocabulary:Vocabulary):\n",
    "    \"\"\"\n",
    "    Function to convert model tensor outputs to sentences\n",
    "\n",
    "    Args:\n",
    "        outputs (torch tensor object): Model's output to be decoded, either in size (seq len, batch, vocab_size) or (batch, seq len, vocab_size)\n",
    "        batch_first (bool): Boolean of if dataloader was configured to batch_first\n",
    "        vocabulary (Vocabulary): dataset Vocabulary Class for decoding\n",
    "\n",
    "    Returns:\n",
    "        list of predicted sentences each corresponding to 1 sample in the batch\n",
    "            - will be of length (batch_size)\n",
    "            eg. ['predicted sentence 1 for sample 1', ...'predicted sentence N for sample N']\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    all_prediction = []\n",
    "    predicted_tokens = outputs.argmax(-1) #flatten vocab size dimensions\n",
    "    if not batch_first:\n",
    "        predicted_tokens = predicted_tokens.T \n",
    "    \n",
    "    for sentence_tokens in predicted_tokens:\n",
    "        sentence_tokens = sentence_tokens.tolist()\n",
    "\n",
    "        try:\n",
    "            #cropping predicted sentence to first EOS\n",
    "            eos_index = sentence_tokens.index(vocabulary.stoi['<EOS>']) #get first instance of <EOS> to crop sentence accordingly\n",
    "            predicted_sentence = sentence_tokens[:eos_index]\n",
    "        except:\n",
    "            predicted_sentence = sentence_tokens\n",
    "\n",
    "        try:\n",
    "            #getting predicted_sentence by remove <SOS>\n",
    "            predicted_sentence.remove(vocabulary.stoi['<SOS>'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        all_prediction.append(\" \".join([vocabulary.itos[idx] for idx in predicted_sentence]))\n",
    "\n",
    "    return all_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, \n",
    "         criterion, \n",
    "         dataloader, \n",
    "         image_size:tuple,\n",
    "         transformer:bool,\n",
    "         batch_first:bool, \n",
    "         vocabulary:Vocabulary, \n",
    "         device:str):\n",
    "    \"\"\"\n",
    "    Function to evaluate model performance\n",
    "\n",
    "    Args:\n",
    "        model: The model that is to be evaluated\n",
    "        criterion: Loss criterion of the model\n",
    "        dataloader: validation / test dataset\n",
    "        image_size (tuple): image size of model\n",
    "        transformer (bool): boolean if decoder is a transformer\n",
    "        batch_first (bool): boolean if dataloader samples tensor are (batch, seq len) or (seq len, batch)\n",
    "        vocabulary (Vocabulary): dataset vocabulary class\n",
    "        device (str): cpu or cuda\n",
    "\n",
    "    Returns:\n",
    "        avg_val_loss: average validation loss\n",
    "        Bleu_score: dictionary of BLEU 1-4 score\n",
    "        Rouge_score: dictionary of Rouge  1,2,L,LSum score\n",
    "        Meteor_score: dictionary of Meteor Score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0\n",
    "\n",
    "    #BLEU predictions container\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (imgs, annotations, all_annotations) in enumerate(dataloader):\n",
    "            #getting img and annotations\n",
    "            imgs = torch.nn.functional.interpolate(imgs, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            imgs = imgs.to(device)\n",
    "            annotations = annotations.to(device)\n",
    "            \n",
    "            if transformer:\n",
    "                #running model prediction\n",
    "                outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                targets = annotations[1:].reshape(-1)\n",
    "                #updating model parameters\n",
    "                loss = criterion(outputs.view(-1, len(vocabulary)), targets)\n",
    "            \n",
    "            else:\n",
    "                if not batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                    \n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.reshape(-1, outputs.shape[2]), annotations.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "                if batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs, atten_weights = model(imgs, annotations[:, :-1]) #training model to guess the last word\n",
    "                    targets = annotations[:, 1:]\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.view(-1, len(vocabulary)), targets.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            #get model predictions and update\n",
    "            predictions.extend(decode_predictions(outputs, batch_first, vocabulary))\n",
    "\n",
    "            #update references\n",
    "            references.extend(all_annotations)\n",
    "\n",
    "        Bleu_score = get_bleu_score(predictions, references)\n",
    "        Rouge_score = get_rouge_score(predictions, references)\n",
    "        Meteor_score = get_meteor_score(predictions, references)\n",
    "\n",
    "        return total_val_loss/(idx+1), Bleu_score, Rouge_score, Meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          criterion, \n",
    "          optimiser, \n",
    "          train_dataloader, \n",
    "          val_dataloader, \n",
    "          image_size: tuple,\n",
    "          batch_first:bool, \n",
    "          transformer:bool,\n",
    "          vocabulary:Vocabulary, \n",
    "          device:str, \n",
    "          num_epochs:int, \n",
    "          early_stopping_threshold: float=0.003,\n",
    "          show_train_metrics:bool=None, \n",
    "          save_every:int=None,\n",
    "          model_name:str=None, \n",
    "          overwrite:bool=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to train the model\n",
    "    If change in val loss is less than 0.01 for 2 epochs in a row, stop training\n",
    "\n",
    "    Args:\n",
    "        model: The model that is to be evaluated\n",
    "        criterion: Loss criterion of the model\n",
    "        optimiser: Optimiser function of the model\n",
    "        train_dataloader: Train dataset\n",
    "        val_dataloader: Validation dataset, use None if no Validation dataset\n",
    "        image_size (tuple): image size of model\n",
    "        batch_first (bool): Boolean if dataloader samples tensor are (batch, seq len) or (seq len, batch)\n",
    "        transformer (bool): Boolean if decoder is a transformer\n",
    "        vocabulary (Vocabulary): Dataset vocabulary class\n",
    "        device (str): cpu or cuda\n",
    "        num_epochs (int): Number of epochs for training\n",
    "        early_stopping_threshold (float, optional): Threshold for early stopping, defaults to 0.003\n",
    "        show_train_metrics (bool, optional): Booleon on should calculate BLEU & Rouge score during training, defaults to False\n",
    "        save_every (int, optional): Save model after every ___ epochs, defaults to None (no saving)\n",
    "        model_name (str, optional): Model Name to be saved after, required if save_every != None, model will be saved as (model_name)_epoch or just model_name\n",
    "        overwrite (bool, optional): Boolean on overwriting model saves or saving each specific epoch as a new model, defaults to False\n",
    "    \n",
    "    Returns\n",
    "        train_loss: list of average training loss per epoch\n",
    "        train_bleu: list of dictionary of training BLEU score per epoch, [] if show_train_metric = False\n",
    "        train_rouge: list of dictionary of training Rouge score per epoch, [] if show_train_metric = False\n",
    "        train_meteor: list of dictionary of training Meteor score per epoch, [] if show_train_metric = False\n",
    "        val_loss: list of average validation loss per epoch, [] if val_dataloader = None\n",
    "        val_bleu: list of dictionary of validation BLEU score per epoch, [] if val_dataloader = None\n",
    "        val_rouge: list of dictionary of validation Rouge score per epoch, [] if val_dataloader = None\n",
    "        val_meteor: list of dictionary of validation Meteor score per epoch, [] if val_dataloader = None\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialise results container\n",
    "    train_loss = []\n",
    "    train_bleu = []\n",
    "    train_rouge = []\n",
    "    train_meteor = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_bleu = []\n",
    "    val_rouge = []\n",
    "    val_meteor = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        #BLEU predictions container\n",
    "        predictions = []\n",
    "        references = []\n",
    "\n",
    "        #start model training\n",
    "        model.train()\n",
    "        for idx, (imgs, annotations, all_annotations) in enumerate(train_dataloader):\n",
    "            \n",
    "            #getting img and annotations\n",
    "            imgs = torch.nn.functional.interpolate(imgs, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            imgs = imgs.to(device)\n",
    "            annotations = annotations.to(device)\n",
    "\n",
    "            if transformer:\n",
    "                #running model prediction\n",
    "                outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                targets = annotations[1:].reshape(-1)\n",
    "                #updating model parameters\n",
    "                loss = criterion(outputs.view(-1, len(vocabulary)), targets)\n",
    "            \n",
    "            else:\n",
    "                if not batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.reshape(-1, outputs.shape[2]), annotations.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "                if batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs, atten_weights = model(imgs, annotations[:, :-1]) #training model to guess the last word\n",
    "                    targets = annotations[:, 1:]\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.view(-1, len(vocabulary)), targets.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "            optimiser.zero_grad() #remove optimiser gradient\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            #calculate loss and update it for each batch\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            if show_train_metrics:\n",
    "                #get model predictions and update\n",
    "                predictions.extend(decode_predictions(outputs, batch_first, vocabulary))\n",
    "\n",
    "                #update references\n",
    "                references.extend(all_annotations)\n",
    "\n",
    "        if show_train_metrics:   \n",
    "            #calculating bleu and rouge score\n",
    "            Bleu_score = get_bleu_score(predictions, references)\n",
    "            Rouge_score = get_rouge_score(predictions, references)\n",
    "            Meteor_score = get_meteor_score(predictions, references)\n",
    "            train_bleu.append(Bleu_score)\n",
    "            train_rouge.append(Rouge_score)\n",
    "            train_meteor.append(Meteor_score)\n",
    "\n",
    "        #updating values\n",
    "        train_loss.append(total_train_loss/(idx+1))\n",
    "\n",
    "        if val_dataloader != None:\n",
    "            #validation\n",
    "            avg_val_loss, val_bleu_score, val_rouge_score, val_meteor_score = eval(\n",
    "                                                                            model=model,\n",
    "                                                                            criterion=criterion,\n",
    "                                                                            dataloader=val_dataloader,\n",
    "                                                                            image_size=image_size,\n",
    "                                                                            transformer=transformer,\n",
    "                                                                            batch_first=batch_first,\n",
    "                                                                            vocabulary=vocabulary,\n",
    "                                                                            device=device\n",
    "                                                                        )\n",
    "            \n",
    "            val_loss.append(avg_val_loss)\n",
    "            val_bleu.append(val_bleu_score)\n",
    "            val_rouge.append(val_rouge_score)\n",
    "            val_meteor.append(val_meteor_score)\n",
    "\n",
    "        #printing progress\n",
    "        if num_epochs <= 10 or (num_epochs >10 and (epoch+1)%5 == 0):\n",
    "            print(f\"Epoch {epoch+1} completed\\navg training loss per batch: {total_train_loss/(idx+1)}\")\n",
    "            \n",
    "            if show_train_metrics:\n",
    "                print(f\"train bleu score:{Bleu_score}\\ntrain rouge score: {Rouge_score}\\ntrain meteor score: {Meteor_score}\\n\")\n",
    "\n",
    "            if val_dataloader != None:\n",
    "                print(f\"avg validation loss per batch: {avg_val_loss}\\nval bleu score: {val_bleu_score}\\nval rouge score: {val_rouge_score}\\nval meteor score: {val_meteor_score}\")\n",
    "\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "            \n",
    "        #saving model every x\n",
    "        if save_every != None and (epoch+1)%save_every == 0:\n",
    "            try:\n",
    "                if overwrite:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}.pt\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_{epoch+1}.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save model at epoch {epoch+1}\")\n",
    "\n",
    "        \n",
    "        #saving best model\n",
    "        if (len(val_loss) > 1) and val_loss[-1] < min(val_loss[:-1]):\n",
    "            try:\n",
    "                torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_best.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save best model\")\n",
    "        \n",
    "\n",
    "        #early stopping\n",
    "        if (len(val_loss) >= 3) and abs(val_loss[-2] - val_loss[-1]) < early_stopping_threshold and abs(val_loss[-3] - val_loss[-2]) < early_stopping_threshold:\n",
    "            print(f\"validation loss did not decrease, stopping training at epoch {epoch +1}\")\n",
    "            try:\n",
    "                if overwrite:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}.pt\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_{epoch+1}.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save model at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_graph(training_data:list, validation_data:list, y_label:str, x_label:str, ylim:list=None):\n",
    "    \"\"\"\n",
    "    Plot a line graph against training and validation data\n",
    "\n",
    "    Args:\n",
    "        training_data (list): Training data to be plotted\n",
    "        validation_data (list): Validation data to be plotted\n",
    "        y_label (str): Label of y axis\n",
    "        x_label (str): Label of x axis\n",
    "        ylim (list, optional): Range of y axis, defaults to None\n",
    "    \"\"\"\n",
    "    #plotting line graph of training data and validation data\n",
    "    plt.plot(training_data, label='Train')\n",
    "    plt.plot(validation_data, label='Validation')\n",
    "\n",
    "    #labels and title\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(f'{y_label} against {x_label}')\n",
    "    \n",
    "    if ylim != None:\n",
    "        plt.ylim(ylim)\n",
    "        \n",
    "    #show legend\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained Model Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedCNNModels(torch.nn.Module):\n",
    "    def __init__(self, model_type:str, num_unfreeze:int, num_class:int):\n",
    "        super(PreTrainedCNNModels, self).__init__()\n",
    "        \"\"\"\n",
    "        Class that contains InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit fine tuned models\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Determines which pre-trained models to use\n",
    "                              Must be: InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit\n",
    "            num_unfreeze (int): Number of layers to unfreeze and finetune\n",
    "            num_class (int): Number of output classes for the classification\n",
    "        \"\"\"\n",
    "        #selecting model type\n",
    "        if model_type == 'InceptionV3':\n",
    "            self.model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "            self.model.aux_logits = False\n",
    "\n",
    "        elif model_type == 'Resnet50':\n",
    "            self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'Resnet152':\n",
    "            self.model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'EfficientNet':\n",
    "            self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        \n",
    "        elif model_type == 'VGG16':\n",
    "            self.model = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'MaxVit':\n",
    "            self.model = models.maxvit_t(weights=models.MaxVit_T_Weights.DEFAULT)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid model type chosen. Please select one of the following\\n[InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit]\")\n",
    "\n",
    "        \n",
    "        #modifying final layer\n",
    "        if model_type in ['InceptionV3', 'Resnet50', 'Resnet152']:\n",
    "            self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_class)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model.classifier = torch.nn.Linear(self.model.classifier.in_features, num_class)\n",
    "\n",
    "        else:\n",
    "            self.model.classifier[-1] = torch.nn.Linear(self.model.classifier[-1].in_features, num_class)\n",
    "\n",
    "\n",
    "        model_paramteres = list(self.model.parameters())\n",
    "        #unfreeze last num_unfreeze layers\n",
    "        for param in model_paramteres[-num_unfreeze:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        #freeze rest of the layers\n",
    "        for param in model_paramteres[:-num_unfreeze]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finetuned InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_incept = torch.load('../models/cnn/incept_12_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (299,299)\n",
    "feature_size = 2048\n",
    "embed_size = 256 \n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 100\n",
    "save_every = 25\n",
    "model_name = \"AttnModel_Incept12FT\"\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptV3EncoderAttentionDecoder(finetuned_model=finetuned_incept,\n",
    "                                      feature_dim=feature_size,\n",
    "                                      embedding_dim=embed_size, \n",
    "                                      hidden_dim=hidden_size, \n",
    "                                      vocab_size=vocab_size, \n",
    "                                      device=device,\n",
    "                                      train_cnn=False,\n",
    "                                      drop_prob=0.3,\n",
    "                                      sample_temp=0.2).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor=train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    image_size=img_size,\n",
    "    transformer=False,\n",
    "    batch_first=True,\n",
    "    vocabulary=dataset.vocab,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_threshold = 0.001,\n",
    "    show_train_metrics=True,\n",
    "    save_every=save_every,\n",
    "    model_name=model_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "visualise_graph(training_data=train_loss,\n",
    "                validation_data=val_loss,\n",
    "                y_label='Loss',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#rouge-1\n",
    "rouge_train =  [rouge_dict['rouge1'] for rouge_dict in train_rouge]\n",
    "rouge_val =  [rouge_dict['rouge1'] for rouge_dict in val_rouge]\n",
    "visualise_graph(training_data=rouge_train,\n",
    "                validation_data=rouge_val,\n",
    "                y_label='Rouge-1 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#bleu4\n",
    "bleu4_train =  [bleu_dict['BLEU4'] for bleu_dict in train_bleu]\n",
    "bleu4_val =  [bleu_dict['BLEU4'] for bleu_dict in val_bleu]\n",
    "visualise_graph(training_data=bleu4_train,\n",
    "                validation_data=bleu4_val,\n",
    "                y_label='BLEU-4 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#meteor\n",
    "meteor_train =  [meteor_dict['meteor'] for meteor_dict in train_meteor]\n",
    "meteor_val =  [meteor_dict['meteor'] for meteor_dict in val_meteor]\n",
    "visualise_graph(training_data=meteor_train,\n",
    "                validation_data=meteor_val,\n",
    "                y_label='Meteor Score',\n",
    "                x_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into csv\n",
    "data = zip(train_loss, bleu4_train, rouge_train, meteor_train, val_loss, bleu4_val, rouge_val, meteor_val)\n",
    "dataframe = pd.DataFrame(data, columns=[\"Train Loss\",\"Train BLEU\", \"Train ROUGE\", \"Train METEOR\",\"Val Loss\", \"Val BLEU\", \"Val ROUGE\", \"Val METEOR\"])\n",
    "dataframe.to_csv(f\"../resources/training_results/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (299,299)\n",
    "feature_size = 2048\n",
    "embed_size = 256 \n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 75\n",
    "save_every = 25\n",
    "model_name = \"AttnModel_InceptPT\"\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptV3EncoderAttentionDecoder(finetuned_model=None,\n",
    "                                      feature_dim=feature_size,\n",
    "                                      embedding_dim=embed_size, \n",
    "                                      hidden_dim=hidden_size, \n",
    "                                      vocab_size=vocab_size, \n",
    "                                      device=device,\n",
    "                                      train_cnn=False,\n",
    "                                      drop_prob=0.3,\n",
    "                                      sample_temp=0.2).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor=train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    image_size=img_size,\n",
    "    transformer=False,\n",
    "    batch_first=True,\n",
    "    vocabulary=dataset.vocab,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_threshold = 0.001,\n",
    "    show_train_metrics=True,\n",
    "    save_every=save_every,\n",
    "    model_name=model_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "visualise_graph(training_data=train_loss,\n",
    "                validation_data=val_loss,\n",
    "                y_label='Loss',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#rouge-1\n",
    "rouge_train =  [rouge_dict['rouge1'] for rouge_dict in train_rouge]\n",
    "rouge_val =  [rouge_dict['rouge1'] for rouge_dict in val_rouge]\n",
    "visualise_graph(training_data=rouge_train,\n",
    "                validation_data=rouge_val,\n",
    "                y_label='Rouge-1 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#bleu4\n",
    "bleu4_train =  [bleu_dict['BLEU4'] for bleu_dict in train_bleu]\n",
    "bleu4_val =  [bleu_dict['BLEU4'] for bleu_dict in val_bleu]\n",
    "visualise_graph(training_data=bleu4_train,\n",
    "                validation_data=bleu4_val,\n",
    "                y_label='BLEU-4 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#meteor\n",
    "meteor_train =  [meteor_dict['meteor'] for meteor_dict in train_meteor]\n",
    "meteor_val =  [meteor_dict['meteor'] for meteor_dict in val_meteor]\n",
    "visualise_graph(training_data=meteor_train,\n",
    "                validation_data=meteor_val,\n",
    "                y_label='Meteor Score',\n",
    "                x_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into csv\n",
    "data = zip(train_loss, bleu4_train, rouge_train, meteor_train, val_loss, bleu4_val, rouge_val, meteor_val)\n",
    "dataframe = pd.DataFrame(data, columns=[\"Train Loss\",\"Train BLEU\", \"Train ROUGE\", \"Train METEOR\",\"Val Loss\", \"Val BLEU\", \"Val ROUGE\", \"Val METEOR\"])\n",
    "dataframe.to_csv(f\"../resources/training_results/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finetuned resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_resnet = torch.load('../models/cnn/resnet_20_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (224,224)\n",
    "feature_size = 2048\n",
    "embed_size = 256 \n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 75\n",
    "save_every = 25\n",
    "model_name = \"AttnModel_Resnet20FT\"\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetEncoderAttentionDecoder(finetuned_model=finetuned_resnet,\n",
    "                                      feature_dim=feature_size,\n",
    "                                      embedding_dim=embed_size, \n",
    "                                      hidden_dim=hidden_size, \n",
    "                                      vocab_size=vocab_size, \n",
    "                                      device=device,\n",
    "                                      train_cnn=False,\n",
    "                                      drop_prob=0.3,\n",
    "                                      sample_temp=0.2).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor=train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    image_size=img_size,\n",
    "    transformer=False,\n",
    "    batch_first=True,\n",
    "    vocabulary=dataset.vocab,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_threshold = 0.001,\n",
    "    show_train_metrics=True,\n",
    "    save_every=save_every,\n",
    "    model_name=model_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "visualise_graph(training_data=train_loss,\n",
    "                validation_data=val_loss,\n",
    "                y_label='Loss',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#rouge-1\n",
    "rouge_train =  [rouge_dict['rouge1'] for rouge_dict in train_rouge]\n",
    "rouge_val =  [rouge_dict['rouge1'] for rouge_dict in val_rouge]\n",
    "visualise_graph(training_data=rouge_train,\n",
    "                validation_data=rouge_val,\n",
    "                y_label='Rouge-1 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#bleu4\n",
    "bleu4_train =  [bleu_dict['BLEU4'] for bleu_dict in train_bleu]\n",
    "bleu4_val =  [bleu_dict['BLEU4'] for bleu_dict in val_bleu]\n",
    "visualise_graph(training_data=bleu4_train,\n",
    "                validation_data=bleu4_val,\n",
    "                y_label='BLEU-4 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#meteor\n",
    "meteor_train =  [meteor_dict['meteor'] for meteor_dict in train_meteor]\n",
    "meteor_val =  [meteor_dict['meteor'] for meteor_dict in val_meteor]\n",
    "visualise_graph(training_data=meteor_train,\n",
    "                validation_data=meteor_val,\n",
    "                y_label='Meteor Score',\n",
    "                x_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into csv\n",
    "data = zip(train_loss, bleu4_train, rouge_train, meteor_train, val_loss, bleu4_val, rouge_val, meteor_val)\n",
    "dataframe = pd.DataFrame(data, columns=[\"Train Loss\",\"Train BLEU\", \"Train ROUGE\", \"Train METEOR\",\"Val Loss\", \"Val BLEU\", \"Val ROUGE\", \"Val METEOR\"])\n",
    "dataframe.to_csv(f\"../resources/training_results/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (224,224)\n",
    "feature_size = 2048\n",
    "embed_size = 256 \n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 75\n",
    "save_every = 5\n",
    "model_name = \"AttnModel_ResnetPT\"\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetEncoderAttentionDecoder(finetuned_model=None,\n",
    "                                      feature_dim=feature_size,\n",
    "                                      embedding_dim=embed_size, \n",
    "                                      hidden_dim=hidden_size, \n",
    "                                      vocab_size=vocab_size, \n",
    "                                      device=device,\n",
    "                                      train_cnn=False,\n",
    "                                      drop_prob=0.3,\n",
    "                                      sample_temp=0.2).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor= train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    image_size=img_size,\n",
    "    transformer=False,\n",
    "    batch_first=True,\n",
    "    vocabulary=dataset.vocab,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_threshold = 0.001,\n",
    "    show_train_metrics=True,\n",
    "    save_every=save_every,\n",
    "    model_name=model_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "visualise_graph(training_data=train_loss,\n",
    "                validation_data=val_loss,\n",
    "                y_label='Loss',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#rouge-1\n",
    "rouge_train =  [rouge_dict['rouge1'] for rouge_dict in train_rouge]\n",
    "rouge_val =  [rouge_dict['rouge1'] for rouge_dict in val_rouge]\n",
    "visualise_graph(training_data=rouge_train,\n",
    "                validation_data=rouge_val,\n",
    "                y_label='Rouge-1 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#bleu4\n",
    "bleu4_train =  [bleu_dict['BLEU4'] for bleu_dict in train_bleu]\n",
    "bleu4_val =  [bleu_dict['BLEU4'] for bleu_dict in val_bleu]\n",
    "visualise_graph(training_data=bleu4_train,\n",
    "                validation_data=bleu4_val,\n",
    "                y_label='BLEU-4 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#meteor\n",
    "meteor_train =  [meteor_dict['meteor'] for meteor_dict in train_meteor]\n",
    "meteor_val =  [meteor_dict['meteor'] for meteor_dict in val_meteor]\n",
    "visualise_graph(training_data=meteor_train,\n",
    "                validation_data=meteor_val,\n",
    "                y_label='Meteor Score',\n",
    "                x_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into csv\n",
    "data = zip(train_loss, bleu4_train, rouge_train, meteor_train, val_loss, bleu4_val, rouge_val, meteor_val)\n",
    "dataframe = pd.DataFrame(data, columns=[\"Train Loss\",\"Train BLEU\", \"Train ROUGE\", \"Train METEOR\",\"Val Loss\", \"Val BLEU\", \"Val ROUGE\", \"Val METEOR\"])\n",
    "dataframe.to_csv(f\"../resources/training_results/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_size = 2048\n",
    "embed_size = 256 \n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_incept = torch.load('../models/cnn/incept_12_best.pt')\n",
    "test_model1 = InceptV3EncoderAttentionDecoder(finetuned_model=finetuned_incept,\n",
    "                                feature_dim=feature_size,\n",
    "                                embedding_dim=embed_size, \n",
    "                                hidden_dim=hidden_size, \n",
    "                                vocab_size=vocab_size, \n",
    "                                device=device,\n",
    "                                train_cnn=False,\n",
    "                                drop_prob=0.3,\n",
    "                                sample_temp=0.2).to(device)\n",
    "test_criterion1 = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "test_model1.load_state_dict(torch.load('../models/image_captioning/AttnModel_Incept12FT.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model2 = InceptV3EncoderAttentionDecoder(finetuned_model=None,\n",
    "                                feature_dim=feature_size,\n",
    "                                embedding_dim=embed_size, \n",
    "                                hidden_dim=hidden_size, \n",
    "                                vocab_size=vocab_size, \n",
    "                                device=device,\n",
    "                                train_cnn=False,\n",
    "                                drop_prob=0.3,\n",
    "                                sample_temp=0.2).to(device)\n",
    "test_criterion2 = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "test_model2.load_state_dict(torch.load('../models/image_captioning/AttnModel_InceptPT.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_resnet = torch.load('../models/cnn/resnet_20_best.pt')\n",
    "test_model3 = ResNetEncoderAttentionDecoder(finetuned_model=finetuned_resnet,\n",
    "                                      feature_dim=feature_size,\n",
    "                                      embedding_dim=embed_size, \n",
    "                                      hidden_dim=hidden_size, \n",
    "                                      vocab_size=vocab_size, \n",
    "                                      device=device,\n",
    "                                      train_cnn=False,\n",
    "                                      drop_prob=0.3,\n",
    "                                      sample_temp=0.2).to(device)\n",
    "test_criterion3 = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "test_model3.load_state_dict(torch.load('../models/image_captioning/AttnModel_Resnet20FT.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model4 = ResNetEncoderAttentionDecoder(finetuned_model=None,\n",
    "                                      feature_dim=feature_size,\n",
    "                                      embedding_dim=embed_size, \n",
    "                                      hidden_dim=hidden_size, \n",
    "                                      vocab_size=vocab_size, \n",
    "                                      device=device,\n",
    "                                      train_cnn=False,\n",
    "                                      drop_prob=0.3,\n",
    "                                      sample_temp=0.2).to(device)\n",
    "test_criterion4 = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "test_model4.load_state_dict(torch.load('../models/image_captioning/AttnModel_ResnetPT.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [[test_model1, test_criterion1, (299,299)],\n",
    "              [test_model2, test_criterion2, (299,299)],\n",
    "              [test_model3, test_criterion3, (224,224)],\n",
    "              [test_model4, test_criterion4, (224,224)]]\n",
    "\n",
    "\n",
    "for idx, models in enumerate(all_models):\n",
    "    test_loss, test_bleu, test_rouge, test_meteor = eval(model=models[0],\n",
    "                                criterion=models[1],\n",
    "                                dataloader=test_dataloader,\n",
    "                                image_size=models[2],\n",
    "                                transformer=False,\n",
    "                                batch_first=True,\n",
    "                                vocabulary=test_dataset.vocab,\n",
    "                                device=device\n",
    "                                )\n",
    "    \n",
    "    print(f\"Test Model {idx}\\nTest Loss: {test_loss}\\nTest BLEU:{test_bleu}\\nTest Rouge:{test_rouge}\\nTest Meteor:{test_meteor}\\n----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Application of model inference & multiple model inference\n",
    "NOTE: the generate captions function should belong to your model class and have your own implementation depending on your model architecture\n",
    "<br/><br/>\n",
    "For easy standarisation you please make your `caption_image` function in your class have the following:\n",
    "1. Inputs: image, vocabulary, device, max_length\n",
    "2. Outputs: string prediction, attention (or None) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming mean and std are defined as follows:\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def unnormalize(image:np.array, mean:np.array, std:np.array):\n",
    "    \"\"\"\n",
    "    Function to unnormalize an image given its mean and std\n",
    "    \n",
    "    Args: \n",
    "        image (np.array): Numpy array of the image\n",
    "        mean (np.array): Numpy array of the mean \n",
    "        std (np.array): Numpy array of the std\n",
    "\n",
    "    Returns:\n",
    "        Unnormalised numpy array of the image\n",
    "    \"\"\"\n",
    "\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)    # for inplace operations\n",
    "    return image\n",
    "\n",
    "\n",
    "def caption_image(model, \n",
    "                  dataloader, \n",
    "                  image_size:tuple,\n",
    "                  vocabulary:Vocabulary, \n",
    "                  device:str, \n",
    "                  mean:np.array=np.array([0.485, 0.456, 0.406]), \n",
    "                  std:np.array=np.array([0.229, 0.224, 0.225]), \n",
    "                  num_batches:int=1, \n",
    "                  num_images:int=5, \n",
    "                  max_length:int=50, \n",
    "                  show_plot:bool=False):\n",
    "    \"\"\"\n",
    "    Function to generate model predictions from a dataloader\n",
    "\n",
    "    Arg:\n",
    "        model: model to general model prediction (ensure that your model has the function caption_image)\n",
    "        dataloader: dataset to generate prediction\n",
    "        image_size (tuple): image size of images for the model\n",
    "        vocabulary (Vocabulary): dataset vocabulary\n",
    "        device (str): cpu or cuda,\n",
    "        mean (np.array): Numpy array of the mean used for normalisation\n",
    "        std (np.array): Numpy array of the std used for normalisation\n",
    "        num_batches (int, optional): how many batches iterating from dataloader, defaults to 1\n",
    "        num_image (int, optional): how many images per batch to generate model prediction, defaults to 5\n",
    "        max_length (int, optional): maximum length of generated captions, defaults to 50\n",
    "        show_plot (bool, optional): show the image and generated captions in a plot, defaults to False\n",
    "    \n",
    "    Returns:\n",
    "        all_predictions (dict): Dictionary containing the list of all generated captions and actual captions\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    #dictionary containing all the generated predictions and actual predictions\n",
    "    all_predictions = {'Predicted': [], 'Possible Captions': []}\n",
    "\n",
    "    #iterate for num of batches we are testing\n",
    "    for j in range(num_batches):\n",
    "        #load images from dataloader\n",
    "        features, annotations, all_annotations = next(iter(dataloader))\n",
    "\n",
    "        #take first k from batch\n",
    "        for i in range(num_images):\n",
    "            features = torch.nn.functional.interpolate(features, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            image = features[i].unsqueeze(0).to(device)\n",
    "            \n",
    "            #generate captions from model\n",
    "            generated_caption, attention = model.caption_image(image, vocabulary, device, max_length=max_length)\n",
    "            \n",
    "            #plot image and captions\n",
    "            if show_plot:\n",
    "                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                img = features[i].squeeze()\n",
    "                img = unnormalize(img, mean, std)  # Unnormalize the image\n",
    "                img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'Model Prediction:\\n{\" \".join(generated_caption[1:-1])}\\n\\nAll Possible Predictions:\\n' + \"\\n\".join(all_annotations[i]), loc='left')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            all_predictions['Predicted'].append(generated_caption)\n",
    "            all_predictions['Possible Captions'].append(all_annotations[i])\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def multiple_model_captions(\n",
    "        model_list:list[str, tuple],\n",
    "        dataloader, \n",
    "        vocabulary:Vocabulary, \n",
    "        device:str, \n",
    "        mean:np.array=np.array([0.485, 0.456, 0.406]), \n",
    "        std:np.array=np.array([0.229, 0.224, 0.225]), \n",
    "        num_batches:int=1, \n",
    "        num_images:int=5, \n",
    "        max_length:int=50, \n",
    "        show_plot:bool=False):\n",
    "    \"\"\"\n",
    "    Function to generate model predictions for multiple models from the same dataloader\n",
    "\n",
    "    Arg:\n",
    "        model_list (list[str, tuple]): list of [model, img_size] to general model prediction (ensure that your model has the function caption_image)\n",
    "            eg. [[model1, (224,224)], [model2, (256,256)]]\n",
    "        dataloader: dataset to generate prediction\n",
    "        vocabulary (Vocabulary): dataset vocabulary\n",
    "        device (str): cpu or cuda,\n",
    "        mean (np.array): Numpy array of the mean used for normalisation\n",
    "        std (np.array): Numpy array of the std used for normalisation\n",
    "        num_batches (int, optional): how many batches iterating from dataloader, defaults to 1\n",
    "        num_image (int, optional): how many images per batch to generate model prediction, defaults to 5\n",
    "        max_length (int, optional): maximum length of generated captions, defaults to 50\n",
    "        show_plot (bool, optional): show the image and generated captions in a plot, defaults to False\n",
    "    \n",
    "    Returns:\n",
    "        all_predictions (dict): Dictionary containing the dictionary of all generated captions for each model and list of actual captions\n",
    "    \n",
    "    \"\"\"\n",
    "    for model, img_size in model_list:\n",
    "        model.eval()\n",
    "\n",
    "    #dictionary containing all the generated predictions and actual predictions\n",
    "    all_predictions = {'Predicted': {}, 'Possible Captions': []}\n",
    "\n",
    "    #iterate for num of batches we are testing\n",
    "    for j in range(num_batches):\n",
    "        #load images from dataloader\n",
    "        features, annotations, all_annotations = next(iter(dataloader))\n",
    "        \n",
    "        #take first k from batch\n",
    "        for i in range(num_images):\n",
    "            image = features[i].unsqueeze(0).to(device)\n",
    "            all_captions = []\n",
    "\n",
    "            for idx, (model, img_size) in enumerate(model_list):\n",
    "                #resize image\n",
    "                image = torch.nn.functional.interpolate(image, size=img_size, mode='bilinear') #resize image\n",
    "                #generate captions from model\n",
    "                generated_caption, attention = model.caption_image(image, vocabulary, device, max_length=max_length)\n",
    "                all_captions.append(\" \".join(generated_caption[1:-1]))\n",
    "                model_predictions = all_predictions['Predicted'].get(f\"model_{idx}\", [])\n",
    "                model_predictions.append(all_captions)\n",
    "                all_predictions['Predicted'][f\"model_{idx}\"] = model_predictions\n",
    "\n",
    "            #plot image and captions\n",
    "            if show_plot:\n",
    "                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                img = features[i].squeeze()\n",
    "                img = unnormalize(img, mean, std)  # Unnormalize the image\n",
    "                img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "\n",
    "                pred = '\\n'.join(all_captions)\n",
    "                annotation = '\\n'.join(all_annotations[i])\n",
    "\n",
    "                ax.set_title(f'All Model Predictions:\\n{pred}\\n\\nAll Possible Predictions:\\n{annotation}', loc='left')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            all_predictions['Possible Captions'].append(all_annotations[i])\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [[test_model1, (299,299)],\n",
    "              [test_model2, (299,299)],\n",
    "              [test_model3, (224,224)],\n",
    "              [test_model4, (224,224)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = multiple_model_captions(\n",
    "    model_list=model_list,\n",
    "    dataloader=test_dataloader,\n",
    "    vocabulary= test_dataset.vocab,\n",
    "    device=device,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    num_batches=10,\n",
    "    num_images=1,\n",
    "    max_length=50,\n",
    "    show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = caption_image(\n",
    "    model=test_model1,\n",
    "    dataloader=test_dataloader,\n",
    "    image_size=(299,299),\n",
    "    vocabulary=test_dataset.vocab,\n",
    "    device=device,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    num_batches=1,\n",
    "    num_images=10,\n",
    "    max_length=50,\n",
    "    show_plot=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
