{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PlayWrite Pipeline\n",
    "\n",
    "What to expect:\n",
    "1. Caption any image uploads with image captioning model\n",
    "2. Generation of music prompt through Llama2-7b\n",
    "3. Music generation through Mustango\n",
    "\n",
    "Note: Llama7b Requires at least 16gb of vram to be loaded in half precision (Bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pickle\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import IPython\n",
    "\n",
    "import gc\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "  \"\"\"\n",
    "  Class to convert the captions to index sequential tensors\n",
    "\n",
    "  Args:\n",
    "    freq_threshold (int, optional): How many times a word has to appear in dataset before it can be added to the vocabulary. Defaults to 2\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, freq_threshold:int=2):\n",
    "    self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"} #index to sentence\n",
    "    self.stoi = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\": 2, \"<UNK>\":3} #sentence to index\n",
    "    self.freq_threshold = freq_threshold #threshold for adding a word to the vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.itos)\n",
    "\n",
    "  @staticmethod\n",
    "  def tokenizer_eng(text):\n",
    "    #convert sentence to list of words\n",
    "    return [tok.text.lower() for tok in word_tokenizer.tokenizer(text)] #convert sentence to words\n",
    "\n",
    "\n",
    "  def build_vocabulary(self, sentence_list):\n",
    "    frequencies = {}\n",
    "    idx = 4 #0-3 are for special tokens\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "      for word in self.tokenizer_eng(sentence): #convert sentence to words\n",
    "        if word not in frequencies:\n",
    "          frequencies[word] = 1\n",
    "        else:\n",
    "          frequencies[word] += 1\n",
    "\n",
    "        if frequencies[word] == self.freq_threshold: #once met freq_threshold, add to vocab list\n",
    "          self.stoi[word] = idx\n",
    "          self.itos[idx] = word\n",
    "          idx += 1\n",
    "\n",
    "  def numericalize(self, text):\n",
    "    tokenized_text = self.tokenizer_eng(text) #convert annnotations to labels by converting each word to the index inside the vocab, else UNK tag\n",
    "    return [\n",
    "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "        for token in tokenized_text\n",
    "    ]\n",
    "  \n",
    "\n",
    "class InceptionV3EncoderCNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    InceptionV3 CNN Encoder Model, feature extraction layer (mixed_7c) is always trainable\n",
    "\n",
    "    Args:\n",
    "        finetuned_model: Finetuned inceptionV3 model, else None\n",
    "        train_cnn (bool, optional): Determines if the entire CNN model will be unfreeze and trained during the training. Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(self, finetuned_model, train_cnn:bool=False):\n",
    "        super(InceptionV3EncoderCNN, self).__init__()\n",
    "        if finetuned_model != None:\n",
    "            self.inception = list(finetuned_model.children())[0]\n",
    "        \n",
    "        else:\n",
    "            self.inception = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "        self.inception.aux_logits = False\n",
    "        \n",
    "        #Remove last classification layer\n",
    "        self.inception.fc = torch.nn.Identity()\n",
    "\n",
    "        #Variable that will hold the features\n",
    "        self.features = None\n",
    "        \n",
    "        #Register the hook to capture features at output of last CNN layer\n",
    "        self.inception.Mixed_7c.register_forward_hook(self.capture_features_hook)\n",
    "\n",
    "        #Train the feature map, the rest depends on train_CNN\n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if 'Mixed_7c' in name:\n",
    "                param.requires_grad_(True)\n",
    "            else:\n",
    "                param.requires_grad_(train_cnn)\n",
    "\n",
    "\n",
    "    def capture_features_hook(self, module, input, output):\n",
    "        self.features = output #update feature \n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Take images and return feature maps of size (batch, height*width)\n",
    "        \"\"\"\n",
    "        _ = self.inception(images)  #Pass through the inception network\n",
    "        batch, feature_maps, size_1, size_2 = self.features.size()  #self.features contain the feature map of size (batch size, 2048, 8,8)\n",
    "        features = self.features.permute(0, 2, 3, 1)\n",
    "        features = features.view(batch, size_1*size_2, feature_maps) #resize to (batch size, h*w, feature_maps)\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class BahdanauAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Attention Module\n",
    "\n",
    "    Args:\n",
    "        feature_dim (int): Dimension of feature maps (h*w)\n",
    "        hidden_dim (int): Dimension of hidden states\n",
    "        output_dim (int, optional): Dimension of output, default to 1\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim:int, hidden_dim:int, output_dim:int = 1):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "         # fully-connected layer to learn first weight matrix Wa\n",
    "        self.W_a = torch.nn.Linear(feature_dim, hidden_dim)\n",
    "        # fully-connected layer to learn the second weight matrix Ua\n",
    "        self.U_a = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        # fully-connected layer to produce score (output), learning weight matrix va\n",
    "        self.v_a = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, features, hidden_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: image features from Encoder\n",
    "            hidden_state: hidden state output for Decoder\n",
    "\n",
    "        Returns:\n",
    "            context: context vector with size (1,2048)\n",
    "            atten_weights: probabilities of feature relevance \n",
    "        \"\"\"\n",
    "        #add additional dimension to a hidden (required for summation) \n",
    "        hidden_state = hidden_state.unsqueeze(1) #(batch size, 1, seq length)\n",
    "\n",
    "        atten_1 = self.W_a(features) #(batch size, h*w, hidden_dim)\n",
    "        atten_2 = self.U_a(hidden_state) #(batch size, 1, hidden_dim)\n",
    "\n",
    "        #apply tangent to combine result from 2 fc layers\n",
    "        atten_tan = torch.tanh(atten_1+atten_2)\n",
    "        atten_score = self.v_a(atten_tan) #(batch size, hidden_dim)\n",
    "        atten_weight = torch.nn.functional.softmax(atten_score, dim = 1) #get softmax probablilities\n",
    "\n",
    "        #multiply each vector with its softmax score and sum to get attention context vector\n",
    "        context = torch.sum(atten_weight * features,  dim = 1) #size of context equals to a number of feature maps\n",
    "        atten_weight = atten_weight.squeeze(dim=2)\n",
    "        \n",
    "        return context, atten_weight\n",
    "\n",
    "\n",
    "class DecoderRNN(torch.nn.Module):\n",
    "     \"\"\"\n",
    "     LSTM decoder model\n",
    "\n",
    "     Args:\n",
    "          feature_dim (int): Feature Map dimension (h*w)\n",
    "          embed_size (int): Embedding dimension to embed words\n",
    "          hidden_size (int): Hidden state dimension for LSTM\n",
    "          vocab_size (int): Total number of unique vocab\n",
    "          drop_prob (float, optional): Dropout layer probability, deafults to 0.5\n",
    "          sample_temp (float, optional): Scale outputs before softmax to allow the model to be more picky as the differences are exaggerated. Defaults to 0.5\n",
    "     \"\"\"\n",
    "     def __init__(self, feature_dim:int, embedding_dim:int, hidden_dim:int, vocab_size:int, drop_prob:float=0.5, sample_temp:float=0.5):\n",
    "          super(DecoderRNN, self).__init__()\n",
    "          \n",
    "          self.feature_dim = feature_dim\n",
    "          self.embedding_dim = embedding_dim\n",
    "          self.hidden_dim = hidden_dim\n",
    "          self.vocab_size = vocab_size\n",
    "          self.sample_temp = sample_temp #scale the outputs b4 softmax\n",
    "\n",
    "          #layers\n",
    "\n",
    "          #embedding layer that turns words into index \n",
    "          self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "          #lstm layer that takes in feature + embedding (image + caption) and output hidden_dim\n",
    "          self.lstm = torch.nn.LSTMCell(embedding_dim + feature_dim, hidden_dim)\n",
    "          #fc linear layer that predicts next word\n",
    "          self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "          #attention layer\n",
    "          self.attention = BahdanauAttention(feature_dim, hidden_dim)\n",
    "          #dropout layer\n",
    "          self.drop = torch.nn.Dropout(p=drop_prob)\n",
    "          #initialisation of fully-connected layers\n",
    "          self.init_h = torch.nn.Linear(feature_dim, hidden_dim) #initiialising hidden state and cell memory using avg of feature\n",
    "          self.init_c = torch.nn.Linear(feature_dim, hidden_dim)\n",
    "\n",
    "     def init_hidden(self, features):\n",
    "          \"\"\"\n",
    "          Initializes hidden state and cell memory using average feature vector\n",
    "          Args:\n",
    "               features: feature map of the image\n",
    "          Returns:\n",
    "               h0: initial hidden state (short-term memory)\n",
    "               c0: initial cell state (long-term memory)\n",
    "          \"\"\"\n",
    "          mean_annotations = torch.mean(features, dim = 1) #getting average of the features\n",
    "          h0 = self.init_h(mean_annotations)\n",
    "          c0 = self.init_c(mean_annotations)\n",
    "          return h0, c0\n",
    "\n",
    "     def forward(self, features, captions, device:str, sample_prob:float=0.2):\n",
    "          \"\"\"\n",
    "          Args:\n",
    "               features: feature map of image\n",
    "               captions: true caption of image\n",
    "               device (str): cuda or cpu\n",
    "               sample_prob (float, optional): Probability for auto-regressive RNN where they train on RNN output rather than true layer, defaults to 0.2\n",
    "\n",
    "          \"\"\"\n",
    "          embed = self.embeddings(captions)\n",
    "          h,c = self.init_hidden(features)\n",
    "          batch_size = captions.size(0) #captions: (batch size, seq length)\n",
    "          seq_len = captions.size(1) \n",
    "          feature_size = features.size(1) #features: (batch size, size, 2048)\n",
    "\n",
    "          #storage of outputs and attention weights of lstm\n",
    "          outputs = torch.zeros(batch_size, seq_len, self.vocab_size).to(device)\n",
    "          atten_weights = torch.zeros(batch_size, seq_len, feature_size).to(device)\n",
    "\n",
    "          #scheduled sampling for training, using the models output to train instead of using the true output\n",
    "          #autoregressive RNN training, only when length of seq > 1 (cannot be first word)\n",
    "          for t in range(seq_len):\n",
    "               s_prob = 0.0 if t==0 else sample_prob\n",
    "               use_sampling = np.random.random() < s_prob\n",
    "\n",
    "               if not use_sampling: #no sampling\n",
    "                    word_embeddings = embed[:, t, :] #embedding until word t\n",
    "               \n",
    "               context, atten_weight = self.attention(features,h)\n",
    "               inputs = torch.cat([word_embeddings, context], 1) #embed captions and features for next lstm state\n",
    "               h, c = self.lstm(inputs, (h,c)) #pass through lstm\n",
    "               output = self.fc(self.drop(h))\n",
    "               \n",
    "               if use_sampling: #using predicted word instead of true output\n",
    "                    scaled_output = output/self.sample_temp #using scaling temp to amplify the values\n",
    "                    #this way softmax will have a larger difference in values\n",
    "                    #makes the model more selective of whats its picking \n",
    "                    \n",
    "                    scoring = torch.nn.functional.log_softmax(scaled_output, dim=1)\n",
    "                    top_idx = scoring.topk(1)[1]\n",
    "                    word_embeddings = self.embeddings(top_idx).squeeze(1) #update word embeddings with predicted instead of actual\n",
    "               \n",
    "               #update results\n",
    "               outputs[:,t,:] = output\n",
    "               atten_weights[:, t, :] = atten_weight\n",
    "\n",
    "          return outputs, atten_weights\n",
    "\n",
    "\n",
    "class InceptV3EncoderAttentionDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    InceptionV3 Encoder with Attention and LSTM Decoder\n",
    "\n",
    "    Args:\n",
    "        finetuned_model: Finetuned InceptionV3 model, else None\n",
    "        feature_dim (int): Feature Map dimension (h*w)\n",
    "        embedding_dim (int): Embedding dimension to embed words\n",
    "        hidden_dim (int): Hidden state dimension for LSTM\n",
    "        vocab_size (int): Total number of unique vocab\n",
    "        device (str): cuda or cpu\n",
    "        train_cnn (boolean, optional): Determines if inceptionCNN model is unfreezed. Defaults to False.\n",
    "        drop_prob (float, optional): Dropout layer probability. Defaults to 0.5.\n",
    "        sample_temp (float, optional): Scale outputs before softmax to allow the model to be more picky as the differences are exaggerated. Defaults to 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, finetuned_model, feature_dim:int, embedding_dim:int, hidden_dim:int, vocab_size:int, device:str, train_cnn:bool=False, drop_prob:float=0.5, sample_temp:float=0.5):\n",
    "        super(InceptV3EncoderAttentionDecoder, self).__init__()\n",
    "        self.encoder = InceptionV3EncoderCNN(finetuned_model, train_cnn)\n",
    "        self.decoder= DecoderRNN(feature_dim, embedding_dim, hidden_dim, vocab_size, drop_prob, sample_temp)\n",
    "        self.sample_temp = sample_temp\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, image, captions):\n",
    "        features = self.encoder(image)\n",
    "        outputs, atten_weights = self.decoder(features, captions, self.device, self.sample_temp)\n",
    "        return outputs, atten_weights\n",
    "    \n",
    "\n",
    "    #for inference\n",
    "    def caption_image(self, image, vocabulary:Vocabulary, device:str, max_length:int=50):\n",
    "        \"\"\"\n",
    "        Generate caption using a greedy algorithm based on image input\n",
    "\n",
    "        Args:\n",
    "            image: image input\n",
    "            vocabulary (Vocabulary): Vocabulary to decode predictions\n",
    "            device (str): cuda or cpu\n",
    "            max_length (int, optional): Max length of generated captions. Defaults to 50.\n",
    "\n",
    "        Returns:\n",
    "            captions: string caption in a list\n",
    "            atten_weights: probabilities of feature relevance \n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "\n",
    "        result_caption = []\n",
    "        result_weights = []\n",
    "\n",
    "        with torch.no_grad(): #no training\n",
    "            input_word = torch.tensor(1).unsqueeze(0).to(device)\n",
    "            result_caption.append(1)\n",
    "            features = self.encoder(image)\n",
    "            h, c = self.decoder.init_hidden(features)\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                embedded_word = self.decoder.embeddings(input_word)\n",
    "                context, atten_weight = self.decoder.attention(features, h)\n",
    "                # input_concat shape at time step t = (batch, embedding_dim + context size)\n",
    "                input_concat = torch.cat([embedded_word, context],  dim = 1)\n",
    "                h, c = self.decoder.lstm(input_concat, (h,c))\n",
    "                h = self.decoder.drop(h)\n",
    "                output = self.decoder.fc(h) \n",
    "                scoring = torch.nn.functional.log_softmax(output, dim=1)\n",
    "                top_idx = scoring[0].topk(1)[1]\n",
    "                result_caption.append(top_idx.item())\n",
    "                result_weights.append(atten_weight)\n",
    "                input_word = top_idx\n",
    "\n",
    "                if (len(result_caption) >= max_length or vocabulary.itos[input_word.item()] == \"<EOS>\"):\n",
    "                    break\n",
    "\n",
    "            return [vocabulary.itos[idx] for idx in result_caption], result_weights\n",
    "\n",
    "\n",
    "class PreTrainedCNNModels(torch.nn.Module):\n",
    "    def __init__(self, model_type:str, num_unfreeze:int, num_class:int):\n",
    "        super(PreTrainedCNNModels, self).__init__()\n",
    "        \"\"\"\n",
    "        Class that contains InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit fine tuned models\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Determines which pre-trained models to use\n",
    "                              Must be: InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit\n",
    "            num_unfreeze (int): Number of layers to unfreeze and finetune\n",
    "            num_class (int): Number of output classes for the classification\n",
    "        \"\"\"\n",
    "        #selecting model type\n",
    "        if model_type == 'InceptionV3':\n",
    "            self.model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "            self.model.aux_logits = False\n",
    "\n",
    "        elif model_type == 'Resnet50':\n",
    "            self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'Resnet152':\n",
    "            self.model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'EfficientNet':\n",
    "            self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        \n",
    "        elif model_type == 'VGG16':\n",
    "            self.model = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'MaxVit':\n",
    "            self.model = models.maxvit_t(weights=models.MaxVit_T_Weights.DEFAULT)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid model type chosen. Please select one of the following\\n[InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit]\")\n",
    "\n",
    "        \n",
    "        #modifying final layer\n",
    "        if model_type in ['InceptionV3', 'Resnet50', 'Resnet152']:\n",
    "            self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_class)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model.classifier = torch.nn.Linear(self.model.classifier.in_features, num_class)\n",
    "\n",
    "        else:\n",
    "            self.model.classifier[-1] = torch.nn.Linear(self.model.classifier[-1].in_features, num_class)\n",
    "\n",
    "\n",
    "        model_paramteres = list(self.model.parameters())\n",
    "        #unfreeze last num_unfreeze layers\n",
    "        for param in model_paramteres[-num_unfreeze:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        #freeze rest of the layers\n",
    "        for param in model_paramteres[:-num_unfreeze]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.model(images)\n",
    "    \n",
    "\n",
    "class PromptTemplate:\n",
    "    system_prompt = None\n",
    "    user_messages = []\n",
    "    model_replies = []\n",
    "\n",
    "    def __init__(self, system_prompt=None):\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def add_user_message(self, message: str, return_prompt=True):\n",
    "        self.user_messages.append(message)\n",
    "        if return_prompt:\n",
    "            return self.build_prompt()\n",
    "\n",
    "    def add_model_reply(self, reply: str, includes_history=True, return_reply=True):\n",
    "        reply_ = reply.replace(self.build_prompt(), \"\") if includes_history else reply\n",
    "        self.model_replies.append(reply_)\n",
    "        if len(self.user_messages) != len(self.model_replies):\n",
    "            raise ValueError(\n",
    "                \"Number of user messages does not equal number of system replies.\"\n",
    "            )\n",
    "        if return_reply:\n",
    "            return reply_\n",
    "\n",
    "    def get_user_messages(self, strip=True):\n",
    "        return [x.strip() for x in self.user_messages] if strip else self.user_messages\n",
    "\n",
    "    def get_model_replies(self, strip=True):\n",
    "        return [x.strip() for x in self.model_replies] if strip else self.model_replies\n",
    "\n",
    "    def clear_chat_history(self):\n",
    "        self.user_messages.clear()\n",
    "        self.model_replies.clear()\n",
    "\n",
    "    def build_prompt(self):\n",
    "        if self.user_messages == [] and self.model_replies == []:\n",
    "            return f\"<s>[INST] <<SYS>>\\n{self.system_prompt}\\n<</SYS>> [/INST]</s>\"\n",
    "        \n",
    "        elif len(self.user_messages) != len(self.model_replies) + 1:\n",
    "            raise ValueError(\n",
    "                \"Error: Expected len(user_messages) = len(model_replies) + 1. Add a new user message!\"\n",
    "            )\n",
    "\n",
    "        if self.system_prompt is not None:\n",
    "            SYS = f\"[INST] <<SYS>>\\n{self.system_prompt}\\n<</SYS>>\"\n",
    "        else:\n",
    "            SYS = \"\"\n",
    "\n",
    "        CONVO = \"\"\n",
    "        SYS = \"<s>\" + SYS\n",
    "        for i in range(len(self.user_messages) - 1):\n",
    "            user_message, model_reply = self.user_messages[i], self.model_replies[i]\n",
    "            conversation_ = f\"{user_message} [/INST] {model_reply} </s>\"\n",
    "            if i != 0:\n",
    "                conversation_ = \"[INST] \" + conversation_\n",
    "            CONVO += conversation_\n",
    "\n",
    "        CONVO += f\"[INST] {self.user_messages[-1]} [/INST]\"\n",
    "\n",
    "        return SYS + CONVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../models/mustango\")\n",
    "from mustango import Mustango\n",
    "os.chdir('../../notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class playWrite():\n",
    "    def __init__(self, \n",
    "                 device:str,\n",
    "                 vocab_path:str,\n",
    "                 image_caption_path:str,\n",
    "                 hg_access_token:str=None,\n",
    "                 llama_model_path: str=None,\n",
    "                 llama_tokenizer_path:str=None\n",
    "                 ):\n",
    "        \n",
    "        self.vocab = self._load_vocab(vocab_path)\n",
    "        self.image_caption = self._load_image_caption(image_caption_path, device)\n",
    "        self.llama_model, self.llama_tokenizer = self._load_llama(hg_access_token, llama_model_path, llama_tokenizer_path)\n",
    "        self.mustango = self._load_mustango()\n",
    "        self.device = device  \n",
    "\n",
    "    def _load_vocab(self, filepath:str):\n",
    "        file = open(filepath, 'rb')\n",
    "        vocab = pickle.load(file)\n",
    "        if isinstance(vocab, Vocabulary):\n",
    "            print(\"Vocabulary Loaded Successfully\")\n",
    "            return vocab\n",
    "        else:\n",
    "            raise Exception(\"Invalid Vocabulary\")\n",
    "    \n",
    "    def _load_image_caption(self, model_path, device):\n",
    "        try:\n",
    "            model = torch.load(model_path).to(device)\n",
    "            print(\"Image Captioning Model Loaded Successfully\")\n",
    "            return model\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Unable to load torch model, reasons: {e}\")\n",
    "\n",
    "    def _load_llama(self, hg_access, llama_model_path, llama_tokenizer_path):\n",
    "        if hg_access != None:\n",
    "            try:\n",
    "                print(\"Loading Llama Models\")\n",
    "                llama_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=hg_access, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "                llama_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=hg_access)\n",
    "                print(\"Llama Loaded Successfully\")\n",
    "                return llama_model, llama_tokenizer\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Unable to load Llama model from hugging face, reasons: {e}\")\n",
    "\n",
    "\n",
    "        elif llama_model_path != None and llama_tokenizer_path != None:\n",
    "            try:\n",
    "                print(\"Loading Llama Models\")\n",
    "                llama_model = AutoModelForCausalLM.from_pretrained(llama_model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "                llama_tokenizer = AutoTokenizer.from_pretrained(llama_tokenizer_path)\n",
    "                print(\"Llama Loaded Successfully\")\n",
    "                return llama_model, llama_tokenizer\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Unable to load Llama model from directory, reasons: {e}\")\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"No Llama resources provided\")\n",
    "\n",
    "    def _load_mustango(self):\n",
    "        try:\n",
    "            mustango = Mustango(\"declare-lab/mustango\")\n",
    "            print(\"Mustango Loaded Successfully\")\n",
    "            return mustango\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Unable to load mustango, reasons: {e}\")\n",
    "\n",
    "\n",
    "    def caption_image(self, image:bytes, model:InceptV3EncoderAttentionDecoder, vocab:Vocabulary, device:str, max_length:int=50):\n",
    "        \"\"\"\n",
    "        Function to caption uploaded image from streamlit\n",
    "\n",
    "        Args:\n",
    "            image (bytes): uploaded image by users in bytes based on streamlit file reading format\n",
    "            model: image captioning model \n",
    "            vocab (Vocabulary): image captioning model vocabulary\n",
    "            device (str): cuda or cpu\n",
    "            max_length (int, optiona;): max length of generated captions, default to 50\n",
    "\n",
    "        Returns:\n",
    "            generated captions: string of image caption\n",
    "        \"\"\"\n",
    "        \n",
    "        pil_image = Image.open(io.BytesIO(image)).convert('RGB') #convert bytes to image\n",
    "\n",
    "        #setup transform to convert image to readable tensor for the model\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((299,299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        transformed_image = transform(pil_image).unsqueeze(0) #unsqueeze to add a batch size of 1\n",
    "        generated_captions, attention = model.caption_image(transformed_image.to(device), vocab, device, max_length) \n",
    "\n",
    "        return \" \".join(generated_captions[1:-1])\n",
    "\n",
    "\n",
    "\n",
    "    def generate_music_prompt(self, caption:str, text_prompt:str, llama_model:AutoModelForCausalLM, llama_tokenizer:AutoTokenizer, device:str):\n",
    "        \"\"\"\n",
    "        Function to generate music prompt by merging image captions and text prompts from user with llama2-7b\n",
    "\n",
    "        Args:\n",
    "            caption (str): generated image caption\n",
    "            text_prompt (str): text prompt from user\n",
    "            llama_model (AutoModelForCausalLM): llama model\n",
    "            llama_tokenizer (AutoTokenizer): llama tokenizer\n",
    "            device (str): cuda or cpu\n",
    "\n",
    "        Returns:\n",
    "            music_prompt: string of llama results\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"=============context================\n",
    "        {caption},\n",
    "        {text_prompt},\n",
    "        =========================================\n",
    "\n",
    "        Take the following 2 context and merge them to create a textual prompt for music generation. Your prompt should be a single line. Do not give prompts that suggest increasing intensity.\n",
    "        The prompt should contain the atmosphere of the song, where the song would fit environment wise and chord progression you have come up with. I have given you some example prompts, format your prompt similarly to them but do not copy their content.\n",
    "        Example prompts: This is a live performance of a classical music piece. There is an orchestra performing the piece with a violin lead playing the main melody. The atmosphere is sentimental and heart-touching. This piece could be playing in the background at a classy restaurant.\n",
    "        The song is an instrumental. The song is in medium tempo with a classical guitar playing a lilting melody in accompaniment style. The song is emotional and romantic. The song is a romantic instrumental song.\n",
    "        This is a new age piece. There is a flute playing the main melody with a lot of staccato notes. The rhythmic background consists of a medium tempo electronic drum beat with percussive elements all over the spectrum. There is a playful atmosphere to the piece.\n",
    "\n",
    "    \"\"\"\n",
    "        promptGenerator =  PromptTemplate(system_prompt=prompt)\n",
    "        llama_prompt = promptGenerator.build_prompt()\n",
    "        config = GenerationConfig(max_new_tokens=1024,\n",
    "                                do_sample=True,\n",
    "                                top_k = 10,\n",
    "                                num_return_sequences = 1,\n",
    "                                return_full_text = False,\n",
    "                                temperature = 0.1,\n",
    "                                )\n",
    "            \n",
    "        encoded_input = llama_tokenizer.encode(llama_prompt, return_tensors='pt', add_special_tokens=False).to(device)\n",
    "        results = llama_model.generate(encoded_input, generation_config=config)\n",
    "        decoded_output = llama_tokenizer.decode(results[0], skip_special_tokens=True)\n",
    "        response = decoded_output.split(\"[/INST]\")[-1].strip()\n",
    "        \n",
    "        #cleaning up the response to remove additional prompts\n",
    "        quote_index = response.find('\"')\n",
    "        last_quote_index = response.rfind('\"')\n",
    "        if quote_index != -1 and last_quote_index != -1: #if the result is in quotation marks\n",
    "            music_prompt = response[quote_index+1:last_quote_index]\n",
    "\n",
    "        else:\n",
    "            colon_index = response.rfind(\":\") #getting text in the format of prompt:\\n{actual prompt}\n",
    "            music_prompt = response[colon_index+3:] #remove the \\n as well\n",
    "        return music_prompt\n",
    "\n",
    "\n",
    "    def generate_music(self, music_prompt:str, model:Mustango, steps:int, guidance:int):\n",
    "        \"\"\"\n",
    "        Function to generate music from music prompt with mustango\n",
    "\n",
    "        Args:\n",
    "            music_prompt (str): text prompt to generate music with mustango\n",
    "            model (Mustango): mustango model\n",
    "            steps (int): Number of epochs the music generation model iterates through\n",
    "            guidance (int): How much guidance needed for the model\n",
    "\n",
    "        Returns:\n",
    "            generated music\n",
    "        \"\"\"\n",
    "        music = model.generate(music_prompt, steps, guidance)\n",
    "        return music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(playwrite:playWrite, byte_image:bytes, text_prompt:str, max_length:int=50, steps:int=100, guidance:int=3, delete_model:bool=True):\n",
    "    \"\"\"\n",
    "    Overall Function to generate music from image and textual prompts\n",
    "\n",
    "    Args:\n",
    "        playwrite (playWrite): Class instance with all models initiated\n",
    "        byte_image (bytes): Image prompt\n",
    "        text_prompt (str): Textural prompt\n",
    "        max_length (int, optional): Maximum caption length, defaults to 50\n",
    "        steps (int, optional): Number of epochs the music generation model iterates through, defaults to 100\n",
    "        guidance (int, optional): How much guidance needed for the model, defaults to 3\n",
    "        delete_mode (bool, optional): Delete models after they are used, mainly used for a memory situation as it requires models to be re-initialiased. Defaults too True\n",
    "\n",
    "    Returns:\n",
    "        generated music\n",
    "    \"\"\"\n",
    "\n",
    "    image_caption = playwrite.caption_image(image=byte_image,\n",
    "                                            model=playwrite.image_caption,\n",
    "                                            vocab=playwrite.vocab,\n",
    "                                            device=playwrite.device,\n",
    "                                            max_length=max_length\n",
    "                                            )\n",
    "    print(f\"Image Caption: {image_caption}\")\n",
    "    if delete_model:\n",
    "        del playwrite.image_caption\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    music_prompt = playwrite.generate_music_prompt(caption=image_caption,\n",
    "                                                   text_prompt=text_prompt,\n",
    "                                                   llama_model=playwrite.llama_model,\n",
    "                                                   llama_tokenizer=playwrite.llama_tokenizer,\n",
    "                                                   device=playwrite.device)\n",
    "    print(f\"Music Prompt: {music_prompt}\")\n",
    "    if delete_model:\n",
    "        del playwrite.llama_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    music = playwrite.generate_music(music_prompt=music_prompt,\n",
    "                                     model=playwrite.mustango,\n",
    "                                     steps=steps,\n",
    "                                     guidance=guidance)\n",
    "    \n",
    "    return music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Loaded Successfully\n",
      "Image Captioning Model Loaded Successfully\n",
      "Loading Llama Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama Loaded Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 1834.04it/s]\n",
      "c:\\Users\\valkr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForTokenClassificationRegression were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'hidden1.bias', 'hidden1.weight', 'hidden2.bias', 'hidden2.weight', 'regressor.bias', 'regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\valkr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "c:\\Users\\valkr\\Desktop\\PlayWrite\\models\\mustango\\audioldm\\audio\\stft.py:42: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n",
      "c:\\Users\\valkr\\Desktop\\PlayWrite\\models\\mustango\\audioldm\\audio\\stft.py:151: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = librosa_mel_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet initialized randomly.\n",
      "Successfully loaded checkpoint from: declare-lab/mustango\n",
      "Mustango Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "playwrite = playWrite(device=device,\n",
    "                      vocab_path='../resources/Vocabulary.pkl',\n",
    "                      image_caption_path='../models/image_captioning/model.pt',\n",
    "                      hg_access_token=None,\n",
    "                      llama_model_path='../models/llama/model',\n",
    "                      llama_tokenizer_path='../models/llama/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../input/Test/images (3).jpeg\", \"rb\") as uploaded_image:\n",
    "    f = uploaded_image.read()\n",
    "    b = bytearray(f)\n",
    "\n",
    "text_prompt = \"A wacky cooking game where you are surrounded by obstacles that hinder your progress while your time starts to run out\"\n",
    "\n",
    "generated_music = generate(playwrite=playwrite,\n",
    "                           byte_image=b,\n",
    "                           text_prompt=text_prompt,\n",
    "                           max_length=50,\n",
    "                           steps=150,\n",
    "                           guidance=3,\n",
    "                           delete_model=True)\n",
    "\n",
    "IPython.display.Audio(data=generated_music, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
