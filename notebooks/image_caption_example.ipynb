{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Notebook on how to use Helper Function\n",
    "What to expect:\n",
    "1. Application of custom data loaders\n",
    "2. Application of base image captioning model\n",
    "3. Application of BLEU and ROGUE scores in training & validation\n",
    "\n",
    "- Note: Due to the limitation of relative imports in notebooks, the full codes from src/helper functions have been copy pasted here to prevent the need for running imports\n",
    "- Proper python functions in src/main.py should rely on imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Application of custom data loaders -- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from PIL import Image, ImageOps\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer to split sentences into a list of words\n",
    "word_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class Vocabulary():\n",
    "  \"\"\"\n",
    "  Class to convert the captions to index sequential tensors\n",
    "\n",
    "  Args:\n",
    "    freq_threshold (int, optional): How many times a word has to appear in dataset before it can be added to the vocabulary. Defaults to 2\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, freq_threshold:int=2):\n",
    "    self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"} #index to sentence\n",
    "    self.stoi = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\": 2, \"<UNK>\":3} #sentence to index\n",
    "    self.freq_threshold = freq_threshold #threshold for adding a word to the vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.itos)\n",
    "\n",
    "  @staticmethod\n",
    "  def tokenizer_eng(text):\n",
    "    #convert sentence to list of words\n",
    "    return [tok.text.lower() for tok in word_tokenizer.tokenizer(text)] #convert sentence to words\n",
    "\n",
    "\n",
    "  def build_vocabulary(self, sentence_list):\n",
    "    frequencies = {}\n",
    "    idx = 4 #0-3 are for special tokens\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "      for word in self.tokenizer_eng(sentence): #convert sentence to words\n",
    "        if word not in frequencies:\n",
    "          frequencies[word] = 1\n",
    "        else:\n",
    "          frequencies[word] += 1\n",
    "\n",
    "        if frequencies[word] == self.freq_threshold: #once met freq_threshold, add to vocab list\n",
    "          self.stoi[word] = idx\n",
    "          self.itos[idx] = word\n",
    "          idx += 1\n",
    "\n",
    "  def numericalize(self, text):\n",
    "    tokenized_text = self.tokenizer_eng(text) #convert annnotations to labels by converting each word to the index inside the vocab, else UNK tag\n",
    "    return [\n",
    "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "        for token in tokenized_text\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Class to setup the custom Dataset for pyTorch Dataloader\n",
    "\n",
    "    Args:\n",
    "        Note: the order of the csv_file and root_dir are directly related, csv_file[0] contains captions for images in root_dir[0]\n",
    "        \n",
    "        csv_file (list): Lists of path to CSV files with annotations.\n",
    "        root_dir (list): List of directory containing images.\n",
    "        img_size (tuple, optional): Image size in the format (width, height), defaults to (256,256)\n",
    "        transform (callable, optional): Optional torchvision transform to be applied on a sample, defaults to None\n",
    "        freq_threshold (int, optional): Freq threshold for Vocabulary Class, defaults to 2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file:list, root_dir:list, img_size:tuple=(256,256), transform=None, freq_threshold=2):\n",
    "        \n",
    "        #dataframe with col name ['image_name', 'captions'] from csv file\n",
    "        self.annotations = pd.DataFrame()\n",
    "        #list containing the int boundary on which image path to look at\n",
    "        #list will containing the num of images in directory which is the boundary\n",
    "        self.root_dir_boundary = []\n",
    "        \n",
    "        for idx, label_files in enumerate(csv_file): \n",
    "            labels = pd.read_csv(label_files, index_col=0) #remove index col\n",
    "            self.annotations = pd.concat([self.annotations, labels], ignore_index=True) #merging annotations into 1 dataset\n",
    "\n",
    "            #getting the image boundary on which idx belongs to which image file path\n",
    "            if idx == 0: \n",
    "                self.root_dir_boundary.append(len(labels))\n",
    "            else:\n",
    "                #get the number of images in root directory and add with the previous to get the range of index that are in this filepath\n",
    "                self.root_dir_boundary.append(self.root_dir_boundary[idx-1] + len(labels))\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "        #initialise vocabulary\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.annotations.iloc[:,1].to_list()) #build vocab with all captions\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.annotations.iloc[idx, 0] #Image name as column 0\n",
    "        #finding the correct root directory filepath in the list does the image belong to\n",
    "        image_dir_idx = 0\n",
    "        while idx >= self.root_dir_boundary[image_dir_idx] and image_dir_idx < len(self.root_dir_boundary):\n",
    "            image_dir_idx += 1\n",
    "\n",
    "        img_path = f\"{self.root_dir[image_dir_idx]}/{img_name}\"\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        image = ImageOps.pad(image, self.img_size) #resize image\n",
    "        annotation = self.annotations.iloc[idx, 1] #Annotation as column 1\n",
    "        \n",
    "        #converting caption to index tensor\n",
    "        numercalized_annotations = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numercalized_annotations += self.vocab.numericalize(annotation)\n",
    "        numercalized_annotations.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        #create list of all captions associated with the image (for BLEU & ROUGE score)\n",
    "        all_img_captions = self.annotations[self.annotations['image_filename'] == img_name]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(numercalized_annotations), all_img_captions.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class padAnnotations():\n",
    "  \"\"\"\n",
    "  Collate function to pad all caption to the same length as max(len(caption)) in a batch\n",
    "\n",
    "  Args:\n",
    "    pad_idx (int): Index Label for the <PAD> token\n",
    "    batch_first (boolean, optional): Decide if the dataset labels should be batch first\n",
    "                                     Either returns (batch size, seq length) or (seq length, batch size)\n",
    "  \"\"\"\n",
    "  def __init__(self, pad_idx, batch_first = False):\n",
    "    self.batch_first = batch_first\n",
    "    self.pad_idx = pad_idx\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    imgs = [item[0].unsqueeze(0) for item in batch] \n",
    "    imgs = torch.cat(imgs, dim=0)\n",
    "    labels = [item[1] for item in batch]\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "\n",
    "    return imgs, labels, item[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Application of custom data loaders -- Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create a custom dataset\n",
    "dataset = CustomDataset(csv_file=['../input/Landscape/Train/Labels/Blip_Label.csv', '../input/Landscape/Train/Labels/Kosmos_Label.csv', '../input/Flicker8k/Train/Labels/Label.csv'],\n",
    "                        root_dir=['../input/Landscape/Train/Images', '../input/Landscape/Train/Images', '../input/Flicker8k/Train/Images'],\n",
    "                        transform=transform)\n",
    "\n",
    "\n",
    "# Create a PyTorch DataLoader\n",
    "dataloader_batchfirst = DataLoader(dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"], \n",
    "                            batch_first=True\n",
    "                        ))\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "                        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
