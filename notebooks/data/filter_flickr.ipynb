{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Flickr30k Dataset\n",
    "- Perform zero-shot classification with bart-large-mnli model\n",
    "- Define labels on both landscape and non-landscape categories\n",
    "- Set threshold for each label under landscape category\n",
    "- Classify the filtered dataset into the primary five classes\n",
    "\n",
    "You may download flickr30k dataset here:\n",
    "https://www.kaggle.com/datasets/adityajn105/flickr30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classification(input_csv:str, output_csv:str, labels:str):\n",
    "    \"\"\"\n",
    "    Performs zero-shot classification on captions from an input CSV and saves the results to an output CSV.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path where the output CSV file will be saved.\n",
    "        labels (str): A list of labels for classification.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    results = []\n",
    "    for caption in tqdm(df['caption'].to_list()):\n",
    "        #run zero shot application\n",
    "        result = classifier(caption, labels, multi_label=False)\n",
    "        results.append((result['labels'][0], result['scores'][0]))\n",
    "\n",
    "    df['label'], df['score'] = zip(*results)\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Classification completed and saved to {output_csv}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"beach\", \"lake\", \"glacier\", \"mountains\", \"snow\", \"desert\", \"sand\", \"forest\", \"sea\", \"park\", \"ice\", \"city\", \"indoor\", \"stadium\", \"urban\", \"grass\", \"pool\", \"garden\"]\n",
    "zero_shot_classification('../../input/Flickr30k/captions.csv', '../../input/Flickr30k/flick30k_all_result.csv', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_flickr(input_csv:str, output_csv:str, thresholds:dict):\n",
    "    \"\"\"\n",
    "    Filters captions based on predefined thresholds for each label and saves the filtered results to a CSV.\n",
    "    \n",
    "    Args:\n",
    "        input_csv (str): Path to the CSV file containing captions and their classification scores.\n",
    "        output_csv (str): Path where the filtered results will be saved.\n",
    "        thresholds (dict): Dictionary where keys are labels and values are the minimum score thresholds for those labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    def filter_row(row):\n",
    "        return row['label'] in thresholds and row['score'] >= thresholds[row['label']]\n",
    "\n",
    "    #filters result\n",
    "    filtered_result_df = df[df.apply(filter_row, axis=1)]\n",
    "\n",
    "    filtered_result_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Total number of qualifying captions: {len(filtered_result_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'sea': 0.7,\n",
    "    'beach': 0.5,\n",
    "    'desert': 0.8,\n",
    "    'forest': 0.7,\n",
    "    'glacier': 0.2,\n",
    "    'mountains': 0.3,  \n",
    "    'snow': 0.5,\n",
    "    'sand': 0.4,\n",
    "    'lake': 0.4\n",
    "}\n",
    "\n",
    "filter_flickr('../../input/Flickr30k/flick30k_all_result.csv', '../../input/Flickr30k/flick30k_filtered_result.csv', thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(csv_file:str, directories:list, output_folder:str):\n",
    "    \"\"\"\n",
    "    Extract images listed in a CSV file from multiple directories and save them to an output folder.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing image filenames.\n",
    "        directories (list): List of directories to search for the images.\n",
    "        output_folder (str): Path to the output folder where the images will be saved.\n",
    "\n",
    "    Returns:\n",
    "        not_found_images (list): A list of filenames of images that were not found in any of the provided directories.\n",
    "    \"\"\"\n",
    "\n",
    "    #create directory if does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    image_filenames = df['image_filename'].unique() #retrieve all images names\n",
    "\n",
    "    not_found_images = [] \n",
    "\n",
    "    for filename in image_filenames:\n",
    "        found = False  \n",
    "        for dir in directories:\n",
    "            #check all directory to retreive image\n",
    "            source_path = os.path.join(dir, filename)\n",
    "            if os.path.exists(source_path):\n",
    "                #copy the image over to the proper folder\n",
    "                shutil.copy2(source_path, os.path.join(output_folder, filename))\n",
    "                print(f\"Copied {filename} to {output_folder}\")\n",
    "                found = True\n",
    "                break \n",
    "        if not found:\n",
    "            #image does not exist\n",
    "            not_found_images.append(filename)\n",
    "\n",
    "    if not_found_images:\n",
    "        print(\"Images not found in any of the provided directories:\")\n",
    "        for img in not_found_images:\n",
    "            print(img)\n",
    "\n",
    "    return not_found_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '../../input/Flicker30k/flick30k_filtered_result.csv'\n",
    "flicker_image_files = ['../../input/Flickr30k/flickr30k_images/']\n",
    "\n",
    "flicker_filtered_image = '../../input/Flickr30k/flicker30k_output_images/'\n",
    "extract_images(csv_file, flicker_image_files, flicker_filtered_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_flickr(input_csv:str, output_csv:str):\n",
    "    \"\"\"\n",
    "    Load CSV and map each label to it's specific class as defined in label_map\n",
    "    Saves output as a csv in output_csv_path\n",
    "    \n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file containing labeled data.\n",
    "        output_csv_path (str): Path where the categorized and classified results will be saved. \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    label_map = {\n",
    "        'beach': 'coast',\n",
    "        'lake': 'coast',\n",
    "        'glacier': 'glacier',\n",
    "        'mountain': 'mountain',\n",
    "        'snow': 'glacier',\n",
    "        'desert': 'desert',\n",
    "        'sand': 'desert',\n",
    "        'forest': 'forest',\n",
    "        'sea': 'coast'\n",
    "    }\n",
    "\n",
    "    df['new_label'] = df['label'].map(label_map)\n",
    "\n",
    "    def determine_label(group):\n",
    "        label_counts = group['new_label'].value_counts()\n",
    "        if len(label_counts) == 1 or label_counts.iloc[0] > label_counts.iloc[1]:\n",
    "            return label_counts.idxmax()\n",
    "        else:\n",
    "            return group.sort_values('score', ascending=False)['new_label'].iloc[0]\n",
    "\n",
    "    grouped = df.groupby('image_filename')\n",
    "    df['classified_label'] = grouped.apply(lambda x: determine_label(x))\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"File saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_flickr('../../input/Flicker30k/flick30k_filtered_result.csv', 'classified_images.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Experimental Results\n",
    "\n",
    "The following were also tested but did not yield as good of a result\n",
    "1. Cosine similarity check with landscape data\n",
    "2. Zero-shot classification of images with OpenAI CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of filter flickr dataset\n",
    "# Looking through all captions of each image, choose those image with at least 3 captions that meets the threshold of the label\n",
    "\n",
    "def filter_flickr(input_csv:str, output_csv:str, thresholds:dict, min_label_count:int=3):\n",
    "    \"\"\"\n",
    "    Filters images based on the specified thresholds for each label, \n",
    "    Each image must have at least min_label_count labels that exist the threshold to be considered \n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file containing labeled data.\n",
    "        output_csv (str): Path where the filtered results will be saved.\n",
    "        thresholds (dict): Dictionary where keys are labels and values are the minimum confidence thresholds for those labels.\n",
    "        min_label_count (int, optional): Range from [0,5]. Minimum number of times a label must exceed its threshold to qualify an image. Defaults to 3.\n",
    "    \"\"\"\n",
    "    if 0 < min_label_count or min_label_count > 5:\n",
    "        raise Exception(\"min_label_count must be within 0 to 5\")\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    grouped = df.groupby('image_filename')\n",
    "    qualify_images = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        label_counts = {label: 0 for label in thresholds.keys()}\n",
    "        for _, row in group.iterrows():\n",
    "            if row['label'] in thresholds and row['confidence'] >= thresholds[row['label']]:\n",
    "                label_counts[row['label']] += 1\n",
    "\n",
    "        if sum(count >= min_label_count for count in label_counts.values()) >= 1:\n",
    "            qualify_images.append(name)\n",
    "\n",
    "    filtered_result_df = df[df['image_filename'].isin(qualify_images)]\n",
    "    filtered_result_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Total number of qualifying images: {len(qualify_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'sea': 0.7,\n",
    "    'beach': 0.5,\n",
    "    'desert': 0.8,\n",
    "    'forest': 0.7,\n",
    "    'glacier': 0.2,\n",
    "    'mountain': 0.5,\n",
    "    'snow': 0.5,\n",
    "    'sand': 0.5,\n",
    "    'lake': 0.5\n",
    "}\n",
    "\n",
    "filter_flickr('flick30k_all_result.csv', 'flick30k_filtered_result.csv', thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption Similarity Check with Kosomos and Blip Caption\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_tfidf(ref_file1:str, ref_file2:str, target_file:str):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF vectors for reference captions and new captions.\n",
    "\n",
    "    Args:\n",
    "        ref_file1 (str): Path to the first reference CSV file containing image captions.\n",
    "        ref_file2 (str): Path to the second reference CSV file containing image captions.\n",
    "        new_file (str): Path to the csv file with the captions to be compared against\n",
    "\n",
    "    Returns:\n",
    "        ref_vectors: TF-IDF vectors for reference captions\n",
    "        target_vectors: TF-IDF vectors for new captions\n",
    "        target_captions_df: DataFrame of new captions\n",
    "    \"\"\"\n",
    "    ref_captions1 = pd.read_csv(ref_file1)['image_caption'].tolist()\n",
    "    ref_captions2 = pd.read_csv(ref_file2)['image_caption'].tolist()\n",
    "    ref_captions = ref_captions1 + ref_captions2\n",
    "    \n",
    "    target_captions_df = pd.read_csv(target_file)\n",
    "    new_captions = target_captions_df['caption'].tolist()\n",
    "\n",
    "    all_captions = ref_captions + new_captions\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_captions_vectors = vectorizer.fit_transform(all_captions)\n",
    "\n",
    "    ref_vectors = all_captions_vectors[:len(ref_captions)]\n",
    "    target_vectors = all_captions_vectors[len(ref_captions):]\n",
    "\n",
    "    return ref_vectors, target_vectors, target_captions_df\n",
    "\n",
    "\n",
    "def filter_flicker(ref_vectors, new_vectors, target_captions_df:pd.DataFrame, threshold:float, output_file:str):\n",
    "    \"\"\"\n",
    "    Filter new captions based on similarity to reference captions using cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        ref_vectors (array-like): TF-IDF vectors of reference captions.\n",
    "        new_vectors (array-like): TF-IDF vectors of new captions.\n",
    "        target_captions_df (DataFrame): DataFrame containing new captions.\n",
    "        threshold (float): Similarity threshold for filtering.\n",
    "        output_file (str): Path to save the filtered captions.\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity(new_vectors, ref_vectors)\n",
    "    max_similarities = np.max(similarities, axis=1)\n",
    "    target_captions_df['similarity_score'] = max_similarities\n",
    "\n",
    "    qualifying_images = set()\n",
    "\n",
    "    for filename in target_captions_df['image_filename'].unique():\n",
    "        image_data = target_captions_df[target_captions_df['image_filename'] == filename]\n",
    "        qualifying_captions = image_data[image_data['similarity_score'] >= threshold]\n",
    "\n",
    "        if len(qualifying_captions) >= 2:\n",
    "            qualifying_images.add(filename)\n",
    "\n",
    "    filtered_df = target_captions_df[target_captions_df['image_filename'].isin(qualifying_images)]\n",
    "\n",
    "    filtered_df.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered captions saved to {output_file}. Total qualifying images: {len(qualifying_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_label = '../../input/Landscape/Label/Blip_Label.csv'\n",
    "kosomos_label = '../../input/Landscape/Label/Kosmos_Label.csv'\n",
    "filtered_flicker30k = '../../input/flick30k_filtered_result.csv'\n",
    "\n",
    "ref_vectors, new_vectors, target_captions_df = compute_tfidf(blip_label, kosomos_label, filtered_flicker30k)\n",
    "\n",
    "filter_flicker(ref_vectors, new_vectors, target_captions_df, 0.4, '../../input/flicker30k_output_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CLIP Model to filter the landscape image\n",
    "\n",
    "def is_landscape(model:CLIPModel, processor:CLIPProcessor, image_path:str, positive_categories:list, negative_categories:list):\n",
    "    \"\"\"\n",
    "    ZeroShot with clip model for positive_categories relating to landscape and negative_categories not relating to landscape.\n",
    "\n",
    "    Args:\n",
    "        model: CLIP model\n",
    "        processor: CLIP embedding model\n",
    "        image_path (str): Path to the image file.\n",
    "        positive_categories (list): List of positive categories related to landscapes.\n",
    "        negative_categories (list): List of negative categories not related to landscapes.\n",
    "\n",
    "    Returns:\n",
    "        maximum positive probability and maximum negative probability.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"File is not an image or cannot be opened: {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    inputs = processor(text=positive_categories + negative_categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "    positive_probs = probs[0][:len(positive_categories)]\n",
    "    negative_probs = probs[0][len(positive_categories):]\n",
    "\n",
    "    max_positive_prob = max(positive_probs).item()\n",
    "    max_negative_prob = max(negative_probs).item()\n",
    "\n",
    "    print(f\"Image: {image_path}, Positive Probability: {max_positive_prob}, Negative Probability: {max_negative_prob}\")\n",
    "\n",
    "    return max_positive_prob, max_negative_prob\n",
    "\n",
    "def classify_images(model:CLIPModel, processor:CLIPProcessor, folder_path, output_folder, positive_categories, negative_categories, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Classify images in a folder into 'similar' or 'landscape' categories based on positive and negative categories.\n",
    "\n",
    "    Args:\n",
    "        model: CLIP model\n",
    "        processor: CLIP embedding model\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "        output_folder (str): Path to the output folder.\n",
    "        positive_categories (list): List of positive categories related to landscapes.\n",
    "        negative_categories (list): List of negative categories not related to landscapes.\n",
    "        threshold (float, optional): Threshold for classification probability, defaults to 0.5.\n",
    "    \"\"\"\n",
    "    #create folders\n",
    "    similar_folder = os.path.join(output_folder, 'similar')\n",
    "    landscape_folder = os.path.join(output_folder, 'landscape')\n",
    "    \n",
    "    if not os.path.exists(similar_folder):\n",
    "        os.makedirs(similar_folder)\n",
    "    if not os.path.exists(landscape_folder):\n",
    "        os.makedirs(landscape_folder)\n",
    "    \n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    total_images = len(image_files)\n",
    "    similar_count, landscape_count = 0, 0\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        positive_prob, negative_prob = is_landscape(model, processor, image_path, positive_categories, negative_categories)\n",
    "        \n",
    "        if positive_prob is None or negative_prob is None:\n",
    "            continue\n",
    "        \n",
    "        if negative_prob > positive_prob:\n",
    "            continue\n",
    "\n",
    "        destination_folder = None\n",
    "        #similar images (images that are hard to be classified, manual classification required)\n",
    "        if abs(positive_prob - negative_prob) <= 0.1:\n",
    "            destination_folder = similar_folder\n",
    "            similar_count += 1\n",
    "\n",
    "        #images that are strongly landscape\n",
    "        elif positive_prob > threshold:\n",
    "            destination_folder = landscape_folder\n",
    "            landscape_count += 1\n",
    "        \n",
    "        if destination_folder:\n",
    "            shutil.copy2(image_path, os.path.join(destination_folder, image_file))\n",
    "\n",
    "    print(f\"Total number of images: {total_images}\")\n",
    "    print(f\"Number of images in 'similar' folder: {similar_count}\")\n",
    "    print(f\"Number of images in 'landscape' folder: {landscape_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "positive_categories = [\"mountain\", \"desert\", \"snow\", \"sea\", \"glacier\", \"beach\"]\n",
    "negative_categories = [\"water\", \"city\", \"indoor\", \"parks\", \"grass\", \"urban\", \"pool\", \"stadium\", \"lake\", \"building\", \"street\", \"transport\", \"house\", \"shop\", \"garden\", \"traffic\"]\n",
    "folder_path = flicker_filtered_image\n",
    "output_folder = '../../input/Flicker8k/Output/Zero_Shot'\n",
    "classify_images(model, processor, folder_path, output_folder, positive_categories, negative_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each image into the 5 Landscape class using CLIP\n",
    "def classify_with_clip(model:CLIPModel, processor:CLIPProcessor, image_filename:str, image_dir:str, labels:dict):\n",
    "    \"\"\"\n",
    "    Classify an image using CLIP model with ZeroShot.\n",
    "\n",
    "    Args:\n",
    "        model: CLIP model\n",
    "        processor: CLIP embedding model\n",
    "        image_filename (str): Filename of the image\n",
    "        image_dir (str): Directory where image is in\n",
    "        labels (dict): Labels for zero shot with a short description\n",
    "\n",
    "    Returns:\n",
    "        str: The determined image class.\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(image_dir, image_filename)\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"No such file: {image_path}\")\n",
    "\n",
    "    #Read images\n",
    "    image = Image.open(image_path)\n",
    "    texts = list(labels.values()) #all labels\n",
    "    \n",
    "    #Embed image and run zeroshot\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    max_index = probs.argmax().item()\n",
    "    chosen_class = list(labels.keys())[max_index]\n",
    "    return chosen_class\n",
    "\n",
    "\n",
    "def determine_image_class(model:CLIPModel, processor:CLIPProcessor, image_df:pd.DataFrame, image_dir:str, labels:dict):\n",
    "    \"\"\"\n",
    "    Runs classification on image using CLIP model with ZeroShot for all images in dataframe\n",
    "\n",
    "    Args:\n",
    "        model: CLIP model\n",
    "        processor: CLIP embedding model\n",
    "        image_df (DataFrame): DataFrame containing all filepath of images\n",
    "        image_dir (str): Directory where image is in\n",
    "        labels (dict): Labels for zero shot with a short description\n",
    "\n",
    "    Returns:\n",
    "        str: The determined image class.\n",
    "    \"\"\"\n",
    "    labels = image_df['label'].tolist()\n",
    "    if len(set(labels)) == 1 and labels[0] in list(labels.keys()):\n",
    "        return labels[0]\n",
    "    else:\n",
    "        return classify_with_clip(model, processor, image_df.name, image_dir, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = '../../input/flicker30k/flicker30k_output_images'\n",
    "df = pd.read_csv('../../input/Flicker30k/flick30k_filtered_result.csv')\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "\n",
    "target_classes = ['glacier', 'coast', 'forest', 'mountains', 'desert']\n",
    "\n",
    "descriptive_texts = {\n",
    "    'glacier': 'A vast body of ice',\n",
    "    'coast': 'Where the land meets the sea',\n",
    "    'forest': 'A dense collection of trees and plants',\n",
    "    'mountains': 'Tall and rocky natural elevations',\n",
    "    'desert': 'A hot, sandy, and arid region'\n",
    "}\n",
    "\n",
    "\n",
    "image_classes = df.groupby('image_filename').apply(determine_image_class, model=model, processor=processor, image_dir=image_directory, labels=descriptive_texts)\n",
    "\n",
    "df['image_class'] = df['image_filename'].map(image_classes)\n",
    "df.to_csv('../../input/Flicker30k/flick30k_filtered_result_updated.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
