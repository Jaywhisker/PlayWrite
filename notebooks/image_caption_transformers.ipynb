{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Notebook on training a MaxxVit Transformer Encoder and Decoder Image Captioning Model\n",
    "What to expect:\n",
    "1. Application of custom data loaders\n",
    "2. Application of image captioning model with transformers (maxxvit)\n",
    "3. Application of BLEU, ROGUE and METEOR\n",
    "4. Application of model training, validation and test\n",
    "4. Application of model inference\n",
    "\n",
    "- Note: Due to the limitation of relative imports in notebooks, the full codes from src/helper functions have been copy pasted here to prevent the need for running imports\n",
    "- Proper python functions in src/main.py will be rely on imports\n",
    "\n",
    "Image Size for MaxVit (finetuned) must be (224,224) but for MaxxVit (pre-trained) must be (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "\n",
    "# Third-party imports\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import timm\n",
    "import torch\n",
    "from torch import Tensor, optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Application of custom data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to split sentences into a list of words\n",
    "word_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary():\n",
    "  \"\"\"\n",
    "  Class to convert the captions to index sequential tensors\n",
    "\n",
    "  Args:\n",
    "    freq_threshold (int, optional): How many times a word has to appear in dataset before it can be added to the vocabulary. Defaults to 2\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, freq_threshold:int=2):\n",
    "    self.itos={ 0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\" } # index to sentence\n",
    "    self.stoi={ \"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3 } # sentence to index\n",
    "    self.freq_threshold = freq_threshold # threshold for adding a word to the vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.itos)\n",
    "\n",
    "  @staticmethod\n",
    "  def tokenizer_eng(text):\n",
    "    # convert sentence to list of words\n",
    "    return [tok.text.lower() for tok in word_tokenizer.tokenizer(text)] # convert sentence to words\n",
    "\n",
    "\n",
    "  def build_vocabulary(self, sentence_list):\n",
    "    frequencies = {}\n",
    "    idx = 4 # 0-3 are for special tokens\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "      for word in self.tokenizer_eng(sentence): # convert sentence to words\n",
    "        if word not in frequencies:\n",
    "          frequencies[word] = 1\n",
    "        else:\n",
    "          frequencies[word] += 1\n",
    "\n",
    "        if frequencies[word] == self.freq_threshold: # once met freq_threshold, add to vocab list\n",
    "          self.stoi[word] = idx\n",
    "          self.itos[idx] = word\n",
    "          idx += 1\n",
    "\n",
    "  def numericalize(self, text):\n",
    "    tokenized_text = self.tokenizer_eng(text) # convert annotations to labels by converting each word to the index inside the vocab, else UNK tag\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for token in tokenized_text:\n",
    "      if token in self.stoi:\n",
    "        result.append(self.stoi[token])\n",
    "      else:\n",
    "        result.append(self.stoi[\"<UNK>\"])\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Class to setup the custom Dataset for pyTorch Dataloader\n",
    "\n",
    "    Args:\n",
    "        Note: the order of the csv_file and root_dir are directly related, csv_file[0] contains captions for images in root_dir[0]\n",
    "        \n",
    "        csv_file (list): Lists of path to CSV files with annotations.\n",
    "        root_dir (list): List of directory containing images.\n",
    "        img_size (tuple, optional): Image size in the format (width, height), defaults to (256,256)\n",
    "        transform (callable, optional): Optional torchvision transform to be applied on a sample, defaults to None\n",
    "        freq_threshold (int, optional): Freq threshold for Vocabulary Class, defaults to 2\n",
    "        vocabulary (Vocabulary, optional): Determines to use an existing vocabulary or create own, defaults to None\n",
    "    Returns:\n",
    "        image: transformed image\n",
    "        labels: tensor object of the labels\n",
    "        all_image_captions: list containing all the captions of the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file:list, root_dir:list, img_size:tuple=(256,256), transform=None, freq_threshold=2, vocabulary=None):\n",
    "        \n",
    "        #dataframe with col name ['image_filename', 'image_captions'] from csv file\n",
    "        self.annotations = pd.DataFrame()\n",
    "        #list containing the int boundary on which image path to look at\n",
    "        #list will containing the num of images in directory which is the boundary\n",
    "        self.root_dir_boundary = []\n",
    "        \n",
    "        for idx, label_files in enumerate(csv_file): \n",
    "            labels = pd.read_csv(label_files, index_col=0) #remove index col\n",
    "            self.annotations = pd.concat([self.annotations, labels], ignore_index=True) #merging annotations into 1 dataset\n",
    "\n",
    "            #getting the image boundary on which idx belongs to which image file path\n",
    "            if idx == 0: \n",
    "                self.root_dir_boundary.append(len(labels))\n",
    "            else:\n",
    "                #get the number of images in root directory and add with the previous to get the range of index that are in this filepath\n",
    "                self.root_dir_boundary.append(self.root_dir_boundary[idx-1] + len(labels))\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "        #initialise vocabulary\n",
    "        if vocabulary == None:\n",
    "            self.vocab = Vocabulary(freq_threshold)\n",
    "            self.vocab.build_vocabulary(self.annotations.iloc[:,1].to_list()) #build vocab with all captions\n",
    "        else:\n",
    "            self.vocab = vocabulary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.annotations.iloc[idx, 0] #Image name as column 0\n",
    "        #finding the correct root directory filepath in the list does the image belong to\n",
    "        image_dir_idx = 0\n",
    "        while idx >= self.root_dir_boundary[image_dir_idx] and image_dir_idx < len(self.root_dir_boundary):\n",
    "            image_dir_idx += 1\n",
    "\n",
    "        img_path = f\"{self.root_dir[image_dir_idx]}/{img_name}\"\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        # image = ImageOps.pad(image, self.img_size) #resize image\n",
    "        annotation = self.annotations.iloc[idx, 1] #Annotation as column 1\n",
    "        \n",
    "        #converting caption to index tensor\n",
    "        numercalized_annotations = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numercalized_annotations += self.vocab.numericalize(annotation)\n",
    "        numercalized_annotations.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        #create list of all captions associated with the image (for BLEU & ROUGE score)\n",
    "        all_img_captions = self.annotations[self.annotations['image_filename'] == img_name]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(numercalized_annotations), all_img_captions.iloc[:,1].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class padAnnotations():\n",
    "  \"\"\"\n",
    "    A custom collate function for PyTorch DataLoader to handle batching of images and captions.    \n",
    "    This collate function is used to dynamically pad the captions in a batch to the same length so that they can be processed as a batch. \n",
    "\n",
    "    Args:\n",
    "      pad_idx (int): The index of the PAD token in the vocabulary, used for padding shorter captions in a batch to match the longest caption.\n",
    "        \n",
    "    Returns: \n",
    "      img: batch image object\n",
    "      labels: batch of tensors of the captions, converted to the same length by adding <PAD>\n",
    "      all_labels: batch of lists of the captions of the images\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, pad_idx:int, batch_first:bool=False):\n",
    "    self.batch_first = batch_first\n",
    "    self.pad_idx = pad_idx\n",
    "      \n",
    "  def __call__(self, batch):\n",
    "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "    imgs = torch.cat(imgs, dim=0)\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "    all_labels = [item[2] for item in batch]\n",
    "\n",
    "    return imgs, targets, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_img_caption_data(dataloader, batch_size: int, dataset: CustomDataset, num_batches: int = 1, num_samples: int=9):\n",
    "    \"\"\"\n",
    "    Function to visualise the dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader (dataloader object): Pytorch dataloader object to visualise\n",
    "        batch_size (int): Batch Size of dataloader\n",
    "        dataset (CustomDataset): dataset used to create dataloader (required to get the vocabulary)\n",
    "        num_batches (int, optional): How many batches to visualise, defaults to 1\n",
    "        num_samples (int, optional): How many images per batch to visualise, defaults to 9\n",
    "\n",
    "    Returns:\n",
    "        Plot of image with its caption and the list of the captions to the image\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        train_features, train_labels, train_all_captions = next(iter(dataloader))\n",
    "        if i == 0:  # Print shape\n",
    "            print(f\"Feature batch shape: {train_features.size()}\")\n",
    "            print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "            print(f\"All captions batch size: {len(train_all_captions)}\")\n",
    "\n",
    "        for j in range(num_samples):\n",
    "            #Prepare image to print\n",
    "            img = train_features[j].squeeze()\n",
    "            img = np.transpose(img.numpy(), (1, 2, 0))  #Convert from (channel, height, width) to (height, width, channel) for matplotlib\n",
    "\n",
    "            #Transpose train_labels and handle out of range indices\n",
    "            label = train_labels[:, j] if train_labels.size(0) != batch_size else train_labels[j]\n",
    "            string_label = [dataset.vocab.itos[idx] for idx in label.tolist()]\n",
    "            actual_caption = \" \".join([token for token in string_label if token not in ['<PAD>', '<SOS>', '<EOS>']])\n",
    "\n",
    "            #Create a new plot for each image\n",
    "            fig, ax = plt.subplots(figsize=(5, 5))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'Caption: {actual_caption}\\n\\nAll Possible Captions:\\n' + \"\\n\".join(train_all_captions[j]), loc='left')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256,256)\n",
    "batch_size = 32\n",
    "\n",
    "#Define transforms for image preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#No data augmentation\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual dataset for this notebook\n",
    "\n",
    "#Create train dataset\n",
    "dataset = CustomDataset(csv_file=['../input/Landscape/Train/Labels/Blip_Label_Clean.csv', '../input/Landscape/Train/Labels/Kosmos_Label_Clean.csv', '../input/FilteredFlicker/Train/Labels/Label.csv'],\n",
    "                        root_dir=['../input/Landscape/Train/Images', '../input/Landscape/Train/Images', '../input/FilteredFlicker/Train/Images'],\n",
    "                        transform=train_transform,\n",
    "                        img_size=image_size)\n",
    "\n",
    "#Create val and test dataset, note that\n",
    "#1. transform uses eval transform with no data augmentation\n",
    "#2. vocab is the original train dataset vocab (to map the correct index)\n",
    "val_dataset = CustomDataset(csv_file=['../input/Landscape/Validation/Labels/Blip_Label_Clean.csv', '../input/Landscape/Validation/Labels/Kosmos_Label_Clean.csv', '../input/FilteredFlicker/Validation/Labels/Label.csv'],\n",
    "                        root_dir=['../input/Landscape/Validation/Images', '../input/Landscape/Validation/Images', '../input/FilteredFlicker/Validation/Images'],\n",
    "                        transform=eval_transform,\n",
    "                        img_size=image_size,\n",
    "                        vocabulary=dataset.vocab)\n",
    "\n",
    "#only focusing on landscape image, please add flicker30k if u would like\n",
    "test_dataset = CustomDataset(csv_file=['../input/Landscape/Test/Labels/Blip_Label_Clean.csv', '../input/Landscape/Test/Labels/Kosmos_Label_Clean.csv'],\n",
    "                        root_dir=['../input/Landscape/Test/Images', '../input/Landscape/Test/Images'],\n",
    "                        transform=eval_transform,\n",
    "                        img_size=image_size,\n",
    "                        vocabulary=dataset.vocab)\n",
    "\n",
    "\n",
    "#create dataloaders, just update the dataset it is from\n",
    "train_dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=False\n",
    "                        ))\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=False\n",
    "                        ))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=False\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img_caption_data(train_dataloader, batch_size, dataset, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Application of transformer image captioning model\n",
    "- Implementing a MaxxVit Vison Transformer Encoder and a Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderMaxVit(torch.nn.Module):\n",
    "  \"\"\"\n",
    "    An encoder module based on the MaxVit architecture, adapted for image captioning by converting images into dense feature embeddings.\n",
    "\n",
    "    The final fully connected (fc) layer of the MaxVit model is replaced to map the extracted features to the desired embedding size.\n",
    "    Feature extraction layer (classifier.5 for pretrained and head for finetuned) is always trainable.\n",
    "\n",
    "    Args:\n",
    "      finetuned_model: Finetuned MaxVit model, else None\n",
    "      embedding_size (int): Embedding Dimension for size of feature embedding\n",
    "      train_CNN (bool, optional): Determines if the entire MaxVit model will be unfreeze and trained during the training. Defaults to False.\n",
    "      drop_p (float, optional): Dropout probability to use in the dropout layer for regularization. Defaults to 0.5.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, finetuned_model, embedding_size:int, train_CNN:bool=False, drop_p:float=0.5):\n",
    "    super(EncoderMaxVit, self).__init__()\n",
    "    self.embed_size = embedding_size\n",
    "\n",
    "    if finetuned_model is not None:\n",
    "      # Initialize custom finetuned model and make the last linear layer trainable\n",
    "      self.model = list(finetuned_model.children())[0]\n",
    "      self.model.classifier[-1] = torch.nn.Linear(in_features=self.model.classifier[-1].in_features, out_features=self.embed_size, bias=True)\n",
    "      #Remove creating feature extraction layer and make it trainable, the rest depends on train_cnn\n",
    "      for name, param in self.model.named_parameters():\n",
    "        if \"classifier.5.weight\" in name or \"classifier.5.bias\" in name:\n",
    "          param.requires_grad = True\n",
    "        else:\n",
    "          param.requires_grad = train_CNN\n",
    "\n",
    "    else:\n",
    "      # Initialize pretrained model and make last linear layer trainable\n",
    "      model_used = 'maxvit_rmlp_tiny_rw_256.sw_in1k'\n",
    "      self.model = timm.create_model(model_used, pretrained=True)\n",
    "      self.model.head.fc = torch.nn.Linear(in_features=self.model.head.fc.in_features, out_features=self.embed_size, bias=True)\n",
    "\n",
    "      for name , param in self.model.named_parameters():\n",
    "        if \"head.fc.weight\" in name or \"head.fc.bias\" in name:\n",
    "          param.requires_grad = True\n",
    "        else:\n",
    "          param.requires_grad = train_CNN\n",
    "\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.dropout = torch.nn.Dropout(drop_p)\n",
    "    \n",
    "              \n",
    "  def forward(self, image):\n",
    "    output = self.model(image)\n",
    "    output = self.dropout(self.relu(output))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "  \"\"\"\n",
    "    Injects some information about the relative or absolute position of the tokens in the sequence.\n",
    "    Embeddings do not encode the relative position of tokens in a sentence. \n",
    "    With positional encoding, words will be closer to each other based on the similarity of their meaning and their position in the sequence, in the d-dimensional space.\n",
    "\n",
    "    Args:\n",
    "      d_model (int): The dimension of the embeddings.\n",
    "      dropout (float, optional): The dropout probability.\n",
    "      max_len (int, optional): The maximum length of the input sequences.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, d_model: int, dropout: float=0.1, max_len: int=5000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(max_len, 1, d_model)\n",
    "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "    \"\"\"\n",
    "    x = x + self.pe[:x.size(0)]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformersDecoder(torch.nn.Module):\n",
    "  \"\"\"\n",
    "    A Transformer-based decoder that generates captions from image features. \n",
    "    Uses the features from the encoder as the memory input to the Transformer layers,\n",
    "    along with the target tokens (captions) which are shifted right.\n",
    "    It applies positional encoding to the target token embeddings to maintain their sequence information.\n",
    "\n",
    "    Args:\n",
    "      embedding_size (int): The size of the embedding vector for each token.\n",
    "      trg_vocab_size (int): The size of the target vocabulary.\n",
    "      num_heads (int): The number of heads in the multihead-attention models.\n",
    "      num_decoder_layers (int): The number of sub-decoder-layers in the decoder.\n",
    "      dropout (float): The dropout probability.\n",
    "\n",
    "    The decoder generates predictions for the next token in the sequence, given the current sequence of tokens and the image features.\n",
    "  \"\"\"\n",
    "   \n",
    "  def __init__(self, embedding_size:int, trg_vocab_size:int, num_heads:int, num_decoder_layers:int, dropout:float):\n",
    "    super(TransformersDecoder, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    self.embedding = torch.nn.Embedding(trg_vocab_size, embedding_size)\n",
    "    self.pos = PositionalEncoding(d_model=embedding_size)\n",
    "    self.decoder_layer = torch.nn.TransformerDecoderLayer(d_model=embedding_size, nhead=num_heads)\n",
    "    self.decoder = torch.nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
    "    self.linear = torch.nn.Linear(embedding_size, trg_vocab_size)\n",
    "    self.drop = torch.nn.Dropout(dropout)\n",
    "      \n",
    "  def make_mask(self, sz):\n",
    "    \"\"\"\n",
    "      Generate a square attention mask of size (sz, sz),\n",
    "      with upper triangular filled with float('-inf').\n",
    "    \"\"\"\n",
    "\n",
    "    mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "  \n",
    "  def forward(self, features, caption, device):\n",
    "    embed = self.drop(self.embedding(caption))\n",
    "    embed = self.pos(embed)\n",
    "    trg_mask = self.make_mask(caption.size(0)).to(device)\n",
    "    decoder = self.decoder(tgt = embed , memory = features.unsqueeze(0), tgt_mask = trg_mask )\n",
    "    output = self.linear(decoder)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderToDecoder(torch.nn.Module):\n",
    "  \"\"\"\n",
    "    MaxVit Encoder with Transformer Decoder\n",
    "\n",
    "    Args:\n",
    "        finetuned_model: Finetuned MaxVit model, else None\n",
    "        embedding_size (int): The size of the embedding vector for the image features and the tokens.\n",
    "        trg_vocab_size (int): The size of the target vocabulary for the decoder.\n",
    "        num_heads (int, optional): The number of attention heads in the Transformer decoder. Defaults to 8\n",
    "        num_decoder_layers (int, optional): The number of layers in the Transformer decoder. Defaults to 6\n",
    "        dropout (float, optional): The dropout probability used in the Transformer decoder. Defaults to 0.2\n",
    "        train_cnn (bool, optional): Determines if MaxVit Encoder should be unfreezed. Defaults to False\n",
    "        device (str, optional): cuda or cpu. Defaults to cuda\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, finetuned_model, embedding_size:int, trg_vocab_size:int, num_heads:int=8, num_decoder_layers:int=6, dropout:float=0.2, train_cnn:bool=False, device:str='cuda'):\n",
    "    super(TransformerEncoderToDecoder,self).__init__()\n",
    "    self.device = device\n",
    "    self.encoder = EncoderMaxVit(finetuned_model, embedding_size, train_cnn)\n",
    "    self.decoder = TransformersDecoder(embedding_size=embedding_size,\n",
    "                                        trg_vocab_size=trg_vocab_size,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_decoder_layers=num_decoder_layers,\n",
    "                                        dropout=dropout)\n",
    "      \n",
    "  def forward(self, image, caption):\n",
    "    features = self.encoder(image)\n",
    "    output = self.decoder(features, caption, self.device)\n",
    "    return output\n",
    "  \n",
    "  #for inference\n",
    "  def caption_image(self, image, vocabulary, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate caption using a greedy algorithm based on image input\n",
    "\n",
    "    Args:\n",
    "        image: image input\n",
    "        vocabulary (Vocabulary): Vocabulary to decode predictions\n",
    "        device (str): cuda or cpu\n",
    "        max_length (int, optional): Max length of generated captions. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        captions: string caption in a list\n",
    "        atten_weights: None for transformer\n",
    "    \"\"\"\n",
    "    outputs=[vocabulary.stoi[\"<SOS>\"]]\n",
    "\n",
    "    for i in range(max_length):\n",
    "      trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "      image = image.to(device)\n",
    "      \n",
    "      with torch.no_grad():\n",
    "        output = self.forward(image, trg_tensor)\n",
    "          \n",
    "      best_guess = output.argmax(2)[-1, :].item()\n",
    "      outputs.append(best_guess)\n",
    "      \n",
    "      if best_guess == vocabulary.stoi[\"<EOS>\"]:\n",
    "        break\n",
    "\n",
    "    caption = [vocabulary.itos[idx] for idx in outputs]\n",
    "    \n",
    "    return caption, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Application of BLEU, ROGUE and METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def get_bleu_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the bleu 1-4 score of a corpus\n",
    "\n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {\"BLEU1\":, \"BLEU2\": ,\"BLEU3\": ,\"BLEU4\": }\n",
    "    \"\"\"\n",
    "    #Tokenize the predictions and references\n",
    "    predicted = [pred.split() for pred in predicted]\n",
    "    references = [[ref.split() for ref in refs] for refs in references]\n",
    "\n",
    "\n",
    "    BLEU1 = corpus_bleu(references, predicted, weights=(1, 0, 0, 0))\n",
    "    BLEU2 = corpus_bleu(references, predicted, weights=(0.5, 0.5, 0, 0))\n",
    "    BLEU3 = corpus_bleu(references, predicted, weights=(0.33, 0.33, 0.33, 0))\n",
    "    BLEU4 = corpus_bleu(references, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return {\"BLEU1\": BLEU1, \"BLEU2\": BLEU2, \"BLEU3\": BLEU3, \"BLEU4\": BLEU4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def get_rouge_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the rogue1, rogue2, rogueL, rogueLSum scores\n",
    "        \n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {'rouge1': , 'rouge2': , 'rougeL': , 'rougeLsum': }\n",
    "    \n",
    "    \"\"\"\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results = rouge.compute(predictions=predicted, references=references)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def get_meteor_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the meteor scores\n",
    "        \n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {'meteor':}\n",
    "    \n",
    "    \"\"\"\n",
    "    meteor = evaluate.load('meteor')\n",
    "    results = meteor.compute(predictions=predicted, references=references)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Application of model training, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(outputs, batch_first:bool, vocabulary:Vocabulary):\n",
    "    \"\"\"\n",
    "    Function to convert model tensor outputs to sentences\n",
    "\n",
    "    Args:\n",
    "        outputs (torch tensor object): Model's output to be decoded, either in size (seq len, batch, vocab_size) or (batch, seq len, vocab_size)\n",
    "        batch_first (bool): Boolean of if dataloader was configured to batch_first\n",
    "        vocabulary (Vocabulary): dataset Vocabulary Class for decoding\n",
    "\n",
    "    Returns:\n",
    "        list of predicted sentences each corresponding to 1 sample in the batch\n",
    "            - will be of length (batch_size)\n",
    "            eg. ['predicted sentence 1 for sample 1', ...'predicted sentence N for sample N']\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    all_prediction = []\n",
    "    predicted_tokens = outputs.argmax(-1) #flatten vocab size dimensions\n",
    "    if not batch_first:\n",
    "        predicted_tokens = predicted_tokens.T \n",
    "    \n",
    "    for sentence_tokens in predicted_tokens:\n",
    "        sentence_tokens = sentence_tokens.tolist()\n",
    "\n",
    "        try:\n",
    "            #cropping predicted sentence to first EOS\n",
    "            eos_index = sentence_tokens.index(vocabulary.stoi['<EOS>']) #get first instance of <EOS> to crop sentence accordingly\n",
    "            predicted_sentence = sentence_tokens[:eos_index]\n",
    "        except:\n",
    "            predicted_sentence = sentence_tokens\n",
    "\n",
    "        try:\n",
    "            #getting predicted_sentence by remove <SOS>\n",
    "            predicted_sentence.remove(vocabulary.stoi['<SOS>'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        all_prediction.append(\" \".join([vocabulary.itos[idx] for idx in predicted_sentence]))\n",
    "\n",
    "    return all_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, \n",
    "         criterion, \n",
    "         dataloader, \n",
    "         image_size:tuple,\n",
    "         transformer:bool,\n",
    "         batch_first:bool, \n",
    "         vocabulary:Vocabulary, \n",
    "         device:str):\n",
    "    \"\"\"\n",
    "    Function to evaluate model performance\n",
    "\n",
    "    Args:\n",
    "        model: The model that is to be evaluated\n",
    "        criterion: Loss criterion of the model\n",
    "        dataloader: validation / test dataset\n",
    "        image_size (tuple): image size of model\n",
    "        transformer (bool): boolean if decoder is a transformer\n",
    "        batch_first (bool): boolean if dataloader samples tensor are (batch, seq len) or (seq len, batch)\n",
    "        vocabulary (Vocabulary): dataset vocabulary class\n",
    "        device (str): cpu or cuda\n",
    "\n",
    "    Returns:\n",
    "        avg_val_loss: average validation loss\n",
    "        Bleu_score: dictionary of BLEU 1-4 score\n",
    "        Rouge_score: dictionary of Rouge  1,2,L,LSum score\n",
    "        Meteor_score: dictionary of Meteor Score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0\n",
    "\n",
    "    #BLEU predictions container\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (imgs, annotations, all_annotations) in enumerate(dataloader):\n",
    "            #getting img and annotations\n",
    "            imgs = torch.nn.functional.interpolate(imgs, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            imgs = imgs.to(device)\n",
    "            annotations = annotations.to(device)\n",
    "            \n",
    "            if transformer:\n",
    "                #running model prediction\n",
    "                outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                targets = annotations[1:].reshape(-1)\n",
    "                #updating model parameters\n",
    "                loss = criterion(outputs.view(-1, len(vocabulary)), targets)\n",
    "            \n",
    "            else:\n",
    "                if not batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                    \n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.reshape(-1, outputs.shape[2]), annotations.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "                if batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs, atten_weights = model(imgs, annotations[:, :-1]) #training model to guess the last word\n",
    "                    targets = annotations[:, 1:]\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.view(-1, len(vocabulary)), targets.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            #get model predictions and update\n",
    "            predictions.extend(decode_predictions(outputs, batch_first, vocabulary))\n",
    "\n",
    "            #update references\n",
    "            references.extend(all_annotations)\n",
    "\n",
    "        Bleu_score = get_bleu_score(predictions, references)\n",
    "        Rouge_score = get_rouge_score(predictions, references)\n",
    "        Meteor_score = get_meteor_score(predictions, references)\n",
    "\n",
    "        return total_val_loss/(idx+1), Bleu_score, Rouge_score, Meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          criterion, \n",
    "          optimiser, \n",
    "          train_dataloader, \n",
    "          val_dataloader, \n",
    "          image_size: tuple,\n",
    "          batch_first:bool, \n",
    "          transformer:bool,\n",
    "          vocabulary:Vocabulary, \n",
    "          device:str, \n",
    "          num_epochs:int, \n",
    "          early_stopping_threshold: float=0.003,\n",
    "          show_train_metrics:bool=None, \n",
    "          save_every:int=None,\n",
    "          model_name:str=None, \n",
    "          overwrite:bool=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to train the model\n",
    "    If change in val loss is less than 0.01 for 2 epochs in a row, stop training\n",
    "\n",
    "    Args:\n",
    "        model: The model that is to be evaluated\n",
    "        criterion: Loss criterion of the model\n",
    "        optimiser: Optimiser function of the model\n",
    "        train_dataloader: Train dataset\n",
    "        val_dataloader: Validation dataset, use None if no Validation dataset\n",
    "        image_size (tuple): image size of model\n",
    "        batch_first (bool): Boolean if dataloader samples tensor are (batch, seq len) or (seq len, batch)\n",
    "        transformer (bool): Boolean if decoder is a transformer\n",
    "        vocabulary (Vocabulary): Dataset vocabulary class\n",
    "        device (str): cpu or cuda\n",
    "        num_epochs (int): Number of epochs for training\n",
    "        early_stopping_threshold (float, optional): Threshold for early stopping, defaults to 0.003\n",
    "        show_train_metrics (bool, optional): Booleon on should calculate BLEU & Rouge score during training, defaults to False\n",
    "        save_every (int, optional): Save model after every ___ epochs, defaults to None (no saving)\n",
    "        model_name (str, optional): Model Name to be saved after, required if save_every != None, model will be saved as (model_name)_epoch or just model_name\n",
    "        overwrite (bool, optional): Boolean on overwriting model saves or saving each specific epoch as a new model, defaults to False\n",
    "    \n",
    "    Returns\n",
    "        train_loss: list of average training loss per epoch\n",
    "        train_bleu: list of dictionary of training BLEU score per epoch, [] if show_train_metric = False\n",
    "        train_rouge: list of dictionary of training Rouge score per epoch, [] if show_train_metric = False\n",
    "        train_meteor: list of dictionary of training Meteor score per epoch, [] if show_train_metric = False\n",
    "        val_loss: list of average validation loss per epoch, [] if val_dataloader = None\n",
    "        val_bleu: list of dictionary of validation BLEU score per epoch, [] if val_dataloader = None\n",
    "        val_rouge: list of dictionary of validation Rouge score per epoch, [] if val_dataloader = None\n",
    "        val_meteor: list of dictionary of validation Meteor score per epoch, [] if val_dataloader = None\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialise results container\n",
    "    train_loss = []\n",
    "    train_bleu = []\n",
    "    train_rouge = []\n",
    "    train_meteor = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_bleu = []\n",
    "    val_rouge = []\n",
    "    val_meteor = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        #BLEU predictions container\n",
    "        predictions = []\n",
    "        references = []\n",
    "\n",
    "        #start model training\n",
    "        model.train()\n",
    "        for idx, (imgs, annotations, all_annotations) in enumerate(train_dataloader):\n",
    "            \n",
    "            #getting img and annotations\n",
    "            imgs = torch.nn.functional.interpolate(imgs, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            imgs = imgs.to(device)\n",
    "            annotations = annotations.to(device)\n",
    "\n",
    "            if transformer:\n",
    "                #running model prediction\n",
    "                outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                targets = annotations[1:].reshape(-1)\n",
    "                #updating model parameters\n",
    "                loss = criterion(outputs.view(-1, len(vocabulary)), targets)\n",
    "            \n",
    "            else:\n",
    "                if not batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.reshape(-1, outputs.shape[2]), annotations.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "                if batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs, atten_weights = model(imgs, annotations[:, :-1]) #training model to guess the last word\n",
    "                    targets = annotations[:, 1:]\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.view(-1, len(vocabulary)), targets.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "            optimiser.zero_grad() #remove optimiser gradient\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            #calculate loss and update it for each batch\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            if show_train_metrics:\n",
    "                #get model predictions and update\n",
    "                predictions.extend(decode_predictions(outputs, batch_first, vocabulary))\n",
    "\n",
    "                #update references\n",
    "                references.extend(all_annotations)\n",
    "\n",
    "        if show_train_metrics:   \n",
    "            #calculating bleu and rouge score\n",
    "            Bleu_score = get_bleu_score(predictions, references)\n",
    "            Rouge_score = get_rouge_score(predictions, references)\n",
    "            Meteor_score = get_meteor_score(predictions, references)\n",
    "            train_bleu.append(Bleu_score)\n",
    "            train_rouge.append(Rouge_score)\n",
    "            train_meteor.append(Meteor_score)\n",
    "\n",
    "        #updating values\n",
    "        train_loss.append(total_train_loss/(idx+1))\n",
    "\n",
    "        if val_dataloader != None:\n",
    "            #validation\n",
    "            avg_val_loss, val_bleu_score, val_rouge_score, val_meteor_score = eval(\n",
    "                                                                            model=model,\n",
    "                                                                            criterion=criterion,\n",
    "                                                                            dataloader=val_dataloader,\n",
    "                                                                            image_size=image_size,\n",
    "                                                                            transformer=transformer,\n",
    "                                                                            batch_first=batch_first,\n",
    "                                                                            vocabulary=vocabulary,\n",
    "                                                                            device=device\n",
    "                                                                        )\n",
    "            \n",
    "            val_loss.append(avg_val_loss)\n",
    "            val_bleu.append(val_bleu_score)\n",
    "            val_rouge.append(val_rouge_score)\n",
    "            val_meteor.append(val_meteor_score)\n",
    "\n",
    "        #printing progress\n",
    "        if num_epochs <= 10 or (num_epochs >10 and (epoch+1)%5 == 0):\n",
    "            print(f\"Epoch {epoch+1} completed\\navg training loss per batch: {total_train_loss/(idx+1)}\")\n",
    "            \n",
    "            if show_train_metrics:\n",
    "                print(f\"train bleu score:{Bleu_score}\\ntrain rouge score: {Rouge_score}\\ntrain meteor score: {Meteor_score}\\n\")\n",
    "\n",
    "            if val_dataloader != None:\n",
    "                print(f\"avg validation loss per batch: {avg_val_loss}\\nval bleu score: {val_bleu_score}\\nval rouge score: {val_rouge_score}\\nval meteor score: {val_meteor_score}\")\n",
    "\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "            \n",
    "        #saving model every x\n",
    "        if save_every != None and (epoch+1)%save_every == 0:\n",
    "            try:\n",
    "                if overwrite:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}.pt\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_{epoch+1}.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save model at epoch {epoch+1}\")\n",
    "\n",
    "        \n",
    "        #saving best model\n",
    "        if (len(val_loss) > 1) and val_loss[-1] < min(val_loss[:-1]):\n",
    "            try:\n",
    "                torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_best.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save best model\")\n",
    "        \n",
    "\n",
    "        #early stopping\n",
    "        if (len(val_loss) >= 3) and abs(val_loss[-2] - val_loss[-1]) < early_stopping_threshold and abs(val_loss[-3] - val_loss[-2]) < early_stopping_threshold:\n",
    "            print(f\"validation loss did not decrease, stopping training at epoch {epoch +1}\")\n",
    "            try:\n",
    "                if overwrite:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}.pt\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_{epoch+1}.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save model at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_graph(training_data:list, validation_data:list, y_label:str, x_label:str, ylim:list=None):\n",
    "    \"\"\"\n",
    "    Plot a line graph against training and validation data\n",
    "\n",
    "    Args:\n",
    "        training_data (list): Training data to be plotted\n",
    "        validation_data (list): Validation data to be plotted\n",
    "        y_label (str): Label of y axis\n",
    "        x_label (str): Label of x axis\n",
    "        ylim (list, optional): Range of y axis, defaults to None\n",
    "    \"\"\"\n",
    "    #plotting line graph of training data and validation data\n",
    "    plt.plot(training_data, label='Train')\n",
    "    plt.plot(validation_data, label='Validation')\n",
    "\n",
    "    #labels and title\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(f'{y_label} against {x_label}')\n",
    "    \n",
    "    if ylim != None:\n",
    "        plt.ylim(ylim)\n",
    "        \n",
    "    #show legend\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedCNNModels(torch.nn.Module):\n",
    "    def __init__(self, model_type:str, num_unfreeze:int, num_class:int):\n",
    "        super(PreTrainedCNNModels, self).__init__()\n",
    "        \"\"\"\n",
    "        Class that contains InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit fine tuned models\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Determines which pre-trained models to use\n",
    "                              Must be: InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit\n",
    "            num_unfreeze (int): Number of layers to unfreeze and finetune\n",
    "            num_class (int): Number of output classes for the classification\n",
    "        \"\"\"\n",
    "        #selecting model type\n",
    "        if model_type == 'InceptionV3':\n",
    "            self.model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "            self.model.aux_logits = False\n",
    "\n",
    "        elif model_type == 'Resnet50':\n",
    "            self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'Resnet152':\n",
    "            self.model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'EfficientNet':\n",
    "            self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        \n",
    "        elif model_type == 'VGG16':\n",
    "            self.model = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'MaxVit':\n",
    "            self.model = models.maxvit_t(weights=models.MaxVit_T_Weights.DEFAULT)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid model type chosen. Please select one of the following\\n[InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit]\")\n",
    "\n",
    "        \n",
    "        #modifying final layer\n",
    "        if model_type in ['InceptionV3', 'Resnet50', 'Resnet152']:\n",
    "            self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_class)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model.classifier = torch.nn.Linear(self.model.classifier.in_features, num_class)\n",
    "\n",
    "        else:\n",
    "            self.model.classifier[-1] = torch.nn.Linear(self.model.classifier[-1].in_features, num_class)\n",
    "\n",
    "\n",
    "        model_paramteres = list(self.model.parameters())\n",
    "        #unfreeze last num_unfreeze layers\n",
    "        for param in model_paramteres[-num_unfreeze:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        #freeze rest of the layers\n",
    "        for param in model_paramteres[:-num_unfreeze]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuned MaxVit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_maxvit = torch.load('../models/cnn/maxvit_20_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (224,224)\n",
    "embed_size = 512 \n",
    "vocab_size = len(dataset.vocab)\n",
    "num_heads = 8\n",
    "num_decoder_layers = 4\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 100\n",
    "save_every = 25\n",
    "dropout = 0.10\n",
    "model_name = \"TransformerModel_20FT\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoderToDecoder(finetuned_model=finetuned_maxvit,\n",
    "                                embedding_size=embed_size,\n",
    "                                trg_vocab_size=vocab_size,\n",
    "                                num_heads=num_heads,\n",
    "                                num_decoder_layers=num_decoder_layers,\n",
    "                                dropout=dropout,\n",
    "                                train_cnn=False,\n",
    "                                device=device).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor= train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    image_size=img_size,\n",
    "    transformer=True,\n",
    "    batch_first=False,\n",
    "    vocabulary=dataset.vocab,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_threshold = 0.001,\n",
    "    show_train_metrics=True,\n",
    "    save_every=save_every,\n",
    "    model_name=model_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "visualise_graph(training_data=train_loss,\n",
    "                validation_data=val_loss,\n",
    "                y_label='Loss',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#rouge-1\n",
    "rouge_train =  [rouge_dict['rouge1'] for rouge_dict in train_rouge]\n",
    "rouge_val =  [rouge_dict['rouge1'] for rouge_dict in val_rouge]\n",
    "visualise_graph(training_data=rouge_train,\n",
    "                validation_data=rouge_val,\n",
    "                y_label='Rouge-1 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#bleu4\n",
    "bleu4_train =  [bleu_dict['BLEU4'] for bleu_dict in train_bleu]\n",
    "bleu4_val =  [bleu_dict['BLEU4'] for bleu_dict in val_bleu]\n",
    "visualise_graph(training_data=bleu4_train,\n",
    "                validation_data=bleu4_val,\n",
    "                y_label='BLEU-4 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#meteor\n",
    "meteor_train =  [meteor_dict['meteor'] for meteor_dict in train_meteor]\n",
    "meteor_val =  [meteor_dict['meteor'] for meteor_dict in val_meteor]\n",
    "visualise_graph(training_data=meteor_train,\n",
    "                validation_data=meteor_val,\n",
    "                y_label='Meteor Score',\n",
    "                x_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into csv\n",
    "data = zip(train_loss, bleu4_train, rouge_train, meteor_train, val_loss, bleu4_val, rouge_val, meteor_val)\n",
    "dataframe = pd.DataFrame(data, columns=[\"Train Loss\",\"Train BLEU\", \"Train ROUGE\", \"Train METEOR\",\"Val Loss\", \"Val BLEU\", \"Val ROUGE\", \"Val METEOR\"])\n",
    "dataframe.to_csv(f\"../resources/training_results/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained MaxVit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (256,256)\n",
    "embed_size = 512 \n",
    "vocab_size = len(dataset.vocab)\n",
    "num_heads = 8\n",
    "num_decoder_layers = 4\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 75\n",
    "save_every = 5\n",
    "dropout = 0.10\n",
    "model_name = \"TransformerModel_PT\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoderToDecoder(finetuned_model=None,\n",
    "                                embedding_size=embed_size,\n",
    "                                trg_vocab_size=vocab_size,\n",
    "                                num_heads=num_heads,\n",
    "                                num_decoder_layers=num_decoder_layers,\n",
    "                                dropout=dropout,\n",
    "                                train_cnn=False,\n",
    "                                device=device).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor= train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    image_size=img_size,\n",
    "    transformer=True,\n",
    "    batch_first=False,\n",
    "    vocabulary=dataset.vocab,\n",
    "    device=device,\n",
    "    num_epochs=75,\n",
    "    early_stopping_threshold = 0.001,\n",
    "    show_train_metrics=True,\n",
    "    save_every=25,\n",
    "    model_name=model_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "visualise_graph(training_data=train_loss,\n",
    "                validation_data=val_loss,\n",
    "                y_label='Loss',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#rouge-1\n",
    "rouge_train =  [rouge_dict['rouge1'] for rouge_dict in train_rouge]\n",
    "rouge_val =  [rouge_dict['rouge1'] for rouge_dict in val_rouge]\n",
    "visualise_graph(training_data=rouge_train,\n",
    "                validation_data=rouge_val,\n",
    "                y_label='Rouge-1 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#bleu4\n",
    "bleu4_train =  [bleu_dict['BLEU4'] for bleu_dict in train_bleu]\n",
    "bleu4_val =  [bleu_dict['BLEU4'] for bleu_dict in val_bleu]\n",
    "visualise_graph(training_data=bleu4_train,\n",
    "                validation_data=bleu4_val,\n",
    "                y_label='BLEU-4 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#meteor\n",
    "meteor_train =  [meteor_dict['meteor'] for meteor_dict in train_meteor]\n",
    "meteor_val =  [meteor_dict['meteor'] for meteor_dict in val_meteor]\n",
    "visualise_graph(training_data=meteor_train,\n",
    "                validation_data=meteor_val,\n",
    "                y_label='Meteor Score',\n",
    "                x_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into csv\n",
    "data = zip(train_loss, bleu4_train, rouge_train, meteor_train, val_loss, bleu4_val, rouge_val, meteor_val)\n",
    "dataframe = pd.DataFrame(data, columns=[\"Train Loss\",\"Train BLEU\", \"Train ROUGE\", \"Train METEOR\",\"Val Loss\", \"Val BLEU\", \"Val ROUGE\", \"Val METEOR\"])\n",
    "dataframe.to_csv(f\"../resources/training_results/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_size = 512 \n",
    "vocab_size = len(dataset.vocab)\n",
    "num_heads = 8\n",
    "num_decoder_layers = 4\n",
    "dropout = 0.10\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_maxvit = torch.load('../models/cnn/maxvit_20_best.pt')\n",
    "test_model1 = TransformerEncoderToDecoder(finetuned_model=finetuned_maxvit,\n",
    "                                embedding_size=embed_size,\n",
    "                                trg_vocab_size=vocab_size,\n",
    "                                num_heads=num_heads,\n",
    "                                num_decoder_layers=num_decoder_layers,\n",
    "                                dropout=dropout,\n",
    "                                train_cnn=False).to(device)\n",
    "test_criterion1 = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "test_model1.load_state_dict(torch.load('../models/image_captioning/TransformerModel_20FT.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model2 = TransformerEncoderToDecoder(finetuned_model=None,\n",
    "                                embedding_size=embed_size,\n",
    "                                trg_vocab_size=vocab_size,\n",
    "                                num_heads=num_heads,\n",
    "                                num_decoder_layers=num_decoder_layers,\n",
    "                                dropout=dropout,\n",
    "                                train_cnn=False).to(device)\n",
    "test_criterion2 = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "test_model2.load_state_dict(torch.load('../models/image_captioning/TransformerModel_PT.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [[test_model1, test_criterion1, (224,224)],\n",
    "              [test_model2, test_criterion2, (256,256)]]\n",
    "\n",
    "\n",
    "for idx, models in enumerate(all_models):\n",
    "    test_loss, test_bleu, test_rouge, test_meteor = eval(\n",
    "                                model=models[0],\n",
    "                                criterion=models[1],\n",
    "                                dataloader=test_dataloader,\n",
    "                                image_size=models[2],\n",
    "                                transformer=True,\n",
    "                                batch_first=False,\n",
    "                                vocabulary=test_dataset.vocab,\n",
    "                                device=device\n",
    "                                )\n",
    "    \n",
    "    print(f\"Test Model {idx}\\nTest Loss: {test_loss}\\nTest BLEU:{test_bleu}\\nTest Rouge:{test_rouge}\\nTest Meteor:{test_meteor}\\n----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Application of model inference & multiple model inference\n",
    "NOTE: the generate captions function should belong to your model class and have your own implementation depending on your model architecture\n",
    "<br/><br/>\n",
    "For easy standarisation you please make your `caption_image` function in your class have the following:\n",
    "1. Inputs: image, vocabulary, device, max_length\n",
    "2. Outputs: string prediction, attention (or None) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming mean and std are defined as follows:\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def unnormalize(image:np.array, mean:np.array, std:np.array):\n",
    "    \"\"\"\n",
    "    Function to unnormalize an image given its mean and std\n",
    "    \n",
    "    Args: \n",
    "        image (np.array): Numpy array of the image\n",
    "        mean (np.array): Numpy array of the mean \n",
    "        std (np.array): Numpy array of the std\n",
    "\n",
    "    Returns:\n",
    "        Unnormalised numpy array of the image\n",
    "    \"\"\"\n",
    "\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)    # for inplace operations\n",
    "    return image\n",
    "\n",
    "\n",
    "def caption_image(model, \n",
    "                  dataloader, \n",
    "                  image_size:tuple,\n",
    "                  vocabulary:Vocabulary, \n",
    "                  device:str, \n",
    "                  mean:np.array=np.array([0.485, 0.456, 0.406]), \n",
    "                  std:np.array=np.array([0.229, 0.224, 0.225]), \n",
    "                  num_batches:int=1, \n",
    "                  num_images:int=5, \n",
    "                  max_length:int=50, \n",
    "                  show_plot:bool=False):\n",
    "    \"\"\"\n",
    "    Function to generate model predictions from a dataloader\n",
    "\n",
    "    Arg:\n",
    "        model: model to general model prediction (ensure that your model has the function caption_image)\n",
    "        dataloader: dataset to generate prediction\n",
    "        image_size (tuple): image size of images for the model\n",
    "        vocabulary (Vocabulary): dataset vocabulary\n",
    "        device (str): cpu or cuda,\n",
    "        mean (np.array): Numpy array of the mean used for normalisation\n",
    "        std (np.array): Numpy array of the std used for normalisation\n",
    "        num_batches (int, optional): how many batches iterating from dataloader, defaults to 1\n",
    "        num_image (int, optional): how many images per batch to generate model prediction, defaults to 5\n",
    "        max_length (int, optional): maximum length of generated captions, defaults to 50\n",
    "        show_plot (bool, optional): show the image and generated captions in a plot, defaults to False\n",
    "    \n",
    "    Returns:\n",
    "        all_predictions (dict): Dictionary containing the list of all generated captions and actual captions\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    #dictionary containing all the generated predictions and actual predictions\n",
    "    all_predictions = {'Predicted': [], 'Possible Captions': []}\n",
    "\n",
    "    #iterate for num of batches we are testing\n",
    "    for j in range(num_batches):\n",
    "        #load images from dataloader\n",
    "        features, annotations, all_annotations = next(iter(dataloader))\n",
    "\n",
    "        #take first k from batch\n",
    "        for i in range(num_images):\n",
    "            features = torch.nn.functional.interpolate(features, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            image = features[i].unsqueeze(0).to(device)\n",
    "            \n",
    "            #generate captions from model\n",
    "            generated_caption, attention = model.caption_image(image, vocabulary, device, max_length=max_length)\n",
    "            \n",
    "            #plot image and captions\n",
    "            if show_plot:\n",
    "                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                img = features[i].squeeze()\n",
    "                img = unnormalize(img, mean, std)  # Unnormalize the image\n",
    "                img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'Model Prediction:\\n{\" \".join(generated_caption[1:-1])}\\n\\nAll Possible Predictions:\\n' + \"\\n\".join(all_annotations[i]), loc='left')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            all_predictions['Predicted'].append(generated_caption)\n",
    "            all_predictions['Possible Captions'].append(all_annotations[i])\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def multiple_model_captions(\n",
    "        model_list:list[str, tuple],\n",
    "        dataloader, \n",
    "        vocabulary:Vocabulary, \n",
    "        device:str, \n",
    "        mean:np.array=np.array([0.485, 0.456, 0.406]), \n",
    "        std:np.array=np.array([0.229, 0.224, 0.225]), \n",
    "        num_batches:int=1, \n",
    "        num_images:int=5, \n",
    "        max_length:int=50, \n",
    "        show_plot:bool=False):\n",
    "    \"\"\"\n",
    "    Function to generate model predictions for multiple models from the same dataloader\n",
    "\n",
    "    Arg:\n",
    "        model_list (list[str, tuple]): list of [model, img_size] to general model prediction (ensure that your model has the function caption_image)\n",
    "            eg. [[model1, (224,224)], [model2, (256,256)]]\n",
    "        dataloader: dataset to generate prediction\n",
    "        vocabulary (Vocabulary): dataset vocabulary\n",
    "        device (str): cpu or cuda,\n",
    "        mean (np.array): Numpy array of the mean used for normalisation\n",
    "        std (np.array): Numpy array of the std used for normalisation\n",
    "        num_batches (int, optional): how many batches iterating from dataloader, defaults to 1\n",
    "        num_image (int, optional): how many images per batch to generate model prediction, defaults to 5\n",
    "        max_length (int, optional): maximum length of generated captions, defaults to 50\n",
    "        show_plot (bool, optional): show the image and generated captions in a plot, defaults to False\n",
    "    \n",
    "    Returns:\n",
    "        all_predictions (dict): Dictionary containing the dictionary of all generated captions for each model and list of actual captions\n",
    "    \n",
    "    \"\"\"\n",
    "    for model, img_size in model_list:\n",
    "        model.eval()\n",
    "\n",
    "    #dictionary containing all the generated predictions and actual predictions\n",
    "    all_predictions = {'Predicted': {}, 'Possible Captions': []}\n",
    "\n",
    "    #iterate for num of batches we are testing\n",
    "    for j in range(num_batches):\n",
    "        #load images from dataloader\n",
    "        features, annotations, all_annotations = next(iter(dataloader))\n",
    "        \n",
    "        #take first k from batch\n",
    "        for i in range(num_images):\n",
    "            image = features[i].unsqueeze(0).to(device)\n",
    "            all_captions = []\n",
    "\n",
    "            for idx, (model, img_size) in enumerate(model_list):\n",
    "                #resize image\n",
    "                image = torch.nn.functional.interpolate(image, size=img_size, mode='bilinear') #resize image\n",
    "                #generate captions from model\n",
    "                generated_caption, attention = model.caption_image(image, vocabulary, device, max_length=max_length)\n",
    "                all_captions.append(\" \".join(generated_caption[1:-1]))\n",
    "                model_predictions = all_predictions['Predicted'].get(f\"model_{idx}\", [])\n",
    "                model_predictions.append(all_captions)\n",
    "                all_predictions['Predicted'][f\"model_{idx}\"] = model_predictions\n",
    "\n",
    "            #plot image and captions\n",
    "            if show_plot:\n",
    "                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                img = features[i].squeeze()\n",
    "                img = unnormalize(img, mean, std)  # Unnormalize the image\n",
    "                img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "\n",
    "                pred = '\\n'.join(all_captions)\n",
    "                annotation = '\\n'.join(all_annotations[i])\n",
    "\n",
    "                ax.set_title(f'All Model Predictions:\\n{pred}\\n\\nAll Possible Predictions:\\n{annotation}', loc='left')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            all_predictions['Possible Captions'].append(all_annotations[i])\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [[test_model1, (224,224)],\n",
    "              [test_model2, (256,256)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = multiple_model_captions(\n",
    "    model_list=model_list,\n",
    "    dataloader=test_dataloader,\n",
    "    vocabulary= test_dataset.vocab,\n",
    "    device=device,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    num_batches=10,\n",
    "    num_images=1,\n",
    "    max_length=50,\n",
    "    show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = caption_image(\n",
    "    model=test_model1,\n",
    "    dataloader=test_dataloader,\n",
    "    image_size=(224,224),\n",
    "    vocabulary=test_dataset.vocab,\n",
    "    device=device,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    num_batches=1,\n",
    "    num_images=10,\n",
    "    max_length=50,\n",
    "    show_plot=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
