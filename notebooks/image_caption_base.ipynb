{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Notebook on training a InceptionV3 Encoder and LSTM Decoder Image Captioning Model\n",
    "What to expect:\n",
    "1. Application of custom data loaders\n",
    "2. Application of base image captioning model (InceptionV3 Encoder, LSTM Decoder)\n",
    "3. Application of BLEU, ROGUE and METEOR\n",
    "4. Application of model training, validation and test for both finetuned and pretrained InceptV3\n",
    "4. Application of model inference\n",
    "\n",
    "- Note: Due to the limitation of relative imports in notebooks, the full codes from src/helper functions have been copy pasted here to prevent the need for running imports\n",
    "- Proper python functions in src/main.py will be rely on imports\n",
    "\n",
    "Image Size must be greater than (299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from PIL import Image, ImageOps\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Application of custom data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer to split sentences into a list of words\n",
    "word_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class Vocabulary():\n",
    "  \"\"\"\n",
    "  Class to convert the captions to index sequential tensors\n",
    "\n",
    "  Args:\n",
    "    freq_threshold (int, optional): How many times a word has to appear in dataset before it can be added to the vocabulary. Defaults to 2\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, freq_threshold:int=2):\n",
    "    self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"} #index to sentence\n",
    "    self.stoi = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\": 2, \"<UNK>\":3} #sentence to index\n",
    "    self.freq_threshold = freq_threshold #threshold for adding a word to the vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.itos)\n",
    "\n",
    "  @staticmethod\n",
    "  def tokenizer_eng(text):\n",
    "    #convert sentence to list of words\n",
    "    return [tok.text.lower() for tok in word_tokenizer.tokenizer(text)] #convert sentence to words\n",
    "\n",
    "\n",
    "  def build_vocabulary(self, sentence_list):\n",
    "    frequencies = {}\n",
    "    idx = 4 #0-3 are for special tokens\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "      for word in self.tokenizer_eng(sentence): #convert sentence to words\n",
    "        if word not in frequencies:\n",
    "          frequencies[word] = 1\n",
    "        else:\n",
    "          frequencies[word] += 1\n",
    "\n",
    "        if frequencies[word] == self.freq_threshold: #once met freq_threshold, add to vocab list\n",
    "          self.stoi[word] = idx\n",
    "          self.itos[idx] = word\n",
    "          idx += 1\n",
    "\n",
    "  def numericalize(self, text):\n",
    "    tokenized_text = self.tokenizer_eng(text) #convert annnotations to labels by converting each word to the index inside the vocab, else UNK tag\n",
    "    return [\n",
    "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "        for token in tokenized_text\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Class to setup the custom Dataset for pyTorch Dataloader\n",
    "\n",
    "    Args:\n",
    "        Note: the order of the csv_file and root_dir are directly related, csv_file[0] contains captions for images in root_dir[0]\n",
    "        \n",
    "        csv_file (list): Lists of path to CSV files with annotations.\n",
    "        root_dir (list): List of directory containing images.\n",
    "        img_size (tuple, optional): Image size in the format (width, height), defaults to (256,256)\n",
    "        transform (callable, optional): Optional torchvision transform to be applied on a sample, defaults to None\n",
    "        freq_threshold (int, optional): Freq threshold for Vocabulary Class, defaults to 2\n",
    "        vocabulary (Vocabulary, optional): Determines to use an existing vocabulary or create own, defaults to None\n",
    "    Returns:\n",
    "        image: transformed image\n",
    "        labels: tensor object of the labels\n",
    "        all_image_captions: list containing all the captions of the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file:list, root_dir:list, img_size:tuple=(256,256), transform=None, freq_threshold=2, vocabulary=None):\n",
    "        \n",
    "        #dataframe with col name ['image_filename', 'image_captions'] from csv file\n",
    "        self.annotations = pd.DataFrame()\n",
    "        #list containing the int boundary on which image path to look at\n",
    "        #list will containing the num of images in directory which is the boundary\n",
    "        self.root_dir_boundary = []\n",
    "        \n",
    "        for idx, label_files in enumerate(csv_file): \n",
    "            labels = pd.read_csv(label_files, index_col=0) #remove index col\n",
    "            self.annotations = pd.concat([self.annotations, labels], ignore_index=True) #merging annotations into 1 dataset\n",
    "\n",
    "            #getting the image boundary on which idx belongs to which image file path\n",
    "            if idx == 0: \n",
    "                self.root_dir_boundary.append(len(labels))\n",
    "            else:\n",
    "                #get the number of images in root directory and add with the previous to get the range of index that are in this filepath\n",
    "                self.root_dir_boundary.append(self.root_dir_boundary[idx-1] + len(labels))\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "        #initialise vocabulary\n",
    "        if vocabulary == None:\n",
    "            self.vocab = Vocabulary(freq_threshold)\n",
    "            self.vocab.build_vocabulary(self.annotations.iloc[:,1].to_list()) #build vocab with all captions\n",
    "        else:\n",
    "            self.vocab = vocabulary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.annotations.iloc[idx, 0] #Image name as column 0\n",
    "        #finding the correct root directory filepath in the list does the image belong to\n",
    "        image_dir_idx = 0\n",
    "        while idx >= self.root_dir_boundary[image_dir_idx] and image_dir_idx < len(self.root_dir_boundary):\n",
    "            image_dir_idx += 1\n",
    "\n",
    "        img_path = f\"{self.root_dir[image_dir_idx]}/{img_name}\"\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        # image = ImageOps.pad(image, self.img_size) #resize image\n",
    "        annotation = self.annotations.iloc[idx, 1] #Annotation as column 1\n",
    "        \n",
    "        #converting caption to index tensor\n",
    "        numercalized_annotations = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numercalized_annotations += self.vocab.numericalize(annotation)\n",
    "        numercalized_annotations.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        #create list of all captions associated with the image (for BLEU & ROUGE score)\n",
    "        all_img_captions = self.annotations[self.annotations['image_filename'] == img_name]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(numercalized_annotations), all_img_captions.iloc[:,1].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class padAnnotations():\n",
    "  \"\"\"\n",
    "  Collate function to pad all caption to the same length as max(len(caption)) in a batch\n",
    "\n",
    "  Args:\n",
    "    pad_idx (int): Index Label for the <PAD> token\n",
    "    batch_first (boolean, optional): Decide if the dataset labels should be batch first\n",
    "                                     Either returns (batch size, seq length) or (seq length, batch size)\n",
    "  \n",
    "  Returns:\n",
    "    img: batch image object\n",
    "    labels: batch of tensors of the captions, converted to the same length by adding <PAD>\n",
    "    all_labels: batch of lists of the captions of the images\n",
    "  \"\"\"\n",
    "  def __init__(self, pad_idx, batch_first = False):\n",
    "    self.batch_first = batch_first\n",
    "    self.pad_idx = pad_idx\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    imgs = [item[0].unsqueeze(0) for item in batch] \n",
    "    imgs = torch.cat(imgs, dim=0)\n",
    "    labels = [item[1] for item in batch]\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "    all_labels = [item[2] for item in batch]\n",
    "\n",
    "    return imgs, labels, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_img_caption_data(dataloader, batch_size: int, dataset: CustomDataset, num_batches: int = 1, num_samples: int=9):\n",
    "    \"\"\"\n",
    "    Function to visualise the dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader (dataloader object): Pytorch dataloader object to visualise\n",
    "        batch_size (int): Batch Size of dataloader\n",
    "        dataset (CustomDataset): dataset used to create dataloader (required to get the vocabulary)\n",
    "        num_batches (int, optional): How many batches to visualise, defaults to 1\n",
    "        num_samples (int, optional): How many images per batch to visualise, defaults to 9\n",
    "\n",
    "    Returns:\n",
    "        Plot of image with its caption and the list of the captions to the image\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        train_features, train_labels, train_all_captions = next(iter(dataloader))\n",
    "        if i == 0:  # Print shape\n",
    "            print(f\"Feature batch shape: {train_features.size()}\")\n",
    "            print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "            print(f\"All captions batch size: {len(train_all_captions)}\")\n",
    "\n",
    "        for j in range(num_samples):\n",
    "            #Prepare image to print\n",
    "            img = train_features[j].squeeze()\n",
    "            img = np.transpose(img.numpy(), (1, 2, 0))  #Convert from (channel, height, width) to (height, width, channel) for matplotlib\n",
    "\n",
    "            #Transpose train_labels and handle out of range indices\n",
    "            label = train_labels[:, j] if train_labels.size(0) != batch_size else train_labels[0]\n",
    "            string_label = [dataset.vocab.itos[idx] for idx in label.tolist()]\n",
    "            actual_caption = \" \".join([token for token in string_label if token not in ['<PAD>', '<SOS>', '<EOS>']])\n",
    "\n",
    "            #Create a new plot for each image\n",
    "            fig, ax = plt.subplots(figsize=(5, 5))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'Caption: {actual_caption}\\n\\nAll Possible Captions:\\n' + \"\\n\".join(train_all_captions[j]), loc='left')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Setup\n",
    "image_size = (299,299)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define transforms for image preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#No data augmentation\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of when trying more than 1 dataset source\n",
    "\n",
    "#Create a custom dataset with more than 1 dataset source\n",
    "test_dataset = CustomDataset(csv_file=['../input/Landscape/Train/Labels/Blip_Label.csv', '../input/Landscape/Train/Labels/Kosmos_Label.csv'],\n",
    "                        root_dir=['../input/Landscape/Train/Images', '../input/Landscape/Train/Images'],\n",
    "                        transform=train_transform,\n",
    "                        img_size=image_size)\n",
    "\n",
    "\n",
    "#Create a PyTorch DataLoader\n",
    "#batch first\n",
    "bf_test_dataloader = DataLoader(test_dataset, \n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = test_dataset.vocab.stoi[\"<PAD>\"], \n",
    "                            batch_first=True\n",
    "                        ))\n",
    "\n",
    "#not batch first\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = test_dataset.vocab.stoi[\"<PAD>\"]\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img_caption_data(bf_test_dataloader, batch_size, test_dataset, num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Setup\n",
    "image_size = (299,299)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define transforms for image preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#No data augmentation\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual dataset for this notebook\n",
    "\n",
    "#Create train dataset\n",
    "dataset = CustomDataset(csv_file=['../input/Landscape/Train/Labels/Blip_Label_Clean.csv', '../input/Landscape/Train/Labels/Kosmos_Label_Clean.csv', '../input/FilteredFlicker/Train/Labels/Label.csv'],\n",
    "                        root_dir=['../input/Landscape/Train/Images', '../input/Landscape/Train/Images', '../input/FilteredFlicker/Train/Images'],\n",
    "                        transform=train_transform,\n",
    "                        img_size=image_size)\n",
    "\n",
    "#Create val and test dataset, note that\n",
    "#1. transform uses eval transform with no data augmentation\n",
    "#2. vocab is the original train dataset vocab (to map the correct index)\n",
    "val_dataset = CustomDataset(csv_file=['../input/Landscape/Validation/Labels/Blip_Label_Clean.csv', '../input/Landscape/Validation/Labels/Kosmos_Label_Clean.csv', '../input/FilteredFlicker/Validation/Labels/Label.csv'],\n",
    "                        root_dir=['../input/Landscape/Validation/Images', '../input/Landscape/Validation/Images', '../input/FilteredFlicker/Validation/Images'],\n",
    "                        transform=eval_transform,\n",
    "                        img_size=image_size,\n",
    "                        vocabulary=dataset.vocab)\n",
    "\n",
    "#only focusing on landscape image, please add flicker30k if u would like\n",
    "test_dataset = CustomDataset(csv_file=['../input/Landscape/Test/Labels/Blip_Label_Clean.csv', '../input/Landscape/Test/Labels/Kosmos_Label_Clean.csv'],\n",
    "                        root_dir=['../input/Landscape/Test/Images', '../input/Landscape/Test/Images'],\n",
    "                        transform=eval_transform,\n",
    "                        img_size=image_size,\n",
    "                        vocabulary=dataset.vocab)\n",
    "\n",
    "\n",
    "#create dataloaders, just update the dataset it is from\n",
    "train_dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=False\n",
    "                        ))\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=False\n",
    "                        ))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn = padAnnotations(\n",
    "                            pad_idx = dataset.vocab.stoi[\"<PAD>\"],\n",
    "                            batch_first=False\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img_caption_data(train_dataloader, batch_size, dataset, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img_caption_data(val_dataloader, batch_size, val_dataset, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_img_caption_data(test_dataloader, batch_size, test_dataset, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Vocabulary as a Pickle file\n",
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_object(dataset.vocab, '../resources/Vocabulary.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Application of base image captioning model\n",
    "- implementing a CNN->LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN encoder (using inceptionV3)\n",
    "class EncoderCNN(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  InceptionV3 CNN Encoder Model, last layer is always trainable\n",
    "\n",
    "  Args:\n",
    "      finetuned_model: Finetuned InceptionV3 model, else None\n",
    "      embed_size (int): Embedding dimension to convert features size to\n",
    "      train_CNN (bool, optional): Determines if the entire CNN model will be unfreeze and trained during the training\n",
    "  \"\"\"\n",
    "  def __init__(self, finetuned_model, embed_size:int, train_CNN:bool=False):\n",
    "    super(EncoderCNN, self).__init__()\n",
    "    #load pre-trained model\n",
    "    if finetuned_model != None:\n",
    "        self.inception = list(finetuned_model.children())[0]\n",
    "    else:\n",
    "        self.inception = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "    self.inception.aux_logits = False\n",
    "\n",
    "    #converting the last layer of inception to linear layer [inception last layer input, embed size]\n",
    "    self.inception.fc = torch.nn.Linear(self.inception.fc.in_features, embed_size) \n",
    "    #Train the feature map, the rest depends on train_CNN\n",
    "    for name, param in self.inception.named_parameters():\n",
    "      if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "        param.requires_grad = True #finetuning the last layer\n",
    "      else:\n",
    "        param.requires_grad = train_CNN\n",
    "\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "\n",
    "  def forward(self, images):\n",
    "    features = self.inception(images)\n",
    "    return self.dropout(self.relu(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  LSTM decoder model\n",
    "\n",
    "  Args:\n",
    "      embed_size (int): Embedding dimension to embed words\n",
    "      hidden_size (int): Hidden state dimension for LSTM\n",
    "      vocab_size (int): Total number of unique vocab\n",
    "      num_layers (int): Total number of LSTM layers\n",
    "  \"\"\"\n",
    "  def __init__(self, embed_size:int, hidden_size:int, vocab_size:int, num_layers:int):\n",
    "    super(DecoderRNN, self).__init__()\n",
    "    self.embed = torch.nn.Embedding(vocab_size, embed_size) #embed / tokenize the word\n",
    "    self.lstm = torch.nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "    self.linear = torch.nn.Linear(hidden_size, vocab_size)  #classification layer\n",
    "    self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "  def forward(self, features, captions):\n",
    "    embeddings = self.dropout(self.embed(captions))\n",
    "    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "\n",
    "    hiddens, _ = self.lstm(embeddings)\n",
    "    outputs = self.linear(hiddens)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Model that merges Encoder CNN to Decoder RNN\n",
    "\n",
    "  Args:\n",
    "      embed_size (int): Embedding dimension to embed words\n",
    "      hidden_size (int): Hidden state dimension for LSTM\n",
    "      vocab_size (int): Total number of unique vocab\n",
    "      num_layers (int): Total number of LSTM layers\n",
    "      train_cnn (bool, optional): Determines if unfreezing entire InceptionV3 model, defaults to False\n",
    "  \"\"\"\n",
    "  def __init__(self, finetuned_model, embed_size:int, hidden_size:int, vocab_size:int, num_layers:int, train_cnn:bool=False):\n",
    "    super(CNNtoRNN, self).__init__()\n",
    "    self.encoderCNN = EncoderCNN(finetuned_model, embed_size, train_cnn)\n",
    "    self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "  #for training with a caption\n",
    "  def forward(self, images, captions):\n",
    "    features = self.encoderCNN(images)\n",
    "    outputs = self.decoderRNN(features, captions)\n",
    "    return outputs\n",
    "\n",
    "  #for prediction where there is a semi caption and they have to continue the caption\n",
    "  def caption_image(self, image, vocabulary, device, max_length=50):\n",
    "    result_caption = []\n",
    "\n",
    "    with torch.no_grad(): #no training\n",
    "      x = self.encoderCNN(image.to(device)).unsqueeze(0) #image as input\n",
    "      states = None\n",
    "\n",
    "      for _ in range(max_length):\n",
    "        hiddens, states = self.decoderRNN.lstm(x, states) #image is the initial state for the LSTM\n",
    "        output = self.decoderRNN.linear(hiddens.squeeze(0)) #get the output from the LSTM for the first word\n",
    "        predicted = output.argmax(1) #first word\n",
    "\n",
    "        result_caption.append(predicted.item())\n",
    "        x = self.decoderRNN.embed(predicted).unsqueeze(0) #update the input to the lstm to now be the first word\n",
    "\n",
    "        if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "          break\n",
    "\n",
    "    return [vocabulary.itos[idx] for idx in result_caption], None  #get the string from the vocabulary instead of the indices, no attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Application of BLEU, ROGUE and METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def get_bleu_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the bleu 1-4 score of a corpus\n",
    "\n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {\"BLEU1\":, \"BLEU2\": ,\"BLEU3\": ,\"BLEU4\": }\n",
    "    \"\"\"\n",
    "    #Tokenize the predictions and references\n",
    "    predicted = [pred.split() for pred in predicted]\n",
    "    references = [[ref.split() for ref in refs] for refs in references]\n",
    "\n",
    "\n",
    "    BLEU1 = corpus_bleu(references, predicted, weights=(1, 0, 0, 0))\n",
    "    BLEU2 = corpus_bleu(references, predicted, weights=(0.5, 0.5, 0, 0))\n",
    "    BLEU3 = corpus_bleu(references, predicted, weights=(0.33, 0.33, 0.33, 0))\n",
    "    BLEU4 = corpus_bleu(references, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return {\"BLEU1\": BLEU1, \"BLEU2\": BLEU2, \"BLEU3\": BLEU3, \"BLEU4\": BLEU4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def get_rouge_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the rogue1, rogue2, rogueL, rogueLSum scores\n",
    "        \n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {'rouge1': , 'rouge2': , 'rougeL': , 'rougeLsum': }\n",
    "    \n",
    "    \"\"\"\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results = rouge.compute(predictions=predicted, references=references)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def get_meteor_score(predicted:list, references:list):\n",
    "    \"\"\"\n",
    "    Calculate the meteor scores\n",
    "        \n",
    "    Args: \n",
    "        predicted (list): List of each individual prediction\n",
    "            eg. [\n",
    "                    \"Transformers Transformers are fast plus efficient\", \n",
    "                    \"Good Morning\", \n",
    "                    \"I am waiting for new Transformers\"\n",
    "                ] \n",
    "\n",
    "        references (list): Nested list of references of each prediction\n",
    "\n",
    "            eg. [\n",
    "                    [\n",
    "                        \"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "                        \"Transformers are awesome because they are fast to execute\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"Good Morning Transformers\", \n",
    "                        \"Morning Transformers\"\n",
    "                    ], \n",
    "                    [\n",
    "                        \"People are eagerly waiting for new Transformer models\", \n",
    "                         \"People are very excited about new Transformers\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    Returns:\n",
    "        results (dictionary): Dictionary in the format of {'meteor':}\n",
    "    \n",
    "    \"\"\"\n",
    "    meteor = evaluate.load('meteor')\n",
    "    results = meteor.compute(predictions=predicted, references=references)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    \"Transformers Transformers are fast plus efficient\", \n",
    "    \"Good Morning\", \n",
    "    \"I am waiting for new Transformers\"\n",
    "]\n",
    "references = [\n",
    "    [\"HuggingFace Transformers are quick, efficient and awesome\", \n",
    "     \"Transformers are awesome because they are fast to execute\"], \n",
    "    [\"Good Morning Transformers\", \"Morning Transformers\"], \n",
    "    [\"People are eagerly waiting for new Transformer models\", \n",
    "     \"People are very excited about new Transformers\"]\n",
    "]\n",
    "\n",
    "print(get_bleu_score(predictions, references))\n",
    "print(get_rouge_score(predictions, references))\n",
    "print(get_meteor_score(predictions, references))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Application of model training, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(outputs, batch_first:bool, vocabulary:Vocabulary):\n",
    "    \"\"\"\n",
    "    Function to convert model tensor outputs to sentences\n",
    "\n",
    "    Args:\n",
    "        outputs (torch tensor object): Model's output to be decoded, either in size (seq len, batch, vocab_size) or (batch, seq len, vocab_size)\n",
    "        batch_first (bool): Boolean of if dataloader was configured to batch_first\n",
    "        vocabulary (Vocabulary): dataset Vocabulary Class for decoding\n",
    "\n",
    "    Returns:\n",
    "        list of predicted sentences each corresponding to 1 sample in the batch\n",
    "            - will be of length (batch_size)\n",
    "            eg. ['predicted sentence 1 for sample 1', ...'predicted sentence N for sample N']\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    all_prediction = []\n",
    "    predicted_tokens = outputs.argmax(-1) #flatten vocab size dimensions\n",
    "    if not batch_first:\n",
    "        predicted_tokens = predicted_tokens.T \n",
    "    \n",
    "    for sentence_tokens in predicted_tokens:\n",
    "        sentence_tokens = sentence_tokens.tolist()\n",
    "\n",
    "        try:\n",
    "            #cropping predicted sentence to first EOS\n",
    "            eos_index = sentence_tokens.index(vocabulary.stoi['<EOS>']) #get first instance of <EOS> to crop sentence accordingly\n",
    "            predicted_sentence = sentence_tokens[:eos_index]\n",
    "        except:\n",
    "            predicted_sentence = sentence_tokens\n",
    "\n",
    "        try:\n",
    "            #getting predicted_sentence by remove <SOS>\n",
    "            predicted_sentence.remove(vocabulary.stoi['<SOS>'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        all_prediction.append(\" \".join([vocabulary.itos[idx] for idx in predicted_sentence]))\n",
    "\n",
    "    return all_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, \n",
    "         criterion, \n",
    "         dataloader, \n",
    "         image_size:tuple,\n",
    "         transformer:bool,\n",
    "         batch_first:bool, \n",
    "         vocabulary:Vocabulary, \n",
    "         device:str):\n",
    "    \"\"\"\n",
    "    Function to evaluate model performance\n",
    "\n",
    "    Args:\n",
    "        model: The model that is to be evaluated\n",
    "        criterion: Loss criterion of the model\n",
    "        dataloader: validation / test dataset\n",
    "        image_size (tuple): image size of model\n",
    "        transformer (bool): boolean if decoder is a transformer\n",
    "        batch_first (bool): boolean if dataloader samples tensor are (batch, seq len) or (seq len, batch)\n",
    "        vocabulary (Vocabulary): dataset vocabulary class\n",
    "        device (str): cpu or cuda\n",
    "\n",
    "    Returns:\n",
    "        avg_val_loss: average validation loss\n",
    "        Bleu_score: dictionary of BLEU 1-4 score\n",
    "        Rouge_score: dictionary of Rouge  1,2,L,LSum score\n",
    "        Meteor_score: dictionary of Meteor Score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0\n",
    "\n",
    "    #BLEU predictions container\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (imgs, annotations, all_annotations) in enumerate(dataloader):\n",
    "            #getting img and annotations\n",
    "            imgs = torch.nn.functional.interpolate(imgs, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            imgs = imgs.to(device)\n",
    "            annotations = annotations.to(device)\n",
    "            \n",
    "            if transformer:\n",
    "                #running model prediction\n",
    "                outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                targets = annotations[1:].reshape(-1)\n",
    "                #updating model parameters\n",
    "                loss = criterion(outputs.view(-1, len(vocabulary)), targets)\n",
    "            \n",
    "            else:\n",
    "                if not batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                    \n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.reshape(-1, outputs.shape[2]), annotations.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "                if batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs, atten_weights = model(imgs, annotations[:, :-1]) #training model to guess the last word\n",
    "                    targets = annotations[:, 1:]\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.view(-1, len(vocabulary)), targets.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            #get model predictions and update\n",
    "            predictions.extend(decode_predictions(outputs, batch_first, vocabulary))\n",
    "\n",
    "            #update references\n",
    "            references.extend(all_annotations)\n",
    "\n",
    "        Bleu_score = get_bleu_score(predictions, references)\n",
    "        Rouge_score = get_rouge_score(predictions, references)\n",
    "        Meteor_score = get_meteor_score(predictions, references)\n",
    "\n",
    "        return total_val_loss/(idx+1), Bleu_score, Rouge_score, Meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          criterion, \n",
    "          optimiser, \n",
    "          train_dataloader, \n",
    "          val_dataloader, \n",
    "          image_size: tuple,\n",
    "          batch_first:bool, \n",
    "          transformer:bool,\n",
    "          vocabulary:Vocabulary, \n",
    "          device:str, \n",
    "          num_epochs:int, \n",
    "          early_stopping_threshold: float=0.003,\n",
    "          show_train_metrics:bool=None, \n",
    "          save_every:int=None,\n",
    "          model_name:str=None, \n",
    "          overwrite:bool=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to train the model\n",
    "    If change in val loss is less than 0.01 for 2 epochs in a row, stop training\n",
    "\n",
    "    Args:\n",
    "        model: The model that is to be evaluated\n",
    "        criterion: Loss criterion of the model\n",
    "        optimiser: Optimiser function of the model\n",
    "        train_dataloader: Train dataset\n",
    "        val_dataloader: Validation dataset, use None if no Validation dataset\n",
    "        image_size (tuple): image size of model\n",
    "        batch_first (bool): Boolean if dataloader samples tensor are (batch, seq len) or (seq len, batch)\n",
    "        transformer (bool): Boolean if decoder is a transformer\n",
    "        vocabulary (Vocabulary): Dataset vocabulary class\n",
    "        device (str): cpu or cuda\n",
    "        num_epochs (int): Number of epochs for training\n",
    "        early_stopping_threshold (float, optional): Threshold for early stopping, defaults to 0.003\n",
    "        show_train_metrics (bool, optional): Booleon on should calculate BLEU & Rouge score during training, defaults to False\n",
    "        save_every (int, optional): Save model after every ___ epochs, defaults to None (no saving)\n",
    "        model_name (str, optional): Model Name to be saved after, required if save_every != None, model will be saved as (model_name)_epoch or just model_name\n",
    "        overwrite (bool, optional): Boolean on overwriting model saves or saving each specific epoch as a new model, defaults to False\n",
    "    \n",
    "    Returns\n",
    "        train_loss: list of average training loss per epoch\n",
    "        train_bleu: list of dictionary of training BLEU score per epoch, [] if show_train_metric = False\n",
    "        train_rouge: list of dictionary of training Rouge score per epoch, [] if show_train_metric = False\n",
    "        train_meteor: list of dictionary of training Meteor score per epoch, [] if show_train_metric = False\n",
    "        val_loss: list of average validation loss per epoch, [] if val_dataloader = None\n",
    "        val_bleu: list of dictionary of validation BLEU score per epoch, [] if val_dataloader = None\n",
    "        val_rouge: list of dictionary of validation Rouge score per epoch, [] if val_dataloader = None\n",
    "        val_meteor: list of dictionary of validation Meteor score per epoch, [] if val_dataloader = None\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialise results container\n",
    "    train_loss = []\n",
    "    train_bleu = []\n",
    "    train_rouge = []\n",
    "    train_meteor = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_bleu = []\n",
    "    val_rouge = []\n",
    "    val_meteor = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        #BLEU predictions container\n",
    "        predictions = []\n",
    "        references = []\n",
    "\n",
    "        #start model training\n",
    "        model.train()\n",
    "        for idx, (imgs, annotations, all_annotations) in enumerate(train_dataloader):\n",
    "            \n",
    "            #getting img and annotations\n",
    "            imgs = torch.nn.functional.interpolate(imgs, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            imgs = imgs.to(device)\n",
    "            annotations = annotations.to(device)\n",
    "\n",
    "            if transformer:\n",
    "                #running model prediction\n",
    "                outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                targets = annotations[1:].reshape(-1)\n",
    "                #updating model parameters\n",
    "                loss = criterion(outputs.view(-1, len(vocabulary)), targets)\n",
    "            \n",
    "            else:\n",
    "                if not batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs = model(imgs, annotations[:-1]) #training model to guess the last word\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.reshape(-1, outputs.shape[2]), annotations.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "                if batch_first:\n",
    "                    #running model prediction\n",
    "                    outputs, atten_weights = model(imgs, annotations[:, :-1]) #training model to guess the last word\n",
    "                    targets = annotations[:, 1:]\n",
    "                    #updating model parameters\n",
    "                    loss = criterion(outputs.view(-1, len(vocabulary)), targets.reshape(-1)) #reshape output (seq_len, N, vocabulary_size) to (N, vocabulary_size)\n",
    "                \n",
    "            optimiser.zero_grad() #remove optimiser gradient\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            #calculate loss and update it for each batch\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            if show_train_metrics:\n",
    "                #get model predictions and update\n",
    "                predictions.extend(decode_predictions(outputs, batch_first, vocabulary))\n",
    "\n",
    "                #update references\n",
    "                references.extend(all_annotations)\n",
    "\n",
    "        if show_train_metrics:   \n",
    "            #calculating bleu and rouge score\n",
    "            Bleu_score = get_bleu_score(predictions, references)\n",
    "            Rouge_score = get_rouge_score(predictions, references)\n",
    "            Meteor_score = get_meteor_score(predictions, references)\n",
    "            train_bleu.append(Bleu_score)\n",
    "            train_rouge.append(Rouge_score)\n",
    "            train_meteor.append(Meteor_score)\n",
    "\n",
    "        #updating values\n",
    "        train_loss.append(total_train_loss/(idx+1))\n",
    "\n",
    "        if val_dataloader != None:\n",
    "            #validation\n",
    "            avg_val_loss, val_bleu_score, val_rouge_score, val_meteor_score = eval(\n",
    "                                                                            model=model,\n",
    "                                                                            criterion=criterion,\n",
    "                                                                            dataloader=val_dataloader,\n",
    "                                                                            image_size=image_size,\n",
    "                                                                            transformer=transformer,\n",
    "                                                                            batch_first=batch_first,\n",
    "                                                                            vocabulary=vocabulary,\n",
    "                                                                            device=device\n",
    "                                                                        )\n",
    "            \n",
    "            val_loss.append(avg_val_loss)\n",
    "            val_bleu.append(val_bleu_score)\n",
    "            val_rouge.append(val_rouge_score)\n",
    "            val_meteor.append(val_meteor_score)\n",
    "\n",
    "        #printing progress\n",
    "        if num_epochs <= 10 or (num_epochs >10 and (epoch+1)%5 == 0):\n",
    "            print(f\"Epoch {epoch+1} completed\\navg training loss per batch: {total_train_loss/(idx+1)}\")\n",
    "            \n",
    "            if show_train_metrics:\n",
    "                print(f\"train bleu score:{Bleu_score}\\ntrain rouge score: {Rouge_score}\\ntrain meteor score: {Meteor_score}\\n\")\n",
    "\n",
    "            if val_dataloader != None:\n",
    "                print(f\"avg validation loss per batch: {avg_val_loss}\\nval bleu score: {val_bleu_score}\\nval rouge score: {val_rouge_score}\\nval meteor score: {val_meteor_score}\")\n",
    "\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "            \n",
    "        #saving model every x\n",
    "        if save_every != None and (epoch+1)%save_every == 0:\n",
    "            try:\n",
    "                if overwrite:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}.pt\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_{epoch+1}.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save model at epoch {epoch+1}\")\n",
    "\n",
    "        \n",
    "        #saving best model\n",
    "        if (len(val_loss) > 1) and val_loss[-1] < min(val_loss[:-1]):\n",
    "            try:\n",
    "                torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_best.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save best model\")\n",
    "        \n",
    "\n",
    "        #early stopping\n",
    "        if (len(val_loss) >= 3) and abs(val_loss[-2] - val_loss[-1]) < early_stopping_threshold and abs(val_loss[-3] - val_loss[-2]) < early_stopping_threshold:\n",
    "            print(f\"validation loss did not decrease, stopping training at epoch {epoch +1}\")\n",
    "            try:\n",
    "                if overwrite:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}.pt\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), f\"../models/image_captioning/{model_name}_{epoch+1}.pt\")\n",
    "            except:\n",
    "                print(f\"Unable to save model at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_graph(training_data:list, validation_data:list, y_label:str, x_label:str, ylim:list=None):\n",
    "    \"\"\"\n",
    "    Plot a line graph against training and validation data\n",
    "\n",
    "    Args:\n",
    "        training_data (list): Training data to be plotted\n",
    "        validation_data (list): Validation data to be plotted\n",
    "        y_label (str): Label of y axis\n",
    "        x_label (str): Label of x axis\n",
    "        ylim (list, optional): Range of y axis, defaults to None\n",
    "    \"\"\"\n",
    "    #plotting line graph of training data and validation data\n",
    "    plt.plot(training_data, label='Train')\n",
    "    plt.plot(validation_data, label='Validation')\n",
    "\n",
    "    #labels and title\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(f'{y_label} against {x_label}')\n",
    "    \n",
    "    if ylim != None:\n",
    "        plt.ylim(ylim)\n",
    "        \n",
    "    #show legend\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained Model Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedCNNModels(torch.nn.Module):\n",
    "    def __init__(self, model_type:str, num_unfreeze:int, num_class:int):\n",
    "        super(PreTrainedCNNModels, self).__init__()\n",
    "        \"\"\"\n",
    "        Class that contains InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit fine tuned models\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Determines which pre-trained models to use\n",
    "                              Must be: InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit\n",
    "            num_unfreeze (int): Number of layers to unfreeze and finetune\n",
    "            num_class (int): Number of output classes for the classification\n",
    "        \"\"\"\n",
    "        #selecting model type\n",
    "        if model_type == 'InceptionV3':\n",
    "            self.model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "            self.model.aux_logits = False\n",
    "\n",
    "        elif model_type == 'Resnet50':\n",
    "            self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'Resnet152':\n",
    "            self.model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'EfficientNet':\n",
    "            self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        \n",
    "        elif model_type == 'VGG16':\n",
    "            self.model = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "        elif model_type == 'MaxVit':\n",
    "            self.model = models.maxvit_t(weights=models.MaxVit_T_Weights.DEFAULT)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid model type chosen. Please select one of the following\\n[InceptionV3, Resnet50, Resnet152, EfficientNet, DenseNet, VGG16, MaxVit]\")\n",
    "\n",
    "        \n",
    "        #modifying final layer\n",
    "        if model_type in ['InceptionV3', 'Resnet50', 'Resnet152']:\n",
    "            self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_class)\n",
    "\n",
    "        elif model_type == 'DenseNet':\n",
    "            self.model.classifier = torch.nn.Linear(self.model.classifier.in_features, num_class)\n",
    "\n",
    "        else:\n",
    "            self.model.classifier[-1] = torch.nn.Linear(self.model.classifier[-1].in_features, num_class)\n",
    "\n",
    "\n",
    "        model_paramteres = list(self.model.parameters())\n",
    "        #unfreeze last num_unfreeze layers\n",
    "        for param in model_paramteres[-num_unfreeze:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        #freeze rest of the layers\n",
    "        for param in model_paramteres[:-num_unfreeze]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finetuned InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_incept = torch.load('../models/cnn/incept_12_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (299,299)\n",
    "embed_size = 256 \n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 3\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 100\n",
    "save_every = 25\n",
    "model_name = \"PTBaseModel\"\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNtoRNN(\n",
    "    finetuned_model=finetuned_incept,\n",
    "    embed_size=embed_size, \n",
    "    hidden_size=hidden_size, \n",
    "    vocab_size=vocab_size, \n",
    "    num_layers=num_layers,\n",
    "    train_cnn=False).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor= train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    image_size=img_size,\n",
    "    transformer=False,\n",
    "    batch_first=False,\n",
    "    vocabulary=dataset.vocab,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_threshold = 0.001,\n",
    "    show_train_metrics=True,\n",
    "    save_every=save_every,\n",
    "    model_name=model_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "visualise_graph(training_data=train_loss,\n",
    "                validation_data=val_loss,\n",
    "                y_label='Loss',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#rouge-1\n",
    "rouge_train =  [rouge_dict['rouge1'] for rouge_dict in train_rouge]\n",
    "rouge_val =  [rouge_dict['rouge1'] for rouge_dict in val_rouge]\n",
    "visualise_graph(training_data=rouge_train,\n",
    "                validation_data=rouge_val,\n",
    "                y_label='Rouge-1 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#bleu4\n",
    "bleu4_train =  [bleu_dict['BLEU4'] for bleu_dict in train_bleu]\n",
    "bleu4_val =  [bleu_dict['BLEU4'] for bleu_dict in val_bleu]\n",
    "visualise_graph(training_data=bleu4_train,\n",
    "                validation_data=bleu4_val,\n",
    "                y_label='BLEU-4 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#meteor\n",
    "meteor_train =  [meteor_dict['meteor'] for meteor_dict in train_meteor]\n",
    "meteor_val =  [meteor_dict['meteor'] for meteor_dict in val_meteor]\n",
    "visualise_graph(training_data=meteor_train,\n",
    "                validation_data=meteor_val,\n",
    "                y_label='Meteor Score',\n",
    "                x_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into csv\n",
    "data = zip(train_loss, bleu4_train, rouge_train, meteor_train, val_loss, bleu4_val, rouge_val, meteor_val)\n",
    "dataframe = pd.DataFrame(data, columns=[\"Train Loss\",\"Train BLEU\", \"Train ROUGE\", \"Train METEOR\",\"Val Loss\", \"Val BLEU\", \"Val ROUGE\", \"Val METEOR\"])\n",
    "dataframe.to_csv(f\"../resources/training_results/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (299,299)\n",
    "embed_size = 256 \n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 3\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 100\n",
    "save_every = 25\n",
    "model_name = \"BaseModel\"\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNtoRNN(\n",
    "    finetuned_model=None,\n",
    "    embed_size=embed_size, \n",
    "    hidden_size=hidden_size, \n",
    "    vocab_size=vocab_size, \n",
    "    num_layers=num_layers,\n",
    "    train_cnn=False).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_bleu, train_rouge, train_meteor, val_loss, val_bleu, val_rouge, val_meteor= train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    image_size=img_size,\n",
    "    transformer=False,\n",
    "    batch_first=False,\n",
    "    vocabulary=dataset.vocab,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_threshold = 0.001,\n",
    "    show_train_metrics=True,\n",
    "    save_every=save_every,\n",
    "    model_name=model_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "visualise_graph(training_data=train_loss,\n",
    "                validation_data=val_loss,\n",
    "                y_label='Loss',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#rouge-1\n",
    "rouge_train =  [rouge_dict['rouge1'] for rouge_dict in train_rouge]\n",
    "rouge_val =  [rouge_dict['rouge1'] for rouge_dict in val_rouge]\n",
    "visualise_graph(training_data=rouge_train,\n",
    "                validation_data=rouge_val,\n",
    "                y_label='Rouge-1 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#bleu4\n",
    "bleu4_train =  [bleu_dict['BLEU4'] for bleu_dict in train_bleu]\n",
    "bleu4_val =  [bleu_dict['BLEU4'] for bleu_dict in val_bleu]\n",
    "visualise_graph(training_data=bleu4_train,\n",
    "                validation_data=bleu4_val,\n",
    "                y_label='BLEU-4 Score',\n",
    "                x_label='Epoch')\n",
    "\n",
    "#meteor\n",
    "meteor_train =  [meteor_dict['meteor'] for meteor_dict in train_meteor]\n",
    "meteor_val =  [meteor_dict['meteor'] for meteor_dict in val_meteor]\n",
    "visualise_graph(training_data=meteor_train,\n",
    "                validation_data=meteor_val,\n",
    "                y_label='Meteor Score',\n",
    "                x_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into csv\n",
    "data = zip(train_loss, bleu4_train, rouge_train, meteor_train, val_loss, bleu4_val, rouge_val, meteor_val)\n",
    "dataframe = pd.DataFrame(data, columns=[\"Train Loss\",\"Train BLEU\", \"Train ROUGE\", \"Train METEOR\",\"Val Loss\", \"Val BLEU\", \"Val ROUGE\", \"Val METEOR\"])\n",
    "dataframe.to_csv(f\"../resources/training_results/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "img_size = (299,299)\n",
    "embed_size = 256 \n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 3\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_incept = torch.load('../models/cnn/incept_12_best.pt')\n",
    "test_model1 = CNNtoRNN(finetuned_incept, embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "test_criterion1 = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "test_model1.load_state_dict(torch.load('../models/image_captioning/PTBaseModel.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading example model\n",
    "test_model2 = CNNtoRNN(None, embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "test_criterion2 = torch.nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "test_model2.load_state_dict(torch.load('../models/image_captioning/BaseModel.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [[test_model1, test_criterion1, (299,299)],\n",
    "              [test_model2, test_criterion2, (299,299)]]\n",
    "\n",
    "\n",
    "for idx, models in enumerate(all_models):\n",
    "    test_loss, test_bleu, test_rouge, test_meteor = eval(model=models[0],\n",
    "                                criterion=models[1],\n",
    "                                dataloader=test_dataloader,\n",
    "                                image_size=models[2],\n",
    "                                transformer=False,\n",
    "                                batch_first=False,\n",
    "                                vocabulary=test_dataset.vocab,\n",
    "                                device=device\n",
    "                                )\n",
    "    \n",
    "    print(f\"Test Model {idx}\\nTest Loss: {test_loss}\\nTest BLEU:{test_bleu}\\nTest Rouge:{test_rouge}\\nTest Meteor:{test_meteor}\\n----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Application of model inference\n",
    "NOTE: the generate captions function should belong to your model class and have your own implementation depending on your model architecture\n",
    "<br/><br/>\n",
    "For easy standarisation you please make your `caption_image` function in your class have the following:\n",
    "1. Inputs: image, vocabulary, device, max_length\n",
    "2. Outputs: string prediction, attention (or None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming mean and std are defined as follows:\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def unnormalize(image:np.array, mean:np.array, std:np.array):\n",
    "    \"\"\"\n",
    "    Function to unnormalize an image given its mean and std\n",
    "    \n",
    "    Args: \n",
    "        image (np.array): Numpy array of the image\n",
    "        mean (np.array): Numpy array of the mean \n",
    "        std (np.array): Numpy array of the std\n",
    "\n",
    "    Returns:\n",
    "        Unnormalised numpy array of the image\n",
    "    \"\"\"\n",
    "\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)    # for inplace operations\n",
    "    return image\n",
    "\n",
    "\n",
    "def caption_image(model, \n",
    "                  dataloader, \n",
    "                  image_size:tuple,\n",
    "                  vocabulary:Vocabulary, \n",
    "                  device:str, \n",
    "                  mean:np.array=np.array([0.485, 0.456, 0.406]), \n",
    "                  std:np.array=np.array([0.229, 0.224, 0.225]), \n",
    "                  num_batches:int=1, \n",
    "                  num_images:int=5, \n",
    "                  max_length:int=50, \n",
    "                  show_plot:bool=False):\n",
    "    \"\"\"\n",
    "    Function to generate model predictions from a dataloader\n",
    "\n",
    "    Arg:\n",
    "        model: model to general model prediction (ensure that your model has the function caption_image)\n",
    "        dataloader: dataset to generate prediction\n",
    "        image_size (tuple): image size of images for the model\n",
    "        vocabulary (Vocabulary): dataset vocabulary\n",
    "        device (str): cpu or cuda,\n",
    "        mean (np.array): Numpy array of the mean used for normalisation\n",
    "        std (np.array): Numpy array of the std used for normalisation\n",
    "        num_batches (int, optional): how many batches iterating from dataloader, defaults to 1\n",
    "        num_image (int, optional): how many images per batch to generate model prediction, defaults to 5\n",
    "        max_length (int, optional): maximum length of generated captions, defaults to 50\n",
    "        show_plot (bool, optional): show the image and generated captions in a plot, defaults to False\n",
    "    \n",
    "    Returns:\n",
    "        all_predictions (dict): Dictionary containing the list of all generated captions and actual captions\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    #dictionary containing all the generated predictions and actual predictions\n",
    "    all_predictions = {'Predicted': [], 'Possible Captions': []}\n",
    "\n",
    "    #iterate for num of batches we are testing\n",
    "    for j in range(num_batches):\n",
    "        #load images from dataloader\n",
    "        features, annotations, all_annotations = next(iter(dataloader))\n",
    "\n",
    "        #take first k from batch\n",
    "        for i in range(num_images):\n",
    "            features = torch.nn.functional.interpolate(features, size=image_size, mode='bilinear') #resize image for model, using same as transforms.resize()\n",
    "            image = features[i].unsqueeze(0).to(device)\n",
    "            \n",
    "            #generate captions from model\n",
    "            generated_caption, attention = model.caption_image(image, vocabulary, device, max_length=max_length)\n",
    "            \n",
    "            #plot image and captions\n",
    "            if show_plot:\n",
    "                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                img = features[i].squeeze()\n",
    "                img = unnormalize(img, mean, std)  # Unnormalize the image\n",
    "                img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'Model Prediction:\\n{\" \".join(generated_caption[1:-1])}\\n\\nAll Possible Predictions:\\n' + \"\\n\".join(all_annotations[i]), loc='left')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            all_predictions['Predicted'].append(generated_caption)\n",
    "            all_predictions['Possible Captions'].append(all_annotations[i])\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def multiple_model_captions(\n",
    "        model_list:list[str, tuple],\n",
    "        dataloader, \n",
    "        vocabulary:Vocabulary, \n",
    "        device:str, \n",
    "        mean:np.array=np.array([0.485, 0.456, 0.406]), \n",
    "        std:np.array=np.array([0.229, 0.224, 0.225]), \n",
    "        num_batches:int=1, \n",
    "        num_images:int=5, \n",
    "        max_length:int=50, \n",
    "        show_plot:bool=False):\n",
    "    \"\"\"\n",
    "    Function to generate model predictions for multiple models from the same dataloader\n",
    "\n",
    "    Arg:\n",
    "        model_list (list[str, tuple]): list of [model, img_size] to general model prediction (ensure that your model has the function caption_image)\n",
    "            eg. [[model1, (224,224)], [model2, (256,256)]]\n",
    "        dataloader: dataset to generate prediction\n",
    "        vocabulary (Vocabulary): dataset vocabulary\n",
    "        device (str): cpu or cuda,\n",
    "        mean (np.array): Numpy array of the mean used for normalisation\n",
    "        std (np.array): Numpy array of the std used for normalisation\n",
    "        num_batches (int, optional): how many batches iterating from dataloader, defaults to 1\n",
    "        num_image (int, optional): how many images per batch to generate model prediction, defaults to 5\n",
    "        max_length (int, optional): maximum length of generated captions, defaults to 50\n",
    "        show_plot (bool, optional): show the image and generated captions in a plot, defaults to False\n",
    "    \n",
    "    Returns:\n",
    "        all_predictions (dict): Dictionary containing the dictionary of all generated captions for each model and list of actual captions\n",
    "    \n",
    "    \"\"\"\n",
    "    for model, img_size in model_list:\n",
    "        model.eval()\n",
    "\n",
    "    #dictionary containing all the generated predictions and actual predictions\n",
    "    all_predictions = {'Predicted': {}, 'Possible Captions': []}\n",
    "\n",
    "    #iterate for num of batches we are testing\n",
    "    for j in range(num_batches):\n",
    "        #load images from dataloader\n",
    "        features, annotations, all_annotations = next(iter(dataloader))\n",
    "        \n",
    "        #take first k from batch\n",
    "        for i in range(num_images):\n",
    "            image = features[i].unsqueeze(0).to(device)\n",
    "            all_captions = []\n",
    "\n",
    "            for idx, (model, img_size) in enumerate(model_list):\n",
    "                #resize image\n",
    "                image = torch.nn.functional.interpolate(image, size=img_size, mode='bilinear') #resize image\n",
    "                #generate captions from model\n",
    "                generated_caption, attention = model.caption_image(image, vocabulary, device, max_length=max_length)\n",
    "                all_captions.append(\" \".join(generated_caption[1:-1]))\n",
    "                model_predictions = all_predictions['Predicted'].get(f\"model_{idx}\", [])\n",
    "                model_predictions.append(all_captions)\n",
    "                all_predictions['Predicted'][f\"model_{idx}\"] = model_predictions\n",
    "\n",
    "            #plot image and captions\n",
    "            if show_plot:\n",
    "                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                img = features[i].squeeze()\n",
    "                img = unnormalize(img, mean, std)  # Unnormalize the image\n",
    "                img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "\n",
    "                pred = '\\n'.join(all_captions)\n",
    "                annotation = '\\n'.join(all_annotations[i])\n",
    "\n",
    "                ax.set_title(f'All Model Predictions:\\n{pred}\\n\\nAll Possible Predictions:\\n{annotation}', loc='left')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            all_predictions['Possible Captions'].append(all_annotations[i])\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = caption_image(\n",
    "    model=test_model1,\n",
    "    dataloader=test_dataloader,\n",
    "    image_size=(299,299),\n",
    "    vocabulary=test_dataset.vocab,\n",
    "    device=device,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    num_batches=1,\n",
    "    num_images=5,\n",
    "    max_length=50,\n",
    "    show_plot=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
